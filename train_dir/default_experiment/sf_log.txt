[2025-07-18 12:41:07,206][2131609] Saving configuration to /home/isaacp/repos/rl_class/train_dir/default_experiment/config.json...
[2025-07-18 12:41:07,208][2131609] Rollout worker 0 uses device cpu
[2025-07-18 12:41:07,209][2131609] Rollout worker 1 uses device cpu
[2025-07-18 12:41:07,209][2131609] Rollout worker 2 uses device cpu
[2025-07-18 12:41:07,209][2131609] Rollout worker 3 uses device cpu
[2025-07-18 12:41:07,209][2131609] Rollout worker 4 uses device cpu
[2025-07-18 12:41:07,209][2131609] Rollout worker 5 uses device cpu
[2025-07-18 12:41:07,209][2131609] Rollout worker 6 uses device cpu
[2025-07-18 12:41:07,210][2131609] Rollout worker 7 uses device cpu
[2025-07-18 12:41:07,235][2131609] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 12:41:07,235][2131609] InferenceWorker_p0-w0: min num requests: 2
[2025-07-18 12:41:07,255][2131609] Starting all processes...
[2025-07-18 12:41:07,255][2131609] Starting process learner_proc0
[2025-07-18 12:41:07,305][2131609] Starting all processes...
[2025-07-18 12:41:07,309][2131609] Starting process inference_proc0-0
[2025-07-18 12:41:07,310][2131609] Starting process rollout_proc0
[2025-07-18 12:41:07,310][2131609] Starting process rollout_proc1
[2025-07-18 12:41:07,310][2131609] Starting process rollout_proc2
[2025-07-18 12:41:07,310][2131609] Starting process rollout_proc3
[2025-07-18 12:41:07,310][2131609] Starting process rollout_proc4
[2025-07-18 12:41:07,311][2131609] Starting process rollout_proc5
[2025-07-18 12:41:07,311][2131609] Starting process rollout_proc6
[2025-07-18 12:41:07,315][2131609] Starting process rollout_proc7
[2025-07-18 12:41:08,181][2134836] Worker 3 uses CPU cores [12, 13, 14, 15]
[2025-07-18 12:41:08,214][2134832] Worker 0 uses CPU cores [0, 1, 2, 3]
[2025-07-18 12:41:08,220][2134837] Worker 4 uses CPU cores [16, 17, 18, 19]
[2025-07-18 12:41:08,220][2134839] Worker 6 uses CPU cores [24, 25, 26, 27]
[2025-07-18 12:41:08,220][2134833] Worker 1 uses CPU cores [4, 5, 6, 7]
[2025-07-18 12:41:08,220][2134834] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 12:41:08,220][2134838] Worker 5 uses CPU cores [20, 21, 22, 23]
[2025-07-18 12:41:08,220][2134834] Set environment var CUDA_VISIBLE_DEVICES to '0' (GPU indices [0]) for inference process 0
[2025-07-18 12:41:08,221][2134840] Worker 7 uses CPU cores [28, 29, 30, 31]
[2025-07-18 12:41:08,222][2134819] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 12:41:08,222][2134819] Set environment var CUDA_VISIBLE_DEVICES to '0' (GPU indices [0]) for learning process 0
[2025-07-18 12:41:08,232][2134834] Num visible devices: 1
[2025-07-18 12:41:08,233][2134819] Num visible devices: 1
[2025-07-18 12:41:08,234][2134819] Starting seed is not provided
[2025-07-18 12:41:08,234][2134819] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 12:41:08,234][2134819] Initializing actor-critic model on device cuda:0
[2025-07-18 12:41:08,234][2134819] RunningMeanStd input shape: (3, 72, 128)
[2025-07-18 12:41:08,235][2134819] RunningMeanStd input shape: (1,)
[2025-07-18 12:41:08,235][2134835] Worker 2 uses CPU cores [8, 9, 10, 11]
[2025-07-18 12:41:08,240][2134819] ConvEncoder: input_channels=3
[2025-07-18 12:41:08,309][2134819] Conv encoder output size: 512
[2025-07-18 12:41:08,309][2134819] Policy head output size: 512
[2025-07-18 12:41:08,315][2134819] Created Actor Critic model with architecture:
[2025-07-18 12:41:08,315][2134819] ActorCriticSharedWeights(
  (obs_normalizer): ObservationNormalizer(
    (running_mean_std): RunningMeanStdDictInPlace(
      (running_mean_std): ModuleDict(
        (obs): RunningMeanStdInPlace()
      )
    )
  )
  (returns_normalizer): RecursiveScriptModule(original_name=RunningMeanStdInPlace)
  (encoder): VizdoomEncoder(
    (basic_encoder): ConvEncoder(
      (enc): RecursiveScriptModule(
        original_name=ConvEncoderImpl
        (conv_head): RecursiveScriptModule(
          original_name=Sequential
          (0): RecursiveScriptModule(original_name=Conv2d)
          (1): RecursiveScriptModule(original_name=ELU)
          (2): RecursiveScriptModule(original_name=Conv2d)
          (3): RecursiveScriptModule(original_name=ELU)
          (4): RecursiveScriptModule(original_name=Conv2d)
          (5): RecursiveScriptModule(original_name=ELU)
        )
        (mlp_layers): RecursiveScriptModule(
          original_name=Sequential
          (0): RecursiveScriptModule(original_name=Linear)
          (1): RecursiveScriptModule(original_name=ELU)
        )
      )
    )
  )
  (core): ModelCoreRNN(
    (core): GRU(512, 512)
  )
  (decoder): MlpDecoder(
    (mlp): Identity()
  )
  (critic_linear): Linear(in_features=512, out_features=1, bias=True)
  (action_parameterization): ActionParameterizationDefault(
    (distribution_linear): Linear(in_features=512, out_features=5, bias=True)
  )
)
[2025-07-18 12:41:08,403][2134819] Using optimizer <class 'torch.optim.adam.Adam'>
[2025-07-18 12:41:08,884][2134819] No checkpoints found
[2025-07-18 12:41:08,884][2134819] Did not load from checkpoint, starting from scratch!
[2025-07-18 12:41:08,884][2134819] Initialized policy 0 weights for model version 0
[2025-07-18 12:41:08,885][2134819] LearnerWorker_p0 finished initialization!
[2025-07-18 12:41:08,885][2134819] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 12:41:08,937][2134834] RunningMeanStd input shape: (3, 72, 128)
[2025-07-18 12:41:08,938][2134834] RunningMeanStd input shape: (1,)
[2025-07-18 12:41:08,943][2134834] ConvEncoder: input_channels=3
[2025-07-18 12:41:08,983][2134834] Conv encoder output size: 512
[2025-07-18 12:41:08,983][2134834] Policy head output size: 512
[2025-07-18 12:41:09,003][2131609] Inference worker 0-0 is ready!
[2025-07-18 12:41:09,004][2131609] All inference workers are ready! Signal rollout workers to start!
[2025-07-18 12:41:09,020][2134836] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 12:41:09,020][2134840] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 12:41:09,020][2134838] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 12:41:09,020][2134833] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 12:41:09,020][2134837] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 12:41:09,020][2134832] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 12:41:09,027][2134835] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 12:41:09,027][2134839] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 12:41:09,171][2134838] Decorrelating experience for 0 frames...
[2025-07-18 12:41:09,171][2134833] Decorrelating experience for 0 frames...
[2025-07-18 12:41:09,172][2134840] Decorrelating experience for 0 frames...
[2025-07-18 12:41:09,172][2134836] Decorrelating experience for 0 frames...
[2025-07-18 12:41:09,299][2134833] Decorrelating experience for 32 frames...
[2025-07-18 12:41:09,299][2134838] Decorrelating experience for 32 frames...
[2025-07-18 12:41:09,301][2134840] Decorrelating experience for 32 frames...
[2025-07-18 12:41:09,301][2134836] Decorrelating experience for 32 frames...
[2025-07-18 12:41:09,318][2134839] Decorrelating experience for 0 frames...
[2025-07-18 12:41:09,450][2134839] Decorrelating experience for 32 frames...
[2025-07-18 12:41:09,454][2134838] Decorrelating experience for 64 frames...
[2025-07-18 12:41:09,455][2134833] Decorrelating experience for 64 frames...
[2025-07-18 12:41:09,462][2134837] Decorrelating experience for 0 frames...
[2025-07-18 12:41:09,585][2134837] Decorrelating experience for 32 frames...
[2025-07-18 12:41:09,592][2134833] Decorrelating experience for 96 frames...
[2025-07-18 12:41:09,609][2134839] Decorrelating experience for 64 frames...
[2025-07-18 12:41:09,615][2134832] Decorrelating experience for 0 frames...
[2025-07-18 12:41:09,630][2134835] Decorrelating experience for 0 frames...
[2025-07-18 12:41:09,738][2134837] Decorrelating experience for 64 frames...
[2025-07-18 12:41:09,738][2134832] Decorrelating experience for 32 frames...
[2025-07-18 12:41:09,743][2134838] Decorrelating experience for 96 frames...
[2025-07-18 12:41:09,758][2134839] Decorrelating experience for 96 frames...
[2025-07-18 12:41:09,874][2134837] Decorrelating experience for 96 frames...
[2025-07-18 12:41:09,894][2134832] Decorrelating experience for 64 frames...
[2025-07-18 12:41:09,912][2134840] Decorrelating experience for 64 frames...
[2025-07-18 12:41:09,912][2134835] Decorrelating experience for 32 frames...
[2025-07-18 12:41:10,045][2134832] Decorrelating experience for 96 frames...
[2025-07-18 12:41:10,055][2134840] Decorrelating experience for 96 frames...
[2025-07-18 12:41:10,127][2134819] Signal inference workers to stop experience collection...
[2025-07-18 12:41:10,128][2134834] InferenceWorker_p0-w0: stopping experience collection
[2025-07-18 12:41:10,183][2134835] Decorrelating experience for 64 frames...
[2025-07-18 12:41:10,218][2134836] Decorrelating experience for 64 frames...
[2025-07-18 12:41:10,322][2134835] Decorrelating experience for 96 frames...
[2025-07-18 12:41:10,354][2134836] Decorrelating experience for 96 frames...
[2025-07-18 12:41:10,452][2131609] Fps is (10 sec: nan, 60 sec: nan, 300 sec: nan). Total num frames: 0. Throughput: 0: nan. Samples: 0. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[2025-07-18 12:41:10,453][2131609] Avg episode reward: [(0, '3.012')]
[2025-07-18 12:41:10,590][2134819] Signal inference workers to resume experience collection...
[2025-07-18 12:41:10,591][2134834] InferenceWorker_p0-w0: resuming experience collection
[2025-07-18 12:41:11,156][2134834] Updated weights for policy 0, policy_version 10 (0.0032)
[2025-07-18 12:41:11,765][2134834] Updated weights for policy 0, policy_version 20 (0.0004)
[2025-07-18 12:41:12,389][2134834] Updated weights for policy 0, policy_version 30 (0.0004)
[2025-07-18 12:41:13,010][2134834] Updated weights for policy 0, policy_version 40 (0.0004)
[2025-07-18 12:41:13,632][2134834] Updated weights for policy 0, policy_version 50 (0.0004)
[2025-07-18 12:41:14,270][2134834] Updated weights for policy 0, policy_version 60 (0.0004)
[2025-07-18 12:41:14,945][2134834] Updated weights for policy 0, policy_version 70 (0.0004)
[2025-07-18 12:41:15,452][2131609] Fps is (10 sec: 63078.9, 60 sec: 63078.9, 300 sec: 63078.9). Total num frames: 315392. Throughput: 0: 14646.5. Samples: 73232. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)
[2025-07-18 12:41:15,453][2131609] Avg episode reward: [(0, '4.384')]
[2025-07-18 12:41:15,454][2134819] Saving new best policy, reward=4.384!
[2025-07-18 12:41:15,600][2134834] Updated weights for policy 0, policy_version 80 (0.0004)
[2025-07-18 12:41:16,191][2134834] Updated weights for policy 0, policy_version 90 (0.0003)
[2025-07-18 12:41:16,834][2134834] Updated weights for policy 0, policy_version 100 (0.0004)
[2025-07-18 12:41:17,453][2134834] Updated weights for policy 0, policy_version 110 (0.0004)
[2025-07-18 12:41:18,063][2134834] Updated weights for policy 0, policy_version 120 (0.0004)
[2025-07-18 12:41:18,701][2134834] Updated weights for policy 0, policy_version 130 (0.0004)
[2025-07-18 12:41:19,308][2134834] Updated weights for policy 0, policy_version 140 (0.0003)
[2025-07-18 12:41:19,919][2134834] Updated weights for policy 0, policy_version 150 (0.0003)
[2025-07-18 12:41:20,452][2131609] Fps is (10 sec: 64717.3, 60 sec: 64717.3, 300 sec: 64717.3). Total num frames: 647168. Throughput: 0: 12251.5. Samples: 122514. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)
[2025-07-18 12:41:20,453][2131609] Avg episode reward: [(0, '4.514')]
[2025-07-18 12:41:20,462][2134819] Saving new best policy, reward=4.514!
[2025-07-18 12:41:20,522][2134834] Updated weights for policy 0, policy_version 160 (0.0003)
[2025-07-18 12:41:21,130][2134834] Updated weights for policy 0, policy_version 170 (0.0004)
[2025-07-18 12:41:21,744][2134834] Updated weights for policy 0, policy_version 180 (0.0004)
[2025-07-18 12:41:22,345][2134834] Updated weights for policy 0, policy_version 190 (0.0004)
[2025-07-18 12:41:22,965][2134834] Updated weights for policy 0, policy_version 200 (0.0004)
[2025-07-18 12:41:23,595][2134834] Updated weights for policy 0, policy_version 210 (0.0003)
[2025-07-18 12:41:24,233][2134834] Updated weights for policy 0, policy_version 220 (0.0004)
[2025-07-18 12:41:24,867][2134834] Updated weights for policy 0, policy_version 230 (0.0004)
[2025-07-18 12:41:25,452][2131609] Fps is (10 sec: 66355.4, 60 sec: 65263.2, 300 sec: 65263.2). Total num frames: 978944. Throughput: 0: 14818.9. Samples: 222282. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)
[2025-07-18 12:41:25,453][2131609] Avg episode reward: [(0, '5.051')]
[2025-07-18 12:41:25,455][2134819] Saving new best policy, reward=5.051!
[2025-07-18 12:41:25,502][2134834] Updated weights for policy 0, policy_version 240 (0.0004)
[2025-07-18 12:41:26,097][2134834] Updated weights for policy 0, policy_version 250 (0.0004)
[2025-07-18 12:41:26,726][2134834] Updated weights for policy 0, policy_version 260 (0.0004)
[2025-07-18 12:41:27,230][2131609] Heartbeat connected on Batcher_0
[2025-07-18 12:41:27,232][2131609] Heartbeat connected on LearnerWorker_p0
[2025-07-18 12:41:27,236][2131609] Heartbeat connected on InferenceWorker_p0-w0
[2025-07-18 12:41:27,238][2131609] Heartbeat connected on RolloutWorker_w0
[2025-07-18 12:41:27,241][2131609] Heartbeat connected on RolloutWorker_w1
[2025-07-18 12:41:27,243][2131609] Heartbeat connected on RolloutWorker_w2
[2025-07-18 12:41:27,245][2131609] Heartbeat connected on RolloutWorker_w3
[2025-07-18 12:41:27,248][2131609] Heartbeat connected on RolloutWorker_w4
[2025-07-18 12:41:27,250][2131609] Heartbeat connected on RolloutWorker_w5
[2025-07-18 12:41:27,252][2131609] Heartbeat connected on RolloutWorker_w6
[2025-07-18 12:41:27,255][2131609] Heartbeat connected on RolloutWorker_w7
[2025-07-18 12:41:27,359][2134834] Updated weights for policy 0, policy_version 270 (0.0003)
[2025-07-18 12:41:27,974][2134834] Updated weights for policy 0, policy_version 280 (0.0004)
[2025-07-18 12:41:28,603][2134834] Updated weights for policy 0, policy_version 290 (0.0004)
[2025-07-18 12:41:29,233][2134834] Updated weights for policy 0, policy_version 300 (0.0004)
[2025-07-18 12:41:29,843][2134834] Updated weights for policy 0, policy_version 310 (0.0003)
[2025-07-18 12:41:30,449][2134834] Updated weights for policy 0, policy_version 320 (0.0003)
[2025-07-18 12:41:30,452][2131609] Fps is (10 sec: 66355.0, 60 sec: 65536.1, 300 sec: 65536.1). Total num frames: 1310720. Throughput: 0: 16050.4. Samples: 321008. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2025-07-18 12:41:30,453][2131609] Avg episode reward: [(0, '6.884')]
[2025-07-18 12:41:30,453][2134819] Saving new best policy, reward=6.884!
[2025-07-18 12:41:31,066][2134834] Updated weights for policy 0, policy_version 330 (0.0003)
[2025-07-18 12:41:31,669][2134834] Updated weights for policy 0, policy_version 340 (0.0004)
[2025-07-18 12:41:32,280][2134834] Updated weights for policy 0, policy_version 350 (0.0004)
[2025-07-18 12:41:32,945][2134834] Updated weights for policy 0, policy_version 360 (0.0004)
[2025-07-18 12:41:33,563][2134834] Updated weights for policy 0, policy_version 370 (0.0003)
[2025-07-18 12:41:34,181][2134834] Updated weights for policy 0, policy_version 380 (0.0004)
[2025-07-18 12:41:34,799][2134834] Updated weights for policy 0, policy_version 390 (0.0004)
[2025-07-18 12:41:35,423][2134834] Updated weights for policy 0, policy_version 400 (0.0004)
[2025-07-18 12:41:35,452][2131609] Fps is (10 sec: 65945.6, 60 sec: 65536.2, 300 sec: 65536.2). Total num frames: 1638400. Throughput: 0: 14816.3. Samples: 370406. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2025-07-18 12:41:35,452][2131609] Avg episode reward: [(0, '11.467')]
[2025-07-18 12:41:35,454][2134819] Saving new best policy, reward=11.467!
[2025-07-18 12:41:36,055][2134834] Updated weights for policy 0, policy_version 410 (0.0004)
[2025-07-18 12:41:36,689][2134834] Updated weights for policy 0, policy_version 420 (0.0004)
[2025-07-18 12:41:37,313][2134834] Updated weights for policy 0, policy_version 430 (0.0004)
[2025-07-18 12:41:37,958][2134834] Updated weights for policy 0, policy_version 440 (0.0004)
[2025-07-18 12:41:38,580][2134834] Updated weights for policy 0, policy_version 450 (0.0004)
[2025-07-18 12:41:39,210][2134834] Updated weights for policy 0, policy_version 460 (0.0004)
[2025-07-18 12:41:39,845][2134834] Updated weights for policy 0, policy_version 470 (0.0004)
[2025-07-18 12:41:40,452][2134834] Updated weights for policy 0, policy_version 480 (0.0003)
[2025-07-18 12:41:40,452][2131609] Fps is (10 sec: 65536.1, 60 sec: 65536.1, 300 sec: 65536.1). Total num frames: 1966080. Throughput: 0: 15619.0. Samples: 468568. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)
[2025-07-18 12:41:40,453][2131609] Avg episode reward: [(0, '16.069')]
[2025-07-18 12:41:40,453][2134819] Saving new best policy, reward=16.069!
[2025-07-18 12:41:41,062][2134834] Updated weights for policy 0, policy_version 490 (0.0003)
[2025-07-18 12:41:41,685][2134834] Updated weights for policy 0, policy_version 500 (0.0004)
[2025-07-18 12:41:42,311][2134834] Updated weights for policy 0, policy_version 510 (0.0003)
[2025-07-18 12:41:42,902][2134834] Updated weights for policy 0, policy_version 520 (0.0004)
[2025-07-18 12:41:43,531][2134834] Updated weights for policy 0, policy_version 530 (0.0004)
[2025-07-18 12:41:44,139][2134834] Updated weights for policy 0, policy_version 540 (0.0003)
[2025-07-18 12:41:44,757][2134834] Updated weights for policy 0, policy_version 550 (0.0004)
[2025-07-18 12:41:45,397][2134834] Updated weights for policy 0, policy_version 560 (0.0004)
[2025-07-18 12:41:45,452][2131609] Fps is (10 sec: 65535.7, 60 sec: 65536.0, 300 sec: 65536.0). Total num frames: 2293760. Throughput: 0: 16224.5. Samples: 567856. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2025-07-18 12:41:45,453][2131609] Avg episode reward: [(0, '15.339')]
[2025-07-18 12:41:46,046][2134834] Updated weights for policy 0, policy_version 570 (0.0004)
[2025-07-18 12:41:46,685][2134834] Updated weights for policy 0, policy_version 580 (0.0004)
[2025-07-18 12:41:47,349][2134834] Updated weights for policy 0, policy_version 590 (0.0004)
[2025-07-18 12:41:47,983][2134834] Updated weights for policy 0, policy_version 600 (0.0004)
[2025-07-18 12:41:48,619][2134834] Updated weights for policy 0, policy_version 610 (0.0004)
[2025-07-18 12:41:49,267][2134834] Updated weights for policy 0, policy_version 620 (0.0004)
[2025-07-18 12:41:49,870][2134834] Updated weights for policy 0, policy_version 630 (0.0004)
[2025-07-18 12:41:50,452][2131609] Fps is (10 sec: 65126.4, 60 sec: 65433.7, 300 sec: 65433.7). Total num frames: 2617344. Throughput: 0: 15389.3. Samples: 615570. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2025-07-18 12:41:50,453][2131609] Avg episode reward: [(0, '17.851')]
[2025-07-18 12:41:50,459][2134819] Saving new best policy, reward=17.851!
[2025-07-18 12:41:50,460][2134834] Updated weights for policy 0, policy_version 640 (0.0003)
[2025-07-18 12:41:51,069][2134834] Updated weights for policy 0, policy_version 650 (0.0003)
[2025-07-18 12:41:51,658][2134834] Updated weights for policy 0, policy_version 660 (0.0003)
[2025-07-18 12:41:52,267][2134834] Updated weights for policy 0, policy_version 670 (0.0003)
[2025-07-18 12:41:52,870][2134834] Updated weights for policy 0, policy_version 680 (0.0003)
[2025-07-18 12:41:53,491][2134834] Updated weights for policy 0, policy_version 690 (0.0004)
[2025-07-18 12:41:54,122][2134834] Updated weights for policy 0, policy_version 700 (0.0004)
[2025-07-18 12:41:54,722][2134834] Updated weights for policy 0, policy_version 710 (0.0004)
[2025-07-18 12:41:55,332][2134834] Updated weights for policy 0, policy_version 720 (0.0004)
[2025-07-18 12:41:55,452][2131609] Fps is (10 sec: 65945.7, 60 sec: 65627.1, 300 sec: 65627.1). Total num frames: 2953216. Throughput: 0: 15905.9. Samples: 715766. Policy #0 lag: (min: 0.0, avg: 0.6, max: 1.0)
[2025-07-18 12:41:55,453][2131609] Avg episode reward: [(0, '21.439')]
[2025-07-18 12:41:55,454][2134819] Saving new best policy, reward=21.439!
[2025-07-18 12:41:55,948][2134834] Updated weights for policy 0, policy_version 730 (0.0004)
[2025-07-18 12:41:56,576][2134834] Updated weights for policy 0, policy_version 740 (0.0004)
[2025-07-18 12:41:57,183][2134834] Updated weights for policy 0, policy_version 750 (0.0004)
[2025-07-18 12:41:57,809][2134834] Updated weights for policy 0, policy_version 760 (0.0004)
[2025-07-18 12:41:58,441][2134834] Updated weights for policy 0, policy_version 770 (0.0004)
[2025-07-18 12:41:59,092][2134834] Updated weights for policy 0, policy_version 780 (0.0004)
[2025-07-18 12:41:59,724][2134834] Updated weights for policy 0, policy_version 790 (0.0004)
[2025-07-18 12:42:00,326][2134834] Updated weights for policy 0, policy_version 800 (0.0003)
[2025-07-18 12:42:00,452][2131609] Fps is (10 sec: 66764.7, 60 sec: 65699.9, 300 sec: 65699.9). Total num frames: 3284992. Throughput: 0: 16468.5. Samples: 814314. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2025-07-18 12:42:00,453][2131609] Avg episode reward: [(0, '20.942')]
[2025-07-18 12:42:00,945][2134834] Updated weights for policy 0, policy_version 810 (0.0004)
[2025-07-18 12:42:01,547][2134834] Updated weights for policy 0, policy_version 820 (0.0004)
[2025-07-18 12:42:02,143][2134834] Updated weights for policy 0, policy_version 830 (0.0003)
[2025-07-18 12:42:02,738][2134834] Updated weights for policy 0, policy_version 840 (0.0003)
[2025-07-18 12:42:03,354][2134834] Updated weights for policy 0, policy_version 850 (0.0004)
[2025-07-18 12:42:03,959][2134834] Updated weights for policy 0, policy_version 860 (0.0003)
[2025-07-18 12:42:04,573][2134834] Updated weights for policy 0, policy_version 870 (0.0003)
[2025-07-18 12:42:05,193][2134834] Updated weights for policy 0, policy_version 880 (0.0004)
[2025-07-18 12:42:05,452][2131609] Fps is (10 sec: 66764.7, 60 sec: 65833.9, 300 sec: 65833.9). Total num frames: 3620864. Throughput: 0: 16505.6. Samples: 865268. Policy #0 lag: (min: 0.0, avg: 0.5, max: 1.0)
[2025-07-18 12:42:05,453][2131609] Avg episode reward: [(0, '18.958')]
[2025-07-18 12:42:05,805][2134834] Updated weights for policy 0, policy_version 890 (0.0004)
[2025-07-18 12:42:06,446][2134834] Updated weights for policy 0, policy_version 900 (0.0004)
[2025-07-18 12:42:07,082][2134834] Updated weights for policy 0, policy_version 910 (0.0004)
[2025-07-18 12:42:07,694][2134834] Updated weights for policy 0, policy_version 920 (0.0003)
[2025-07-18 12:42:08,310][2134834] Updated weights for policy 0, policy_version 930 (0.0004)
[2025-07-18 12:42:08,935][2134834] Updated weights for policy 0, policy_version 940 (0.0004)
[2025-07-18 12:42:09,536][2134834] Updated weights for policy 0, policy_version 950 (0.0003)
[2025-07-18 12:42:10,176][2134834] Updated weights for policy 0, policy_version 960 (0.0004)
[2025-07-18 12:42:10,452][2131609] Fps is (10 sec: 66355.2, 60 sec: 65809.1, 300 sec: 65809.1). Total num frames: 3948544. Throughput: 0: 16496.1. Samples: 964608. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2025-07-18 12:42:10,453][2131609] Avg episode reward: [(0, '24.071')]
[2025-07-18 12:42:10,453][2134819] Saving new best policy, reward=24.071!
[2025-07-18 12:42:10,789][2134834] Updated weights for policy 0, policy_version 970 (0.0004)
[2025-07-18 12:42:11,280][2134819] Stopping Batcher_0...
[2025-07-18 12:42:11,280][2134819] Saving /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 12:42:11,280][2131609] Component Batcher_0 stopped!
[2025-07-18 12:42:11,281][2134819] Loop batcher_evt_loop terminating...
[2025-07-18 12:42:11,292][2134834] Weights refcount: 2 0
[2025-07-18 12:42:11,292][2134834] Stopping InferenceWorker_p0-w0...
[2025-07-18 12:42:11,293][2134834] Loop inference_proc0-0_evt_loop terminating...
[2025-07-18 12:42:11,293][2131609] Component InferenceWorker_p0-w0 stopped!
[2025-07-18 12:42:11,304][2134835] Stopping RolloutWorker_w2...
[2025-07-18 12:42:11,304][2134835] Loop rollout_proc2_evt_loop terminating...
[2025-07-18 12:42:11,304][2131609] Component RolloutWorker_w2 stopped!
[2025-07-18 12:42:11,304][2134840] Stopping RolloutWorker_w7...
[2025-07-18 12:42:11,305][2134840] Loop rollout_proc7_evt_loop terminating...
[2025-07-18 12:42:11,305][2131609] Component RolloutWorker_w7 stopped!
[2025-07-18 12:42:11,306][2134838] Stopping RolloutWorker_w5...
[2025-07-18 12:42:11,306][2134838] Loop rollout_proc5_evt_loop terminating...
[2025-07-18 12:42:11,306][2131609] Component RolloutWorker_w5 stopped!
[2025-07-18 12:42:11,306][2134832] Stopping RolloutWorker_w0...
[2025-07-18 12:42:11,306][2134832] Loop rollout_proc0_evt_loop terminating...
[2025-07-18 12:42:11,306][2134837] Stopping RolloutWorker_w4...
[2025-07-18 12:42:11,306][2131609] Component RolloutWorker_w0 stopped!
[2025-07-18 12:42:11,306][2134833] Stopping RolloutWorker_w1...
[2025-07-18 12:42:11,306][2134837] Loop rollout_proc4_evt_loop terminating...
[2025-07-18 12:42:11,306][2134833] Loop rollout_proc1_evt_loop terminating...
[2025-07-18 12:42:11,306][2131609] Component RolloutWorker_w4 stopped!
[2025-07-18 12:42:11,307][2131609] Component RolloutWorker_w1 stopped!
[2025-07-18 12:42:11,307][2134839] Stopping RolloutWorker_w6...
[2025-07-18 12:42:11,307][2131609] Component RolloutWorker_w6 stopped!
[2025-07-18 12:42:11,307][2134836] Stopping RolloutWorker_w3...
[2025-07-18 12:42:11,307][2134839] Loop rollout_proc6_evt_loop terminating...
[2025-07-18 12:42:11,307][2134836] Loop rollout_proc3_evt_loop terminating...
[2025-07-18 12:42:11,307][2131609] Component RolloutWorker_w3 stopped!
[2025-07-18 12:42:11,313][2134819] Saving new best policy, reward=25.980!
[2025-07-18 12:42:11,351][2134819] Saving /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 12:42:11,389][2134819] Stopping LearnerWorker_p0...
[2025-07-18 12:42:11,389][2134819] Loop learner_proc0_evt_loop terminating...
[2025-07-18 12:42:11,389][2131609] Component LearnerWorker_p0 stopped!
[2025-07-18 12:42:11,390][2131609] Waiting for process learner_proc0 to stop...
[2025-07-18 12:42:11,938][2131609] Waiting for process inference_proc0-0 to join...
[2025-07-18 12:42:11,939][2131609] Waiting for process rollout_proc0 to join...
[2025-07-18 12:42:11,939][2131609] Waiting for process rollout_proc1 to join...
[2025-07-18 12:42:11,939][2131609] Waiting for process rollout_proc2 to join...
[2025-07-18 12:42:11,940][2131609] Waiting for process rollout_proc3 to join...
[2025-07-18 12:42:11,940][2131609] Waiting for process rollout_proc4 to join...
[2025-07-18 12:42:11,940][2131609] Waiting for process rollout_proc5 to join...
[2025-07-18 12:42:11,941][2131609] Waiting for process rollout_proc6 to join...
[2025-07-18 12:42:11,941][2131609] Waiting for process rollout_proc7 to join...
[2025-07-18 12:42:11,941][2131609] Batcher 0 profile tree view:
batching: 4.6999, releasing_batches: 0.0103
[2025-07-18 12:42:11,942][2131609] InferenceWorker_p0-w0 profile tree view:
wait_policy: 0.0000
  wait_policy_total: 1.3157
update_model: 0.9089
  weight_update: 0.0004
one_step: 0.0009
  handle_policy_step: 56.9841
    deserialize: 2.5305, stack: 0.2878, obs_to_device_normalize: 15.1368, forward: 23.3652, send_messages: 3.6948
    prepare_outputs: 9.6482
      to_cpu: 6.8239
[2025-07-18 12:42:11,942][2131609] Learner 0 profile tree view:
misc: 0.0035, prepare_batch: 2.9473
train: 7.7025
  epoch_init: 0.0024, minibatch_init: 0.0024, losses_postprocess: 0.1575, kl_divergence: 0.1523, after_optimizer: 1.6586
  calculate_losses: 3.1213
    losses_init: 0.0012, forward_head: 0.2357, bptt_initial: 1.8067, tail: 0.2154, advantages_returns: 0.0522, losses: 0.4093
    bptt: 0.3443
      bptt_forward_core: 0.3290
  update: 2.4653
    clip: 0.2768
[2025-07-18 12:42:11,942][2131609] RolloutWorker_w0 profile tree view:
wait_for_trajectories: 0.0482, enqueue_policy_requests: 2.2719, env_step: 32.4243, overhead: 1.6266, complete_rollouts: 0.0709
save_policy_outputs: 2.8208
  split_output_tensors: 0.9564
[2025-07-18 12:42:11,942][2131609] RolloutWorker_w7 profile tree view:
wait_for_trajectories: 0.0482, enqueue_policy_requests: 2.2519, env_step: 31.6023, overhead: 1.5753, complete_rollouts: 0.0680
save_policy_outputs: 2.6843
  split_output_tensors: 0.9299
[2025-07-18 12:42:11,943][2131609] Loop Runner_EvtLoop terminating...
[2025-07-18 12:42:11,943][2131609] Runner profile tree view:
main_loop: 64.6885
[2025-07-18 12:42:11,943][2131609] Collected {0: 4005888}, FPS: 61925.8
[2025-07-18 12:46:34,092][2131609] Could not query the git revision for the logs, perhaps git is not available
[2025-07-18 12:46:34,093][2131609] Loading existing experiment configuration from /home/isaacp/repos/rl_class/train_dir/default_experiment/config.json
[2025-07-18 12:46:34,093][2131609] Overriding arg 'num_workers' with value 1 passed from command line
[2025-07-18 12:46:34,093][2131609] Adding new argument 'no_render'=True that is not in the saved config file!
[2025-07-18 12:46:34,094][2131609] Adding new argument 'save_video'=True that is not in the saved config file!
[2025-07-18 12:46:34,094][2131609] Adding new argument 'video_frames'=1000000000.0 that is not in the saved config file!
[2025-07-18 12:46:34,094][2131609] Adding new argument 'video_name'=None that is not in the saved config file!
[2025-07-18 12:46:34,094][2131609] Adding new argument 'max_num_frames'=1000000000.0 that is not in the saved config file!
[2025-07-18 12:46:34,094][2131609] Adding new argument 'max_num_episodes'=10 that is not in the saved config file!
[2025-07-18 12:46:34,095][2131609] Adding new argument 'push_to_hub'=False that is not in the saved config file!
[2025-07-18 12:46:34,095][2131609] Adding new argument 'hf_repository'=None that is not in the saved config file!
[2025-07-18 12:46:34,095][2131609] Adding new argument 'policy_index'=0 that is not in the saved config file!
[2025-07-18 12:46:34,095][2131609] Adding new argument 'eval_deterministic'=False that is not in the saved config file!
[2025-07-18 12:46:34,096][2131609] Adding new argument 'train_script'=None that is not in the saved config file!
[2025-07-18 12:46:34,096][2131609] Adding new argument 'enjoy_script'=None that is not in the saved config file!
[2025-07-18 12:46:34,096][2131609] Using frameskip 1 and render_action_repeat=4 for evaluation
[2025-07-18 12:46:34,109][2131609] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 12:46:34,110][2131609] RunningMeanStd input shape: (3, 72, 128)
[2025-07-18 12:46:34,111][2131609] RunningMeanStd input shape: (1,)
[2025-07-18 12:46:34,116][2131609] ConvEncoder: input_channels=3
[2025-07-18 12:46:34,158][2131609] Conv encoder output size: 512
[2025-07-18 12:46:34,159][2131609] Policy head output size: 512
[2025-07-18 12:46:34,279][2131609] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 12:46:34,281][2131609] Could not load from checkpoint, attempt 0
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.10/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device)
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.10/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 12:46:34,282][2131609] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 12:46:34,282][2131609] Could not load from checkpoint, attempt 1
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.10/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device)
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.10/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 12:46:34,283][2131609] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 12:46:34,283][2131609] Could not load from checkpoint, attempt 2
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.10/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device)
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.10/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 12:46:51,267][2136417] Saving configuration to /home/isaacp/repos/rl_class/train_dir/default_experiment/config.json...
[2025-07-18 12:46:51,269][2136417] Rollout worker 0 uses device cpu
[2025-07-18 12:46:51,270][2136417] Rollout worker 1 uses device cpu
[2025-07-18 12:46:51,270][2136417] Rollout worker 2 uses device cpu
[2025-07-18 12:46:51,270][2136417] Rollout worker 3 uses device cpu
[2025-07-18 12:46:51,270][2136417] Rollout worker 4 uses device cpu
[2025-07-18 12:46:51,271][2136417] Rollout worker 5 uses device cpu
[2025-07-18 12:46:51,271][2136417] Rollout worker 6 uses device cpu
[2025-07-18 12:46:51,271][2136417] Rollout worker 7 uses device cpu
[2025-07-18 12:46:51,297][2136417] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 12:46:51,298][2136417] InferenceWorker_p0-w0: min num requests: 2
[2025-07-18 12:46:51,318][2136417] Starting all processes...
[2025-07-18 12:46:51,319][2136417] Starting process learner_proc0
[2025-07-18 12:46:51,368][2136417] Starting all processes...
[2025-07-18 12:46:51,371][2136417] Starting process inference_proc0-0
[2025-07-18 12:46:51,371][2136417] Starting process rollout_proc0
[2025-07-18 12:46:51,372][2136417] Starting process rollout_proc1
[2025-07-18 12:46:51,372][2136417] Starting process rollout_proc2
[2025-07-18 12:46:51,372][2136417] Starting process rollout_proc3
[2025-07-18 12:46:51,372][2136417] Starting process rollout_proc4
[2025-07-18 12:46:51,372][2136417] Starting process rollout_proc5
[2025-07-18 12:46:51,373][2136417] Starting process rollout_proc6
[2025-07-18 12:46:51,373][2136417] Starting process rollout_proc7
[2025-07-18 12:46:52,267][2136566] Worker 4 uses CPU cores [16, 17, 18, 19]
[2025-07-18 12:46:52,271][2136567] Worker 5 uses CPU cores [20, 21, 22, 23]
[2025-07-18 12:46:52,277][2136548] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 12:46:52,277][2136548] Set environment var CUDA_VISIBLE_DEVICES to '0' (GPU indices [0]) for learning process 0
[2025-07-18 12:46:52,285][2136568] Worker 6 uses CPU cores [24, 25, 26, 27]
[2025-07-18 12:46:52,289][2136548] Num visible devices: 1
[2025-07-18 12:46:52,289][2136548] Starting seed is not provided
[2025-07-18 12:46:52,289][2136548] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 12:46:52,289][2136548] Initializing actor-critic model on device cuda:0
[2025-07-18 12:46:52,289][2136548] RunningMeanStd input shape: (3, 72, 128)
[2025-07-18 12:46:52,290][2136565] Worker 3 uses CPU cores [12, 13, 14, 15]
[2025-07-18 12:46:52,290][2136548] RunningMeanStd input shape: (1,)
[2025-07-18 12:46:52,295][2136562] Worker 1 uses CPU cores [4, 5, 6, 7]
[2025-07-18 12:46:52,295][2136561] Worker 0 uses CPU cores [0, 1, 2, 3]
[2025-07-18 12:46:52,296][2136548] ConvEncoder: input_channels=3
[2025-07-18 12:46:52,301][2136564] Worker 2 uses CPU cores [8, 9, 10, 11]
[2025-07-18 12:46:52,344][2136569] Worker 7 uses CPU cores [28, 29, 30, 31]
[2025-07-18 12:46:52,364][2136563] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 12:46:52,364][2136563] Set environment var CUDA_VISIBLE_DEVICES to '0' (GPU indices [0]) for inference process 0
[2025-07-18 12:46:52,369][2136548] Conv encoder output size: 512
[2025-07-18 12:46:52,369][2136548] Policy head output size: 512
[2025-07-18 12:46:52,375][2136548] Created Actor Critic model with architecture:
[2025-07-18 12:46:52,375][2136548] ActorCriticSharedWeights(
  (obs_normalizer): ObservationNormalizer(
    (running_mean_std): RunningMeanStdDictInPlace(
      (running_mean_std): ModuleDict(
        (obs): RunningMeanStdInPlace()
      )
    )
  )
  (returns_normalizer): RecursiveScriptModule(original_name=RunningMeanStdInPlace)
  (encoder): VizdoomEncoder(
    (basic_encoder): ConvEncoder(
      (enc): RecursiveScriptModule(
        original_name=ConvEncoderImpl
        (conv_head): RecursiveScriptModule(
          original_name=Sequential
          (0): RecursiveScriptModule(original_name=Conv2d)
          (1): RecursiveScriptModule(original_name=ELU)
          (2): RecursiveScriptModule(original_name=Conv2d)
          (3): RecursiveScriptModule(original_name=ELU)
          (4): RecursiveScriptModule(original_name=Conv2d)
          (5): RecursiveScriptModule(original_name=ELU)
        )
        (mlp_layers): RecursiveScriptModule(
          original_name=Sequential
          (0): RecursiveScriptModule(original_name=Linear)
          (1): RecursiveScriptModule(original_name=ELU)
        )
      )
    )
  )
  (core): ModelCoreRNN(
    (core): GRU(512, 512)
  )
  (decoder): MlpDecoder(
    (mlp): Identity()
  )
  (critic_linear): Linear(in_features=512, out_features=1, bias=True)
  (action_parameterization): ActionParameterizationDefault(
    (distribution_linear): Linear(in_features=512, out_features=5, bias=True)
  )
)
[2025-07-18 12:46:52,376][2136563] Num visible devices: 1
[2025-07-18 12:46:52,459][2136548] Using optimizer <class 'torch.optim.adam.Adam'>
[2025-07-18 12:46:52,932][2136548] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 12:46:52,933][2136548] Could not load from checkpoint, attempt 0
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.10/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device)
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.10/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 12:46:52,933][2136548] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 12:46:52,934][2136548] Could not load from checkpoint, attempt 1
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.10/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device)
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.10/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 12:46:52,934][2136548] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 12:46:52,934][2136548] Could not load from checkpoint, attempt 2
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.10/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device)
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.10/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 12:46:52,934][2136548] Did not load from checkpoint, starting from scratch!
[2025-07-18 12:46:52,934][2136548] Initialized policy 0 weights for model version 0
[2025-07-18 12:46:52,935][2136548] LearnerWorker_p0 finished initialization!
[2025-07-18 12:46:52,935][2136548] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 12:46:52,983][2136563] RunningMeanStd input shape: (3, 72, 128)
[2025-07-18 12:46:52,984][2136563] RunningMeanStd input shape: (1,)
[2025-07-18 12:46:52,989][2136563] ConvEncoder: input_channels=3
[2025-07-18 12:46:53,029][2136563] Conv encoder output size: 512
[2025-07-18 12:46:53,029][2136563] Policy head output size: 512
[2025-07-18 12:46:53,048][2136417] Inference worker 0-0 is ready!
[2025-07-18 12:46:53,048][2136417] All inference workers are ready! Signal rollout workers to start!
[2025-07-18 12:46:53,064][2136567] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 12:46:53,065][2136561] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 12:46:53,065][2136562] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 12:46:53,065][2136566] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 12:46:53,065][2136568] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 12:46:53,065][2136565] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 12:46:53,065][2136564] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 12:46:53,066][2136569] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 12:46:53,206][2136561] Decorrelating experience for 0 frames...
[2025-07-18 12:46:53,206][2136566] Decorrelating experience for 0 frames...
[2025-07-18 12:46:53,211][2136564] Decorrelating experience for 0 frames...
[2025-07-18 12:46:53,227][2136567] Decorrelating experience for 0 frames...
[2025-07-18 12:46:53,227][2136562] Decorrelating experience for 0 frames...
[2025-07-18 12:46:53,328][2136561] Decorrelating experience for 32 frames...
[2025-07-18 12:46:53,351][2136567] Decorrelating experience for 32 frames...
[2025-07-18 12:46:53,351][2136562] Decorrelating experience for 32 frames...
[2025-07-18 12:46:53,358][2136565] Decorrelating experience for 0 frames...
[2025-07-18 12:46:53,489][2136565] Decorrelating experience for 32 frames...
[2025-07-18 12:46:53,489][2136561] Decorrelating experience for 64 frames...
[2025-07-18 12:46:53,489][2136564] Decorrelating experience for 32 frames...
[2025-07-18 12:46:53,506][2136562] Decorrelating experience for 64 frames...
[2025-07-18 12:46:53,506][2136567] Decorrelating experience for 64 frames...
[2025-07-18 12:46:53,626][2136561] Decorrelating experience for 96 frames...
[2025-07-18 12:46:53,632][2136566] Decorrelating experience for 32 frames...
[2025-07-18 12:46:53,644][2136564] Decorrelating experience for 64 frames...
[2025-07-18 12:46:53,645][2136565] Decorrelating experience for 64 frames...
[2025-07-18 12:46:53,762][2136568] Decorrelating experience for 0 frames...
[2025-07-18 12:46:53,788][2136565] Decorrelating experience for 96 frames...
[2025-07-18 12:46:53,789][2136566] Decorrelating experience for 64 frames...
[2025-07-18 12:46:53,790][2136564] Decorrelating experience for 96 frames...
[2025-07-18 12:46:53,796][2136562] Decorrelating experience for 96 frames...
[2025-07-18 12:46:53,889][2136568] Decorrelating experience for 32 frames...
[2025-07-18 12:46:53,925][2136566] Decorrelating experience for 96 frames...
[2025-07-18 12:46:53,944][2136567] Decorrelating experience for 96 frames...
[2025-07-18 12:46:54,052][2136568] Decorrelating experience for 64 frames...
[2025-07-18 12:46:54,065][2136569] Decorrelating experience for 0 frames...
[2025-07-18 12:46:54,166][2136548] Signal inference workers to stop experience collection...
[2025-07-18 12:46:54,171][2136563] InferenceWorker_p0-w0: stopping experience collection
[2025-07-18 12:46:54,205][2136569] Decorrelating experience for 32 frames...
[2025-07-18 12:46:54,206][2136568] Decorrelating experience for 96 frames...
[2025-07-18 12:46:54,359][2136569] Decorrelating experience for 64 frames...
[2025-07-18 12:46:54,494][2136569] Decorrelating experience for 96 frames...
[2025-07-18 12:46:54,531][2136417] Fps is (10 sec: nan, 60 sec: nan, 300 sec: nan). Total num frames: 0. Throughput: 0: nan. Samples: 0. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[2025-07-18 12:46:54,531][2136417] Avg episode reward: [(0, '2.627')]
[2025-07-18 12:46:54,654][2136548] Signal inference workers to resume experience collection...
[2025-07-18 12:46:54,655][2136563] InferenceWorker_p0-w0: resuming experience collection
[2025-07-18 12:46:55,176][2136563] Updated weights for policy 0, policy_version 10 (0.0032)
[2025-07-18 12:46:55,824][2136563] Updated weights for policy 0, policy_version 20 (0.0004)
[2025-07-18 12:46:56,439][2136563] Updated weights for policy 0, policy_version 30 (0.0004)
[2025-07-18 12:46:57,077][2136563] Updated weights for policy 0, policy_version 40 (0.0004)
[2025-07-18 12:46:57,698][2136563] Updated weights for policy 0, policy_version 50 (0.0003)
[2025-07-18 12:46:58,363][2136563] Updated weights for policy 0, policy_version 60 (0.0004)
[2025-07-18 12:46:58,965][2136563] Updated weights for policy 0, policy_version 70 (0.0004)
[2025-07-18 12:46:59,531][2136417] Fps is (10 sec: 63898.0, 60 sec: 63898.0, 300 sec: 63898.0). Total num frames: 319488. Throughput: 0: 14702.5. Samples: 73512. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)
[2025-07-18 12:46:59,531][2136417] Avg episode reward: [(0, '4.467')]
[2025-07-18 12:46:59,533][2136548] Saving new best policy, reward=4.467!
[2025-07-18 12:46:59,623][2136563] Updated weights for policy 0, policy_version 80 (0.0004)
[2025-07-18 12:47:00,240][2136563] Updated weights for policy 0, policy_version 90 (0.0004)
[2025-07-18 12:47:00,859][2136563] Updated weights for policy 0, policy_version 100 (0.0004)
[2025-07-18 12:47:01,501][2136563] Updated weights for policy 0, policy_version 110 (0.0004)
[2025-07-18 12:47:02,130][2136563] Updated weights for policy 0, policy_version 120 (0.0004)
[2025-07-18 12:47:02,751][2136563] Updated weights for policy 0, policy_version 130 (0.0004)
[2025-07-18 12:47:03,373][2136563] Updated weights for policy 0, policy_version 140 (0.0003)
[2025-07-18 12:47:03,995][2136563] Updated weights for policy 0, policy_version 150 (0.0004)
[2025-07-18 12:47:04,531][2136417] Fps is (10 sec: 64717.0, 60 sec: 64717.0, 300 sec: 64717.0). Total num frames: 647168. Throughput: 0: 12238.4. Samples: 122384. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2025-07-18 12:47:04,532][2136417] Avg episode reward: [(0, '5.126')]
[2025-07-18 12:47:04,532][2136548] Saving new best policy, reward=5.126!
[2025-07-18 12:47:04,636][2136563] Updated weights for policy 0, policy_version 160 (0.0004)
[2025-07-18 12:47:05,282][2136563] Updated weights for policy 0, policy_version 170 (0.0004)
[2025-07-18 12:47:05,921][2136563] Updated weights for policy 0, policy_version 180 (0.0003)
[2025-07-18 12:47:06,527][2136563] Updated weights for policy 0, policy_version 190 (0.0003)
[2025-07-18 12:47:07,116][2136563] Updated weights for policy 0, policy_version 200 (0.0003)
[2025-07-18 12:47:07,706][2136563] Updated weights for policy 0, policy_version 210 (0.0003)
[2025-07-18 12:47:08,309][2136563] Updated weights for policy 0, policy_version 220 (0.0003)
[2025-07-18 12:47:08,936][2136563] Updated weights for policy 0, policy_version 230 (0.0004)
[2025-07-18 12:47:09,531][2136417] Fps is (10 sec: 65945.6, 60 sec: 65263.1, 300 sec: 65263.1). Total num frames: 978944. Throughput: 0: 14776.0. Samples: 221640. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)
[2025-07-18 12:47:09,532][2136417] Avg episode reward: [(0, '8.231')]
[2025-07-18 12:47:09,533][2136548] Saving new best policy, reward=8.231!
[2025-07-18 12:47:09,581][2136563] Updated weights for policy 0, policy_version 240 (0.0003)
[2025-07-18 12:47:10,160][2136563] Updated weights for policy 0, policy_version 250 (0.0003)
[2025-07-18 12:47:10,773][2136563] Updated weights for policy 0, policy_version 260 (0.0004)
[2025-07-18 12:47:11,292][2136417] Heartbeat connected on Batcher_0
[2025-07-18 12:47:11,294][2136417] Heartbeat connected on LearnerWorker_p0
[2025-07-18 12:47:11,299][2136417] Heartbeat connected on InferenceWorker_p0-w0
[2025-07-18 12:47:11,301][2136417] Heartbeat connected on RolloutWorker_w0
[2025-07-18 12:47:11,304][2136417] Heartbeat connected on RolloutWorker_w1
[2025-07-18 12:47:11,306][2136417] Heartbeat connected on RolloutWorker_w2
[2025-07-18 12:47:11,308][2136417] Heartbeat connected on RolloutWorker_w3
[2025-07-18 12:47:11,311][2136417] Heartbeat connected on RolloutWorker_w4
[2025-07-18 12:47:11,313][2136417] Heartbeat connected on RolloutWorker_w5
[2025-07-18 12:47:11,316][2136417] Heartbeat connected on RolloutWorker_w6
[2025-07-18 12:47:11,319][2136417] Heartbeat connected on RolloutWorker_w7
[2025-07-18 12:47:11,375][2136563] Updated weights for policy 0, policy_version 270 (0.0003)
[2025-07-18 12:47:11,983][2136563] Updated weights for policy 0, policy_version 280 (0.0004)
[2025-07-18 12:47:12,583][2136563] Updated weights for policy 0, policy_version 290 (0.0003)
[2025-07-18 12:47:13,189][2136563] Updated weights for policy 0, policy_version 300 (0.0004)
[2025-07-18 12:47:13,784][2136563] Updated weights for policy 0, policy_version 310 (0.0003)
[2025-07-18 12:47:14,398][2136563] Updated weights for policy 0, policy_version 320 (0.0004)
[2025-07-18 12:47:14,531][2136417] Fps is (10 sec: 67174.3, 60 sec: 65945.6, 300 sec: 65945.6). Total num frames: 1318912. Throughput: 0: 16141.8. Samples: 322836. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)
[2025-07-18 12:47:14,532][2136417] Avg episode reward: [(0, '9.845')]
[2025-07-18 12:47:14,532][2136548] Saving new best policy, reward=9.845!
[2025-07-18 12:47:15,017][2136563] Updated weights for policy 0, policy_version 330 (0.0004)
[2025-07-18 12:47:15,607][2136563] Updated weights for policy 0, policy_version 340 (0.0004)
[2025-07-18 12:47:16,216][2136563] Updated weights for policy 0, policy_version 350 (0.0004)
[2025-07-18 12:47:16,839][2136563] Updated weights for policy 0, policy_version 360 (0.0004)
[2025-07-18 12:47:17,453][2136563] Updated weights for policy 0, policy_version 370 (0.0004)
[2025-07-18 12:47:18,047][2136563] Updated weights for policy 0, policy_version 380 (0.0004)
[2025-07-18 12:47:18,677][2136563] Updated weights for policy 0, policy_version 390 (0.0004)
[2025-07-18 12:47:19,300][2136563] Updated weights for policy 0, policy_version 400 (0.0004)
[2025-07-18 12:47:19,531][2136417] Fps is (10 sec: 67174.3, 60 sec: 66027.6, 300 sec: 66027.6). Total num frames: 1650688. Throughput: 0: 14910.3. Samples: 372758. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)
[2025-07-18 12:47:19,532][2136417] Avg episode reward: [(0, '15.961')]
[2025-07-18 12:47:19,534][2136548] Saving new best policy, reward=15.961!
[2025-07-18 12:47:19,940][2136563] Updated weights for policy 0, policy_version 410 (0.0004)
[2025-07-18 12:47:20,539][2136563] Updated weights for policy 0, policy_version 420 (0.0004)
[2025-07-18 12:47:21,164][2136563] Updated weights for policy 0, policy_version 430 (0.0004)
[2025-07-18 12:47:21,786][2136563] Updated weights for policy 0, policy_version 440 (0.0004)
[2025-07-18 12:47:22,430][2136563] Updated weights for policy 0, policy_version 450 (0.0004)
[2025-07-18 12:47:23,063][2136563] Updated weights for policy 0, policy_version 460 (0.0004)
[2025-07-18 12:47:23,688][2136563] Updated weights for policy 0, policy_version 470 (0.0004)
[2025-07-18 12:47:24,325][2136563] Updated weights for policy 0, policy_version 480 (0.0004)
[2025-07-18 12:47:24,531][2136417] Fps is (10 sec: 65945.7, 60 sec: 65945.7, 300 sec: 65945.7). Total num frames: 1978368. Throughput: 0: 15721.7. Samples: 471650. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2025-07-18 12:47:24,531][2136417] Avg episode reward: [(0, '18.114')]
[2025-07-18 12:47:24,532][2136548] Saving new best policy, reward=18.114!
[2025-07-18 12:47:24,966][2136563] Updated weights for policy 0, policy_version 490 (0.0004)
[2025-07-18 12:47:25,571][2136563] Updated weights for policy 0, policy_version 500 (0.0003)
[2025-07-18 12:47:26,182][2136563] Updated weights for policy 0, policy_version 510 (0.0004)
[2025-07-18 12:47:26,788][2136563] Updated weights for policy 0, policy_version 520 (0.0004)
[2025-07-18 12:47:27,407][2136563] Updated weights for policy 0, policy_version 530 (0.0004)
[2025-07-18 12:47:28,027][2136563] Updated weights for policy 0, policy_version 540 (0.0004)
[2025-07-18 12:47:28,640][2136563] Updated weights for policy 0, policy_version 550 (0.0004)
[2025-07-18 12:47:29,250][2136563] Updated weights for policy 0, policy_version 560 (0.0004)
[2025-07-18 12:47:29,531][2136417] Fps is (10 sec: 65945.5, 60 sec: 66004.1, 300 sec: 66004.1). Total num frames: 2310144. Throughput: 0: 16318.1. Samples: 571132. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)
[2025-07-18 12:47:29,532][2136417] Avg episode reward: [(0, '20.348')]
[2025-07-18 12:47:29,533][2136548] Saving new best policy, reward=20.348!
[2025-07-18 12:47:29,865][2136563] Updated weights for policy 0, policy_version 570 (0.0004)
[2025-07-18 12:47:30,490][2136563] Updated weights for policy 0, policy_version 580 (0.0004)
[2025-07-18 12:47:31,114][2136563] Updated weights for policy 0, policy_version 590 (0.0004)
[2025-07-18 12:47:31,737][2136563] Updated weights for policy 0, policy_version 600 (0.0003)
[2025-07-18 12:47:32,366][2136563] Updated weights for policy 0, policy_version 610 (0.0004)
[2025-07-18 12:47:32,970][2136563] Updated weights for policy 0, policy_version 620 (0.0004)
[2025-07-18 12:47:33,590][2136563] Updated weights for policy 0, policy_version 630 (0.0004)
[2025-07-18 12:47:34,191][2136563] Updated weights for policy 0, policy_version 640 (0.0003)
[2025-07-18 12:47:34,531][2136417] Fps is (10 sec: 66354.8, 60 sec: 66048.0, 300 sec: 66048.0). Total num frames: 2641920. Throughput: 0: 15502.8. Samples: 620112. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2025-07-18 12:47:34,532][2136417] Avg episode reward: [(0, '21.767')]
[2025-07-18 12:47:34,532][2136548] Saving new best policy, reward=21.767!
[2025-07-18 12:47:34,801][2136563] Updated weights for policy 0, policy_version 650 (0.0003)
[2025-07-18 12:47:35,400][2136563] Updated weights for policy 0, policy_version 660 (0.0003)
[2025-07-18 12:47:36,000][2136563] Updated weights for policy 0, policy_version 670 (0.0003)
[2025-07-18 12:47:36,592][2136563] Updated weights for policy 0, policy_version 680 (0.0003)
[2025-07-18 12:47:37,185][2136563] Updated weights for policy 0, policy_version 690 (0.0003)
[2025-07-18 12:47:37,780][2136563] Updated weights for policy 0, policy_version 700 (0.0004)
[2025-07-18 12:47:38,372][2136563] Updated weights for policy 0, policy_version 710 (0.0003)
[2025-07-18 12:47:38,987][2136563] Updated weights for policy 0, policy_version 720 (0.0004)
[2025-07-18 12:47:39,531][2136417] Fps is (10 sec: 67174.0, 60 sec: 66264.1, 300 sec: 66264.1). Total num frames: 2981888. Throughput: 0: 16049.0. Samples: 722204. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)
[2025-07-18 12:47:39,532][2136417] Avg episode reward: [(0, '23.402')]
[2025-07-18 12:47:39,533][2136548] Saving new best policy, reward=23.402!
[2025-07-18 12:47:39,632][2136563] Updated weights for policy 0, policy_version 730 (0.0004)
[2025-07-18 12:47:40,266][2136563] Updated weights for policy 0, policy_version 740 (0.0004)
[2025-07-18 12:47:40,902][2136563] Updated weights for policy 0, policy_version 750 (0.0004)
[2025-07-18 12:47:41,497][2136563] Updated weights for policy 0, policy_version 760 (0.0004)
[2025-07-18 12:47:42,119][2136563] Updated weights for policy 0, policy_version 770 (0.0004)
[2025-07-18 12:47:42,728][2136563] Updated weights for policy 0, policy_version 780 (0.0004)
[2025-07-18 12:47:43,347][2136563] Updated weights for policy 0, policy_version 790 (0.0004)
[2025-07-18 12:47:43,961][2136563] Updated weights for policy 0, policy_version 800 (0.0004)
[2025-07-18 12:47:44,531][2136417] Fps is (10 sec: 67174.6, 60 sec: 66273.3, 300 sec: 66273.3). Total num frames: 3313664. Throughput: 0: 16621.7. Samples: 821488. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2025-07-18 12:47:44,531][2136417] Avg episode reward: [(0, '26.788')]
[2025-07-18 12:47:44,532][2136548] Saving new best policy, reward=26.788!
[2025-07-18 12:47:44,581][2136563] Updated weights for policy 0, policy_version 810 (0.0004)
[2025-07-18 12:47:45,181][2136563] Updated weights for policy 0, policy_version 820 (0.0004)
[2025-07-18 12:47:45,814][2136563] Updated weights for policy 0, policy_version 830 (0.0004)
[2025-07-18 12:47:46,438][2136563] Updated weights for policy 0, policy_version 840 (0.0004)
[2025-07-18 12:47:47,060][2136563] Updated weights for policy 0, policy_version 850 (0.0004)
[2025-07-18 12:47:47,651][2136563] Updated weights for policy 0, policy_version 860 (0.0004)
[2025-07-18 12:47:48,251][2136563] Updated weights for policy 0, policy_version 870 (0.0003)
[2025-07-18 12:47:48,851][2136563] Updated weights for policy 0, policy_version 880 (0.0004)
[2025-07-18 12:47:49,470][2136563] Updated weights for policy 0, policy_version 890 (0.0004)
[2025-07-18 12:47:49,531][2136417] Fps is (10 sec: 66355.5, 60 sec: 66280.7, 300 sec: 66280.7). Total num frames: 3645440. Throughput: 0: 16638.2. Samples: 871102. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)
[2025-07-18 12:47:49,531][2136417] Avg episode reward: [(0, '25.847')]
[2025-07-18 12:47:50,080][2136563] Updated weights for policy 0, policy_version 900 (0.0003)
[2025-07-18 12:47:50,696][2136563] Updated weights for policy 0, policy_version 910 (0.0004)
[2025-07-18 12:47:51,321][2136563] Updated weights for policy 0, policy_version 920 (0.0003)
[2025-07-18 12:47:51,908][2136563] Updated weights for policy 0, policy_version 930 (0.0003)
[2025-07-18 12:47:52,521][2136563] Updated weights for policy 0, policy_version 940 (0.0004)
[2025-07-18 12:47:53,151][2136563] Updated weights for policy 0, policy_version 950 (0.0004)
[2025-07-18 12:47:53,744][2136563] Updated weights for policy 0, policy_version 960 (0.0003)
[2025-07-18 12:47:54,350][2136563] Updated weights for policy 0, policy_version 970 (0.0004)
[2025-07-18 12:47:54,531][2136417] Fps is (10 sec: 67174.5, 60 sec: 66423.5, 300 sec: 66423.5). Total num frames: 3985408. Throughput: 0: 16671.5. Samples: 971856. Policy #0 lag: (min: 0.0, avg: 0.6, max: 1.0)
[2025-07-18 12:47:54,531][2136417] Avg episode reward: [(0, '21.239')]
[2025-07-18 12:47:54,834][2136548] Stopping Batcher_0...
[2025-07-18 12:47:54,834][2136548] Loop batcher_evt_loop terminating...
[2025-07-18 12:47:54,834][2136548] Saving /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 12:47:54,834][2136417] Component Batcher_0 stopped!
[2025-07-18 12:47:54,847][2136563] Weights refcount: 2 0
[2025-07-18 12:47:54,848][2136563] Stopping InferenceWorker_p0-w0...
[2025-07-18 12:47:54,848][2136563] Loop inference_proc0-0_evt_loop terminating...
[2025-07-18 12:47:54,848][2136417] Component InferenceWorker_p0-w0 stopped!
[2025-07-18 12:47:54,853][2136562] Stopping RolloutWorker_w1...
[2025-07-18 12:47:54,853][2136562] Loop rollout_proc1_evt_loop terminating...
[2025-07-18 12:47:54,853][2136417] Component RolloutWorker_w1 stopped!
[2025-07-18 12:47:54,855][2136561] Stopping RolloutWorker_w0...
[2025-07-18 12:47:54,855][2136561] Loop rollout_proc0_evt_loop terminating...
[2025-07-18 12:47:54,855][2136417] Component RolloutWorker_w0 stopped!
[2025-07-18 12:47:54,856][2136568] Stopping RolloutWorker_w6...
[2025-07-18 12:47:54,856][2136564] Stopping RolloutWorker_w2...
[2025-07-18 12:47:54,856][2136568] Loop rollout_proc6_evt_loop terminating...
[2025-07-18 12:47:54,856][2136564] Loop rollout_proc2_evt_loop terminating...
[2025-07-18 12:47:54,856][2136417] Component RolloutWorker_w6 stopped!
[2025-07-18 12:47:54,856][2136417] Component RolloutWorker_w2 stopped!
[2025-07-18 12:47:54,857][2136566] Stopping RolloutWorker_w4...
[2025-07-18 12:47:54,857][2136417] Component RolloutWorker_w4 stopped!
[2025-07-18 12:47:54,857][2136566] Loop rollout_proc4_evt_loop terminating...
[2025-07-18 12:47:54,857][2136567] Stopping RolloutWorker_w5...
[2025-07-18 12:47:54,857][2136567] Loop rollout_proc5_evt_loop terminating...
[2025-07-18 12:47:54,857][2136417] Component RolloutWorker_w5 stopped!
[2025-07-18 12:47:54,857][2136565] Stopping RolloutWorker_w3...
[2025-07-18 12:47:54,858][2136565] Loop rollout_proc3_evt_loop terminating...
[2025-07-18 12:47:54,857][2136417] Component RolloutWorker_w3 stopped!
[2025-07-18 12:47:54,858][2136569] Stopping RolloutWorker_w7...
[2025-07-18 12:47:54,858][2136569] Loop rollout_proc7_evt_loop terminating...
[2025-07-18 12:47:54,858][2136417] Component RolloutWorker_w7 stopped!
[2025-07-18 12:47:54,878][2136548] Saving /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 12:47:54,916][2136548] Stopping LearnerWorker_p0...
[2025-07-18 12:47:54,916][2136548] Loop learner_proc0_evt_loop terminating...
[2025-07-18 12:47:54,916][2136417] Component LearnerWorker_p0 stopped!
[2025-07-18 12:47:54,917][2136417] Waiting for process learner_proc0 to stop...
[2025-07-18 12:47:55,467][2136417] Waiting for process inference_proc0-0 to join...
[2025-07-18 12:47:55,467][2136417] Waiting for process rollout_proc0 to join...
[2025-07-18 12:47:55,468][2136417] Waiting for process rollout_proc1 to join...
[2025-07-18 12:47:55,468][2136417] Waiting for process rollout_proc2 to join...
[2025-07-18 12:47:55,469][2136417] Waiting for process rollout_proc3 to join...
[2025-07-18 12:47:55,469][2136417] Waiting for process rollout_proc4 to join...
[2025-07-18 12:47:55,469][2136417] Waiting for process rollout_proc5 to join...
[2025-07-18 12:47:55,469][2136417] Waiting for process rollout_proc6 to join...
[2025-07-18 12:47:55,470][2136417] Waiting for process rollout_proc7 to join...
[2025-07-18 12:47:55,470][2136417] Batcher 0 profile tree view:
batching: 4.7478, releasing_batches: 0.0098
[2025-07-18 12:47:55,470][2136417] InferenceWorker_p0-w0 profile tree view:
wait_policy: 0.0000
  wait_policy_total: 1.2940
update_model: 0.8893
  weight_update: 0.0003
one_step: 0.0008
  handle_policy_step: 56.6067
    deserialize: 2.4502, stack: 0.2775, obs_to_device_normalize: 15.0850, forward: 23.1996, send_messages: 3.6534
    prepare_outputs: 9.6522
      to_cpu: 6.8335
[2025-07-18 12:47:55,470][2136417] Learner 0 profile tree view:
misc: 0.0025, prepare_batch: 2.9513
train: 7.7061
  epoch_init: 0.0025, minibatch_init: 0.0025, losses_postprocess: 0.1622, kl_divergence: 0.1635, after_optimizer: 1.6533
  calculate_losses: 3.1045
    losses_init: 0.0013, forward_head: 0.2355, bptt_initial: 1.7794, tail: 0.2166, advantages_returns: 0.0528, losses: 0.4164
    bptt: 0.3456
      bptt_forward_core: 0.3304
  update: 2.4756
    clip: 0.2778
[2025-07-18 12:47:55,471][2136417] RolloutWorker_w0 profile tree view:
wait_for_trajectories: 0.0491, enqueue_policy_requests: 2.2667, env_step: 32.5037, overhead: 1.6134, complete_rollouts: 0.0699
save_policy_outputs: 2.7974
  split_output_tensors: 0.9545
[2025-07-18 12:47:55,471][2136417] RolloutWorker_w7 profile tree view:
wait_for_trajectories: 0.0471, enqueue_policy_requests: 2.2690, env_step: 33.9541, overhead: 1.5804, complete_rollouts: 0.0706
save_policy_outputs: 2.7424
  split_output_tensors: 0.9435
[2025-07-18 12:47:55,471][2136417] Loop Runner_EvtLoop terminating...
[2025-07-18 12:47:55,471][2136417] Runner profile tree view:
main_loop: 64.1530
[2025-07-18 12:47:55,471][2136417] Collected {0: 4005888}, FPS: 62442.7
[2025-07-18 12:47:55,535][2136417] Could not query the git revision for the logs, perhaps git is not available
[2025-07-18 12:47:55,535][2136417] Loading existing experiment configuration from /home/isaacp/repos/rl_class/train_dir/default_experiment/config.json
[2025-07-18 12:47:55,536][2136417] Overriding arg 'num_workers' with value 1 passed from command line
[2025-07-18 12:47:55,536][2136417] Adding new argument 'no_render'=True that is not in the saved config file!
[2025-07-18 12:47:55,536][2136417] Adding new argument 'save_video'=True that is not in the saved config file!
[2025-07-18 12:47:55,536][2136417] Adding new argument 'video_frames'=1000000000.0 that is not in the saved config file!
[2025-07-18 12:47:55,536][2136417] Adding new argument 'video_name'=None that is not in the saved config file!
[2025-07-18 12:47:55,537][2136417] Adding new argument 'max_num_frames'=1000000000.0 that is not in the saved config file!
[2025-07-18 12:47:55,537][2136417] Adding new argument 'max_num_episodes'=10 that is not in the saved config file!
[2025-07-18 12:47:55,537][2136417] Adding new argument 'push_to_hub'=False that is not in the saved config file!
[2025-07-18 12:47:55,537][2136417] Adding new argument 'hf_repository'=None that is not in the saved config file!
[2025-07-18 12:47:55,538][2136417] Adding new argument 'policy_index'=0 that is not in the saved config file!
[2025-07-18 12:47:55,538][2136417] Adding new argument 'eval_deterministic'=False that is not in the saved config file!
[2025-07-18 12:47:55,538][2136417] Adding new argument 'train_script'=None that is not in the saved config file!
[2025-07-18 12:47:55,538][2136417] Adding new argument 'enjoy_script'=None that is not in the saved config file!
[2025-07-18 12:47:55,538][2136417] Using frameskip 1 and render_action_repeat=4 for evaluation
[2025-07-18 12:47:55,550][2136417] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 12:47:55,551][2136417] RunningMeanStd input shape: (3, 72, 128)
[2025-07-18 12:47:55,551][2136417] RunningMeanStd input shape: (1,)
[2025-07-18 12:47:55,556][2136417] ConvEncoder: input_channels=3
[2025-07-18 12:47:55,597][2136417] Conv encoder output size: 512
[2025-07-18 12:47:55,597][2136417] Policy head output size: 512
[2025-07-18 12:47:55,693][2136417] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 12:47:55,695][2136417] Could not load from checkpoint, attempt 0
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.10/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device)
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.10/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 12:47:55,696][2136417] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 12:47:55,696][2136417] Could not load from checkpoint, attempt 1
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.10/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device)
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.10/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 12:47:55,697][2136417] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 12:47:55,697][2136417] Could not load from checkpoint, attempt 2
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.10/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device)
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.10/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 12:56:29,978][2140324] Saving configuration to /home/isaacp/repos/rl_class/train_dir/default_experiment/config.json...
[2025-07-18 12:56:29,980][2140324] Rollout worker 0 uses device cpu
[2025-07-18 12:56:29,980][2140324] Rollout worker 1 uses device cpu
[2025-07-18 12:56:29,980][2140324] Rollout worker 2 uses device cpu
[2025-07-18 12:56:29,980][2140324] Rollout worker 3 uses device cpu
[2025-07-18 12:56:29,980][2140324] Rollout worker 4 uses device cpu
[2025-07-18 12:56:29,981][2140324] Rollout worker 5 uses device cpu
[2025-07-18 12:56:29,981][2140324] Rollout worker 6 uses device cpu
[2025-07-18 12:56:29,981][2140324] Rollout worker 7 uses device cpu
[2025-07-18 12:56:30,007][2140324] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 12:56:30,007][2140324] InferenceWorker_p0-w0: min num requests: 2
[2025-07-18 12:56:30,029][2140324] Starting all processes...
[2025-07-18 12:56:30,029][2140324] Starting process learner_proc0
[2025-07-18 12:56:30,079][2140324] Starting all processes...
[2025-07-18 12:56:30,084][2140324] Starting process inference_proc0-0
[2025-07-18 12:56:30,084][2140324] Starting process rollout_proc0
[2025-07-18 12:56:30,084][2140324] Starting process rollout_proc1
[2025-07-18 12:56:30,085][2140324] Starting process rollout_proc2
[2025-07-18 12:56:30,085][2140324] Starting process rollout_proc3
[2025-07-18 12:56:30,086][2140324] Starting process rollout_proc4
[2025-07-18 12:56:30,087][2140324] Starting process rollout_proc5
[2025-07-18 12:56:30,087][2140324] Starting process rollout_proc6
[2025-07-18 12:56:30,087][2140324] Starting process rollout_proc7
[2025-07-18 12:56:30,932][2140505] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 12:56:30,932][2140505] Set environment var CUDA_VISIBLE_DEVICES to '0' (GPU indices [0]) for learning process 0
[2025-07-18 12:56:30,944][2140505] Num visible devices: 1
[2025-07-18 12:56:30,944][2140505] Starting seed is not provided
[2025-07-18 12:56:30,945][2140505] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 12:56:30,945][2140505] Initializing actor-critic model on device cuda:0
[2025-07-18 12:56:30,945][2140505] RunningMeanStd input shape: (3, 72, 128)
[2025-07-18 12:56:30,946][2140505] RunningMeanStd input shape: (1,)
[2025-07-18 12:56:30,951][2140505] ConvEncoder: input_channels=3
[2025-07-18 12:56:30,952][2140525] Worker 2 uses CPU cores [8, 9, 10, 11]
[2025-07-18 12:56:30,964][2140527] Worker 4 uses CPU cores [16, 17, 18, 19]
[2025-07-18 12:56:30,980][2140524] Worker 1 uses CPU cores [4, 5, 6, 7]
[2025-07-18 12:56:30,991][2140505] Conv encoder output size: 512
[2025-07-18 12:56:30,991][2140505] Policy head output size: 512
[2025-07-18 12:56:30,998][2140505] Created Actor Critic model with architecture:
[2025-07-18 12:56:30,998][2140505] ActorCriticSharedWeights(
  (obs_normalizer): ObservationNormalizer(
    (running_mean_std): RunningMeanStdDictInPlace(
      (running_mean_std): ModuleDict(
        (obs): RunningMeanStdInPlace()
      )
    )
  )
  (returns_normalizer): RecursiveScriptModule(original_name=RunningMeanStdInPlace)
  (encoder): VizdoomEncoder(
    (basic_encoder): ConvEncoder(
      (enc): RecursiveScriptModule(
        original_name=ConvEncoderImpl
        (conv_head): RecursiveScriptModule(
          original_name=Sequential
          (0): RecursiveScriptModule(original_name=Conv2d)
          (1): RecursiveScriptModule(original_name=ELU)
          (2): RecursiveScriptModule(original_name=Conv2d)
          (3): RecursiveScriptModule(original_name=ELU)
          (4): RecursiveScriptModule(original_name=Conv2d)
          (5): RecursiveScriptModule(original_name=ELU)
        )
        (mlp_layers): RecursiveScriptModule(
          original_name=Sequential
          (0): RecursiveScriptModule(original_name=Linear)
          (1): RecursiveScriptModule(original_name=ELU)
        )
      )
    )
  )
  (core): ModelCoreRNN(
    (core): GRU(512, 512)
  )
  (decoder): MlpDecoder(
    (mlp): Identity()
  )
  (critic_linear): Linear(in_features=512, out_features=1, bias=True)
  (action_parameterization): ActionParameterizationDefault(
    (distribution_linear): Linear(in_features=512, out_features=5, bias=True)
  )
)
[2025-07-18 12:56:31,003][2140526] Worker 3 uses CPU cores [12, 13, 14, 15]
[2025-07-18 12:56:31,012][2140523] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 12:56:31,012][2140523] Set environment var CUDA_VISIBLE_DEVICES to '0' (GPU indices [0]) for inference process 0
[2025-07-18 12:56:31,013][2140522] Worker 0 uses CPU cores [0, 1, 2, 3]
[2025-07-18 12:56:31,029][2140523] Num visible devices: 1
[2025-07-18 12:56:31,032][2140528] Worker 6 uses CPU cores [24, 25, 26, 27]
[2025-07-18 12:56:31,037][2140530] Worker 7 uses CPU cores [28, 29, 30, 31]
[2025-07-18 12:56:31,040][2140529] Worker 5 uses CPU cores [20, 21, 22, 23]
[2025-07-18 12:56:31,086][2140505] Using optimizer <class 'torch.optim.adam.Adam'>
[2025-07-18 12:56:31,611][2140505] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 12:56:31,612][2140505] Could not load from checkpoint, attempt 0
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 12:56:31,613][2140505] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 12:56:31,613][2140505] Could not load from checkpoint, attempt 1
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 12:56:31,613][2140505] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 12:56:31,614][2140505] Could not load from checkpoint, attempt 2
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 12:56:31,614][2140505] Did not load from checkpoint, starting from scratch!
[2025-07-18 12:56:31,614][2140505] Initialized policy 0 weights for model version 0
[2025-07-18 12:56:31,615][2140505] LearnerWorker_p0 finished initialization!
[2025-07-18 12:56:31,615][2140505] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 12:56:31,663][2140523] RunningMeanStd input shape: (3, 72, 128)
[2025-07-18 12:56:31,664][2140523] RunningMeanStd input shape: (1,)
[2025-07-18 12:56:31,669][2140523] ConvEncoder: input_channels=3
[2025-07-18 12:56:31,707][2140523] Conv encoder output size: 512
[2025-07-18 12:56:31,707][2140523] Policy head output size: 512
[2025-07-18 12:56:31,726][2140324] Inference worker 0-0 is ready!
[2025-07-18 12:56:31,726][2140324] All inference workers are ready! Signal rollout workers to start!
[2025-07-18 12:56:31,742][2140524] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 12:56:31,743][2140527] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 12:56:31,743][2140529] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 12:56:31,743][2140526] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 12:56:31,743][2140525] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 12:56:31,743][2140530] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 12:56:31,743][2140528] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 12:56:31,743][2140522] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 12:56:31,883][2140529] Decorrelating experience for 0 frames...
[2025-07-18 12:56:31,884][2140524] Decorrelating experience for 0 frames...
[2025-07-18 12:56:31,884][2140527] Decorrelating experience for 0 frames...
[2025-07-18 12:56:31,885][2140522] Decorrelating experience for 0 frames...
[2025-07-18 12:56:31,889][2140528] Decorrelating experience for 0 frames...
[2025-07-18 12:56:31,889][2140526] Decorrelating experience for 0 frames...
[2025-07-18 12:56:32,010][2140527] Decorrelating experience for 32 frames...
[2025-07-18 12:56:32,015][2140522] Decorrelating experience for 32 frames...
[2025-07-18 12:56:32,017][2140526] Decorrelating experience for 32 frames...
[2025-07-18 12:56:32,017][2140528] Decorrelating experience for 32 frames...
[2025-07-18 12:56:32,034][2140525] Decorrelating experience for 0 frames...
[2025-07-18 12:56:32,034][2140530] Decorrelating experience for 0 frames...
[2025-07-18 12:56:32,166][2140529] Decorrelating experience for 32 frames...
[2025-07-18 12:56:32,166][2140530] Decorrelating experience for 32 frames...
[2025-07-18 12:56:32,167][2140525] Decorrelating experience for 32 frames...
[2025-07-18 12:56:32,167][2140527] Decorrelating experience for 64 frames...
[2025-07-18 12:56:32,173][2140522] Decorrelating experience for 64 frames...
[2025-07-18 12:56:32,182][2140526] Decorrelating experience for 64 frames...
[2025-07-18 12:56:32,304][2140527] Decorrelating experience for 96 frames...
[2025-07-18 12:56:32,308][2140524] Decorrelating experience for 32 frames...
[2025-07-18 12:56:32,312][2140522] Decorrelating experience for 96 frames...
[2025-07-18 12:56:32,321][2140529] Decorrelating experience for 64 frames...
[2025-07-18 12:56:32,326][2140526] Decorrelating experience for 96 frames...
[2025-07-18 12:56:32,460][2140529] Decorrelating experience for 96 frames...
[2025-07-18 12:56:32,467][2140524] Decorrelating experience for 64 frames...
[2025-07-18 12:56:32,475][2140525] Decorrelating experience for 64 frames...
[2025-07-18 12:56:32,613][2140524] Decorrelating experience for 96 frames...
[2025-07-18 12:56:32,619][2140525] Decorrelating experience for 96 frames...
[2025-07-18 12:56:32,624][2140528] Decorrelating experience for 64 frames...
[2025-07-18 12:56:32,629][2140530] Decorrelating experience for 64 frames...
[2025-07-18 12:56:32,773][2140530] Decorrelating experience for 96 frames...
[2025-07-18 12:56:32,774][2140528] Decorrelating experience for 96 frames...
[2025-07-18 12:56:32,799][2140505] Signal inference workers to stop experience collection...
[2025-07-18 12:56:32,801][2140523] InferenceWorker_p0-w0: stopping experience collection
[2025-07-18 12:56:33,261][2140324] Fps is (10 sec: nan, 60 sec: nan, 300 sec: nan). Total num frames: 0. Throughput: 0: nan. Samples: 0. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[2025-07-18 12:56:33,262][2140324] Avg episode reward: [(0, '2.854')]
[2025-07-18 12:56:33,280][2140505] Signal inference workers to resume experience collection...
[2025-07-18 12:56:33,280][2140523] InferenceWorker_p0-w0: resuming experience collection
[2025-07-18 12:56:33,843][2140523] Updated weights for policy 0, policy_version 10 (0.0032)
[2025-07-18 12:56:34,476][2140523] Updated weights for policy 0, policy_version 20 (0.0004)
[2025-07-18 12:56:35,115][2140523] Updated weights for policy 0, policy_version 30 (0.0003)
[2025-07-18 12:56:35,729][2140523] Updated weights for policy 0, policy_version 40 (0.0003)
[2025-07-18 12:56:36,361][2140523] Updated weights for policy 0, policy_version 50 (0.0003)
[2025-07-18 12:56:36,980][2140523] Updated weights for policy 0, policy_version 60 (0.0003)
[2025-07-18 12:56:37,608][2140523] Updated weights for policy 0, policy_version 70 (0.0003)
[2025-07-18 12:56:38,212][2140523] Updated weights for policy 0, policy_version 80 (0.0003)
[2025-07-18 12:56:38,261][2140324] Fps is (10 sec: 65536.4, 60 sec: 65536.4, 300 sec: 65536.4). Total num frames: 327680. Throughput: 0: 14888.9. Samples: 74444. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)
[2025-07-18 12:56:38,262][2140324] Avg episode reward: [(0, '4.597')]
[2025-07-18 12:56:38,272][2140505] Saving new best policy, reward=4.597!
[2025-07-18 12:56:38,855][2140523] Updated weights for policy 0, policy_version 90 (0.0004)
[2025-07-18 12:56:39,472][2140523] Updated weights for policy 0, policy_version 100 (0.0003)
[2025-07-18 12:56:40,109][2140523] Updated weights for policy 0, policy_version 110 (0.0003)
[2025-07-18 12:56:40,731][2140523] Updated weights for policy 0, policy_version 120 (0.0003)
[2025-07-18 12:56:41,340][2140523] Updated weights for policy 0, policy_version 130 (0.0003)
[2025-07-18 12:56:41,936][2140523] Updated weights for policy 0, policy_version 140 (0.0003)
[2025-07-18 12:56:42,553][2140523] Updated weights for policy 0, policy_version 150 (0.0003)
[2025-07-18 12:56:43,173][2140523] Updated weights for policy 0, policy_version 160 (0.0003)
[2025-07-18 12:56:43,261][2140324] Fps is (10 sec: 65946.0, 60 sec: 65946.0, 300 sec: 65946.0). Total num frames: 659456. Throughput: 0: 12355.3. Samples: 123552. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2025-07-18 12:56:43,262][2140324] Avg episode reward: [(0, '4.455')]
[2025-07-18 12:56:43,776][2140523] Updated weights for policy 0, policy_version 170 (0.0003)
[2025-07-18 12:56:44,398][2140523] Updated weights for policy 0, policy_version 180 (0.0003)
[2025-07-18 12:56:45,027][2140523] Updated weights for policy 0, policy_version 190 (0.0003)
[2025-07-18 12:56:45,650][2140523] Updated weights for policy 0, policy_version 200 (0.0003)
[2025-07-18 12:56:46,260][2140523] Updated weights for policy 0, policy_version 210 (0.0004)
[2025-07-18 12:56:46,894][2140523] Updated weights for policy 0, policy_version 220 (0.0004)
[2025-07-18 12:56:47,548][2140523] Updated weights for policy 0, policy_version 230 (0.0004)
[2025-07-18 12:56:48,181][2140523] Updated weights for policy 0, policy_version 240 (0.0003)
[2025-07-18 12:56:48,261][2140324] Fps is (10 sec: 65945.7, 60 sec: 65809.3, 300 sec: 65809.3). Total num frames: 987136. Throughput: 0: 14874.4. Samples: 223116. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2025-07-18 12:56:48,262][2140324] Avg episode reward: [(0, '4.714')]
[2025-07-18 12:56:48,263][2140505] Saving new best policy, reward=4.714!
[2025-07-18 12:56:48,808][2140523] Updated weights for policy 0, policy_version 250 (0.0003)
[2025-07-18 12:56:49,415][2140523] Updated weights for policy 0, policy_version 260 (0.0003)
[2025-07-18 12:56:50,004][2140324] Heartbeat connected on LearnerWorker_p0
[2025-07-18 12:56:50,006][2140324] Heartbeat connected on Batcher_0
[2025-07-18 12:56:50,010][2140324] Heartbeat connected on RolloutWorker_w0
[2025-07-18 12:56:50,011][2140324] Heartbeat connected on InferenceWorker_p0-w0
[2025-07-18 12:56:50,013][2140324] Heartbeat connected on RolloutWorker_w1
[2025-07-18 12:56:50,016][2140324] Heartbeat connected on RolloutWorker_w2
[2025-07-18 12:56:50,017][2140523] Updated weights for policy 0, policy_version 270 (0.0003)
[2025-07-18 12:56:50,019][2140324] Heartbeat connected on RolloutWorker_w3
[2025-07-18 12:56:50,021][2140324] Heartbeat connected on RolloutWorker_w4
[2025-07-18 12:56:50,023][2140324] Heartbeat connected on RolloutWorker_w5
[2025-07-18 12:56:50,027][2140324] Heartbeat connected on RolloutWorker_w6
[2025-07-18 12:56:50,028][2140324] Heartbeat connected on RolloutWorker_w7
[2025-07-18 12:56:50,646][2140523] Updated weights for policy 0, policy_version 280 (0.0003)
[2025-07-18 12:56:51,264][2140523] Updated weights for policy 0, policy_version 290 (0.0003)
[2025-07-18 12:56:51,873][2140523] Updated weights for policy 0, policy_version 300 (0.0003)
[2025-07-18 12:56:52,474][2140523] Updated weights for policy 0, policy_version 310 (0.0003)
[2025-07-18 12:56:53,100][2140523] Updated weights for policy 0, policy_version 320 (0.0003)
[2025-07-18 12:56:53,261][2140324] Fps is (10 sec: 65945.7, 60 sec: 65945.8, 300 sec: 65945.8). Total num frames: 1318912. Throughput: 0: 16119.7. Samples: 322392. Policy #0 lag: (min: 0.0, avg: 0.6, max: 1.0)
[2025-07-18 12:56:53,262][2140324] Avg episode reward: [(0, '4.825')]
[2025-07-18 12:56:53,262][2140505] Saving new best policy, reward=4.825!
[2025-07-18 12:56:53,723][2140523] Updated weights for policy 0, policy_version 330 (0.0003)
[2025-07-18 12:56:54,322][2140523] Updated weights for policy 0, policy_version 340 (0.0003)
[2025-07-18 12:56:54,942][2140523] Updated weights for policy 0, policy_version 350 (0.0003)
[2025-07-18 12:56:55,566][2140523] Updated weights for policy 0, policy_version 360 (0.0004)
[2025-07-18 12:56:56,166][2140523] Updated weights for policy 0, policy_version 370 (0.0003)
[2025-07-18 12:56:56,765][2140523] Updated weights for policy 0, policy_version 380 (0.0003)
[2025-07-18 12:56:57,403][2140523] Updated weights for policy 0, policy_version 390 (0.0003)
[2025-07-18 12:56:58,025][2140523] Updated weights for policy 0, policy_version 400 (0.0003)
[2025-07-18 12:56:58,261][2140324] Fps is (10 sec: 66765.0, 60 sec: 66191.6, 300 sec: 66191.6). Total num frames: 1654784. Throughput: 0: 14884.0. Samples: 372100. Policy #0 lag: (min: 0.0, avg: 0.6, max: 1.0)
[2025-07-18 12:56:58,262][2140324] Avg episode reward: [(0, '4.941')]
[2025-07-18 12:56:58,263][2140505] Saving new best policy, reward=4.941!
[2025-07-18 12:56:58,622][2140523] Updated weights for policy 0, policy_version 410 (0.0003)
[2025-07-18 12:56:59,223][2140523] Updated weights for policy 0, policy_version 420 (0.0003)
[2025-07-18 12:56:59,848][2140523] Updated weights for policy 0, policy_version 430 (0.0003)
[2025-07-18 12:57:00,446][2140523] Updated weights for policy 0, policy_version 440 (0.0003)
[2025-07-18 12:57:01,073][2140523] Updated weights for policy 0, policy_version 450 (0.0003)
[2025-07-18 12:57:01,682][2140523] Updated weights for policy 0, policy_version 460 (0.0003)
[2025-07-18 12:57:02,296][2140523] Updated weights for policy 0, policy_version 470 (0.0003)
[2025-07-18 12:57:02,921][2140523] Updated weights for policy 0, policy_version 480 (0.0003)
[2025-07-18 12:57:03,261][2140324] Fps is (10 sec: 66764.8, 60 sec: 66218.8, 300 sec: 66218.8). Total num frames: 1986560. Throughput: 0: 15755.8. Samples: 472674. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)
[2025-07-18 12:57:03,262][2140324] Avg episode reward: [(0, '5.061')]
[2025-07-18 12:57:03,262][2140505] Saving new best policy, reward=5.061!
[2025-07-18 12:57:03,540][2140523] Updated weights for policy 0, policy_version 490 (0.0003)
[2025-07-18 12:57:04,167][2140523] Updated weights for policy 0, policy_version 500 (0.0003)
[2025-07-18 12:57:04,784][2140523] Updated weights for policy 0, policy_version 510 (0.0003)
[2025-07-18 12:57:05,413][2140523] Updated weights for policy 0, policy_version 520 (0.0004)
[2025-07-18 12:57:06,047][2140523] Updated weights for policy 0, policy_version 530 (0.0003)
[2025-07-18 12:57:06,665][2140523] Updated weights for policy 0, policy_version 540 (0.0003)
[2025-07-18 12:57:07,278][2140523] Updated weights for policy 0, policy_version 550 (0.0003)
[2025-07-18 12:57:07,882][2140523] Updated weights for policy 0, policy_version 560 (0.0003)
[2025-07-18 12:57:08,261][2140324] Fps is (10 sec: 66354.9, 60 sec: 66238.2, 300 sec: 66238.2). Total num frames: 2318336. Throughput: 0: 16333.8. Samples: 571684. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2025-07-18 12:57:08,262][2140324] Avg episode reward: [(0, '5.486')]
[2025-07-18 12:57:08,263][2140505] Saving new best policy, reward=5.486!
[2025-07-18 12:57:08,488][2140523] Updated weights for policy 0, policy_version 570 (0.0003)
[2025-07-18 12:57:09,115][2140523] Updated weights for policy 0, policy_version 580 (0.0003)
[2025-07-18 12:57:09,721][2140523] Updated weights for policy 0, policy_version 590 (0.0003)
[2025-07-18 12:57:10,333][2140523] Updated weights for policy 0, policy_version 600 (0.0003)
[2025-07-18 12:57:10,959][2140523] Updated weights for policy 0, policy_version 610 (0.0003)
[2025-07-18 12:57:11,589][2140523] Updated weights for policy 0, policy_version 620 (0.0003)
[2025-07-18 12:57:12,200][2140523] Updated weights for policy 0, policy_version 630 (0.0003)
[2025-07-18 12:57:12,829][2140523] Updated weights for policy 0, policy_version 640 (0.0003)
[2025-07-18 12:57:13,261][2140324] Fps is (10 sec: 66355.2, 60 sec: 66252.9, 300 sec: 66252.9). Total num frames: 2650112. Throughput: 0: 15540.2. Samples: 621608. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)
[2025-07-18 12:57:13,262][2140324] Avg episode reward: [(0, '6.164')]
[2025-07-18 12:57:13,262][2140505] Saving new best policy, reward=6.164!
[2025-07-18 12:57:13,424][2140523] Updated weights for policy 0, policy_version 650 (0.0003)
[2025-07-18 12:57:14,039][2140523] Updated weights for policy 0, policy_version 660 (0.0003)
[2025-07-18 12:57:14,674][2140523] Updated weights for policy 0, policy_version 670 (0.0004)
[2025-07-18 12:57:15,290][2140523] Updated weights for policy 0, policy_version 680 (0.0004)
[2025-07-18 12:57:15,915][2140523] Updated weights for policy 0, policy_version 690 (0.0003)
[2025-07-18 12:57:16,526][2140523] Updated weights for policy 0, policy_version 700 (0.0003)
[2025-07-18 12:57:17,132][2140523] Updated weights for policy 0, policy_version 710 (0.0003)
[2025-07-18 12:57:17,762][2140523] Updated weights for policy 0, policy_version 720 (0.0003)
[2025-07-18 12:57:18,261][2140324] Fps is (10 sec: 66355.5, 60 sec: 66264.3, 300 sec: 66264.3). Total num frames: 2981888. Throughput: 0: 16021.2. Samples: 720954. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)
[2025-07-18 12:57:18,262][2140324] Avg episode reward: [(0, '7.786')]
[2025-07-18 12:57:18,263][2140505] Saving new best policy, reward=7.786!
[2025-07-18 12:57:18,376][2140523] Updated weights for policy 0, policy_version 730 (0.0003)
[2025-07-18 12:57:19,006][2140523] Updated weights for policy 0, policy_version 740 (0.0004)
[2025-07-18 12:57:19,624][2140523] Updated weights for policy 0, policy_version 750 (0.0003)
[2025-07-18 12:57:20,230][2140523] Updated weights for policy 0, policy_version 760 (0.0003)
[2025-07-18 12:57:20,814][2140523] Updated weights for policy 0, policy_version 770 (0.0003)
[2025-07-18 12:57:21,431][2140523] Updated weights for policy 0, policy_version 780 (0.0003)
[2025-07-18 12:57:22,049][2140523] Updated weights for policy 0, policy_version 790 (0.0003)
[2025-07-18 12:57:22,662][2140523] Updated weights for policy 0, policy_version 800 (0.0004)
[2025-07-18 12:57:23,261][2140324] Fps is (10 sec: 66354.9, 60 sec: 66273.3, 300 sec: 66273.3). Total num frames: 3313664. Throughput: 0: 16589.1. Samples: 820954. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2025-07-18 12:57:23,262][2140324] Avg episode reward: [(0, '8.243')]
[2025-07-18 12:57:23,263][2140505] Saving new best policy, reward=8.243!
[2025-07-18 12:57:23,309][2140523] Updated weights for policy 0, policy_version 810 (0.0003)
[2025-07-18 12:57:23,911][2140523] Updated weights for policy 0, policy_version 820 (0.0004)
[2025-07-18 12:57:24,526][2140523] Updated weights for policy 0, policy_version 830 (0.0003)
[2025-07-18 12:57:25,152][2140523] Updated weights for policy 0, policy_version 840 (0.0004)
[2025-07-18 12:57:25,781][2140523] Updated weights for policy 0, policy_version 850 (0.0003)
[2025-07-18 12:57:26,394][2140523] Updated weights for policy 0, policy_version 860 (0.0003)
[2025-07-18 12:57:27,012][2140523] Updated weights for policy 0, policy_version 870 (0.0003)
[2025-07-18 12:57:27,628][2140523] Updated weights for policy 0, policy_version 880 (0.0004)
[2025-07-18 12:57:28,237][2140523] Updated weights for policy 0, policy_version 890 (0.0003)
[2025-07-18 12:57:28,261][2140324] Fps is (10 sec: 66355.1, 60 sec: 66280.8, 300 sec: 66280.8). Total num frames: 3645440. Throughput: 0: 16591.1. Samples: 870152. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2025-07-18 12:57:28,262][2140324] Avg episode reward: [(0, '11.546')]
[2025-07-18 12:57:28,263][2140505] Saving new best policy, reward=11.546!
[2025-07-18 12:57:28,838][2140523] Updated weights for policy 0, policy_version 900 (0.0003)
[2025-07-18 12:57:29,446][2140523] Updated weights for policy 0, policy_version 910 (0.0003)
[2025-07-18 12:57:30,055][2140523] Updated weights for policy 0, policy_version 920 (0.0003)
[2025-07-18 12:57:30,644][2140523] Updated weights for policy 0, policy_version 930 (0.0003)
[2025-07-18 12:57:31,235][2140523] Updated weights for policy 0, policy_version 940 (0.0003)
[2025-07-18 12:57:31,853][2140523] Updated weights for policy 0, policy_version 950 (0.0003)
[2025-07-18 12:57:32,444][2140523] Updated weights for policy 0, policy_version 960 (0.0003)
[2025-07-18 12:57:33,049][2140523] Updated weights for policy 0, policy_version 970 (0.0003)
[2025-07-18 12:57:33,261][2140324] Fps is (10 sec: 67174.6, 60 sec: 66423.5, 300 sec: 66423.5). Total num frames: 3985408. Throughput: 0: 16632.4. Samples: 971576. Policy #0 lag: (min: 0.0, avg: 0.6, max: 1.0)
[2025-07-18 12:57:33,262][2140324] Avg episode reward: [(0, '16.329')]
[2025-07-18 12:57:33,262][2140505] Saving new best policy, reward=16.329!
[2025-07-18 12:57:33,552][2140505] Stopping Batcher_0...
[2025-07-18 12:57:33,552][2140505] Saving /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 12:57:33,552][2140324] Component Batcher_0 stopped!
[2025-07-18 12:57:33,553][2140505] Loop batcher_evt_loop terminating...
[2025-07-18 12:57:33,567][2140523] Weights refcount: 2 0
[2025-07-18 12:57:33,568][2140523] Stopping InferenceWorker_p0-w0...
[2025-07-18 12:57:33,568][2140523] Loop inference_proc0-0_evt_loop terminating...
[2025-07-18 12:57:33,568][2140324] Component InferenceWorker_p0-w0 stopped!
[2025-07-18 12:57:33,571][2140528] Stopping RolloutWorker_w6...
[2025-07-18 12:57:33,572][2140530] Stopping RolloutWorker_w7...
[2025-07-18 12:57:33,572][2140528] Loop rollout_proc6_evt_loop terminating...
[2025-07-18 12:57:33,572][2140530] Loop rollout_proc7_evt_loop terminating...
[2025-07-18 12:57:33,571][2140324] Component RolloutWorker_w6 stopped!
[2025-07-18 12:57:33,572][2140526] Stopping RolloutWorker_w3...
[2025-07-18 12:57:33,572][2140526] Loop rollout_proc3_evt_loop terminating...
[2025-07-18 12:57:33,572][2140324] Component RolloutWorker_w7 stopped!
[2025-07-18 12:57:33,572][2140324] Component RolloutWorker_w3 stopped!
[2025-07-18 12:57:33,573][2140525] Stopping RolloutWorker_w2...
[2025-07-18 12:57:33,573][2140324] Component RolloutWorker_w2 stopped!
[2025-07-18 12:57:33,573][2140525] Loop rollout_proc2_evt_loop terminating...
[2025-07-18 12:57:33,574][2140527] Stopping RolloutWorker_w4...
[2025-07-18 12:57:33,574][2140324] Component RolloutWorker_w4 stopped!
[2025-07-18 12:57:33,574][2140527] Loop rollout_proc4_evt_loop terminating...
[2025-07-18 12:57:33,574][2140522] Stopping RolloutWorker_w0...
[2025-07-18 12:57:33,574][2140324] Component RolloutWorker_w0 stopped!
[2025-07-18 12:57:33,575][2140522] Loop rollout_proc0_evt_loop terminating...
[2025-07-18 12:57:33,575][2140529] Stopping RolloutWorker_w5...
[2025-07-18 12:57:33,575][2140324] Component RolloutWorker_w5 stopped!
[2025-07-18 12:57:33,575][2140529] Loop rollout_proc5_evt_loop terminating...
[2025-07-18 12:57:33,576][2140524] Stopping RolloutWorker_w1...
[2025-07-18 12:57:33,576][2140524] Loop rollout_proc1_evt_loop terminating...
[2025-07-18 12:57:33,576][2140324] Component RolloutWorker_w1 stopped!
[2025-07-18 12:57:33,598][2140505] Saving /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 12:57:33,639][2140505] Stopping LearnerWorker_p0...
[2025-07-18 12:57:33,639][2140505] Loop learner_proc0_evt_loop terminating...
[2025-07-18 12:57:33,639][2140324] Component LearnerWorker_p0 stopped!
[2025-07-18 12:57:33,640][2140324] Waiting for process learner_proc0 to stop...
[2025-07-18 12:57:34,187][2140324] Waiting for process inference_proc0-0 to join...
[2025-07-18 12:57:34,188][2140324] Waiting for process rollout_proc0 to join...
[2025-07-18 12:57:34,188][2140324] Waiting for process rollout_proc1 to join...
[2025-07-18 12:57:34,188][2140324] Waiting for process rollout_proc2 to join...
[2025-07-18 12:57:34,189][2140324] Waiting for process rollout_proc3 to join...
[2025-07-18 12:57:34,189][2140324] Waiting for process rollout_proc4 to join...
[2025-07-18 12:57:34,189][2140324] Waiting for process rollout_proc5 to join...
[2025-07-18 12:57:34,189][2140324] Waiting for process rollout_proc6 to join...
[2025-07-18 12:57:34,190][2140324] Waiting for process rollout_proc7 to join...
[2025-07-18 12:57:34,190][2140324] Batcher 0 profile tree view:
batching: 4.6287, releasing_batches: 0.0089
[2025-07-18 12:57:34,190][2140324] InferenceWorker_p0-w0 profile tree view:
wait_policy: 0.0000
  wait_policy_total: 1.2627
update_model: 0.8751
  weight_update: 0.0003
one_step: 0.0009
  handle_policy_step: 56.8033
    deserialize: 2.3923, stack: 0.2727, obs_to_device_normalize: 15.0130, forward: 23.4659, send_messages: 3.7572
    prepare_outputs: 9.8318
      to_cpu: 7.0231
[2025-07-18 12:57:34,190][2140324] Learner 0 profile tree view:
misc: 0.0022, prepare_batch: 2.9256
train: 7.7072
  epoch_init: 0.0018, minibatch_init: 0.0020, losses_postprocess: 0.1740, kl_divergence: 0.1501, after_optimizer: 1.6909
  calculate_losses: 3.1508
    losses_init: 0.0010, forward_head: 0.2263, bptt_initial: 1.8836, tail: 0.2037, advantages_returns: 0.0519, losses: 0.4025
    bptt: 0.3336
      bptt_forward_core: 0.3196
  update: 2.4136
    clip: 0.2651
[2025-07-18 12:57:34,191][2140324] RolloutWorker_w0 profile tree view:
wait_for_trajectories: 0.0438, enqueue_policy_requests: 2.1859, env_step: 33.5719, overhead: 1.5067, complete_rollouts: 0.0637
save_policy_outputs: 2.5494
  split_output_tensors: 0.8605
[2025-07-18 12:57:34,191][2140324] RolloutWorker_w7 profile tree view:
wait_for_trajectories: 0.0430, enqueue_policy_requests: 2.2126, env_step: 33.3204, overhead: 1.4953, complete_rollouts: 0.0634
save_policy_outputs: 2.5875
  split_output_tensors: 0.8739
[2025-07-18 12:57:34,191][2140324] Loop Runner_EvtLoop terminating...
[2025-07-18 12:57:34,191][2140324] Runner profile tree view:
main_loop: 64.1629
[2025-07-18 12:57:34,192][2140324] Collected {0: 4005888}, FPS: 62433.1
[2025-07-18 12:57:34,262][2140324] Could not query the git revision for the logs, perhaps git is not available
[2025-07-18 12:57:34,262][2140324] Loading existing experiment configuration from /home/isaacp/repos/rl_class/train_dir/default_experiment/config.json
[2025-07-18 12:57:34,263][2140324] Overriding arg 'num_workers' with value 1 passed from command line
[2025-07-18 12:57:34,263][2140324] Adding new argument 'no_render'=True that is not in the saved config file!
[2025-07-18 12:57:34,263][2140324] Adding new argument 'save_video'=True that is not in the saved config file!
[2025-07-18 12:57:34,263][2140324] Adding new argument 'video_frames'=1000000000.0 that is not in the saved config file!
[2025-07-18 12:57:34,263][2140324] Adding new argument 'video_name'=None that is not in the saved config file!
[2025-07-18 12:57:34,264][2140324] Adding new argument 'max_num_frames'=1000000000.0 that is not in the saved config file!
[2025-07-18 12:57:34,264][2140324] Adding new argument 'max_num_episodes'=10 that is not in the saved config file!
[2025-07-18 12:57:34,264][2140324] Adding new argument 'push_to_hub'=False that is not in the saved config file!
[2025-07-18 12:57:34,264][2140324] Adding new argument 'hf_repository'=None that is not in the saved config file!
[2025-07-18 12:57:34,264][2140324] Adding new argument 'policy_index'=0 that is not in the saved config file!
[2025-07-18 12:57:34,265][2140324] Adding new argument 'eval_deterministic'=False that is not in the saved config file!
[2025-07-18 12:57:34,265][2140324] Adding new argument 'train_script'=None that is not in the saved config file!
[2025-07-18 12:57:34,265][2140324] Adding new argument 'enjoy_script'=None that is not in the saved config file!
[2025-07-18 12:57:34,265][2140324] Using frameskip 1 and render_action_repeat=4 for evaluation
[2025-07-18 12:57:34,277][2140324] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 12:57:34,278][2140324] RunningMeanStd input shape: (3, 72, 128)
[2025-07-18 12:57:34,279][2140324] RunningMeanStd input shape: (1,)
[2025-07-18 12:57:34,284][2140324] ConvEncoder: input_channels=3
[2025-07-18 12:57:34,321][2140324] Conv encoder output size: 512
[2025-07-18 12:57:34,322][2140324] Policy head output size: 512
[2025-07-18 12:57:34,418][2140324] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 12:57:34,420][2140324] Could not load from checkpoint, attempt 0
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 12:57:34,420][2140324] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 12:57:34,421][2140324] Could not load from checkpoint, attempt 1
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 12:57:34,421][2140324] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 12:57:34,422][2140324] Could not load from checkpoint, attempt 2
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 13:03:34,572][2140324] Could not query the git revision for the logs, perhaps git is not available
[2025-07-18 13:03:34,573][2140324] Loading existing experiment configuration from /home/isaacp/repos/rl_class/train_dir/default_experiment/config.json
[2025-07-18 13:03:34,573][2140324] Overriding arg 'num_workers' with value 1 passed from command line
[2025-07-18 13:03:34,573][2140324] Adding new argument 'no_render'=True that is not in the saved config file!
[2025-07-18 13:03:34,574][2140324] Adding new argument 'save_video'=True that is not in the saved config file!
[2025-07-18 13:03:34,574][2140324] Adding new argument 'video_frames'=1000000000.0 that is not in the saved config file!
[2025-07-18 13:03:34,574][2140324] Adding new argument 'video_name'=None that is not in the saved config file!
[2025-07-18 13:03:34,574][2140324] Adding new argument 'max_num_frames'=1000000000.0 that is not in the saved config file!
[2025-07-18 13:03:34,575][2140324] Adding new argument 'max_num_episodes'=10 that is not in the saved config file!
[2025-07-18 13:03:34,575][2140324] Adding new argument 'push_to_hub'=False that is not in the saved config file!
[2025-07-18 13:03:34,575][2140324] Adding new argument 'hf_repository'=None that is not in the saved config file!
[2025-07-18 13:03:34,575][2140324] Adding new argument 'policy_index'=0 that is not in the saved config file!
[2025-07-18 13:03:34,575][2140324] Adding new argument 'eval_deterministic'=False that is not in the saved config file!
[2025-07-18 13:03:34,576][2140324] Adding new argument 'train_script'=None that is not in the saved config file!
[2025-07-18 13:03:34,576][2140324] Adding new argument 'enjoy_script'=None that is not in the saved config file!
[2025-07-18 13:03:34,576][2140324] Using frameskip 1 and render_action_repeat=4 for evaluation
[2025-07-18 13:03:34,589][2140324] RunningMeanStd input shape: (3, 72, 128)
[2025-07-18 13:03:34,589][2140324] RunningMeanStd input shape: (1,)
[2025-07-18 13:03:34,593][2140324] ConvEncoder: input_channels=3
[2025-07-18 13:03:34,610][2140324] Conv encoder output size: 512
[2025-07-18 13:03:34,611][2140324] Policy head output size: 512
[2025-07-18 13:03:34,635][2140324] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 13:03:34,636][2140324] Could not load from checkpoint, attempt 0
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device, weights_only=False)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 13:03:34,637][2140324] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 13:03:34,637][2140324] Could not load from checkpoint, attempt 1
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device, weights_only=False)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 13:03:34,637][2140324] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 13:03:34,638][2140324] Could not load from checkpoint, attempt 2
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device, weights_only=False)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 13:04:56,042][2140324] Could not query the git revision for the logs, perhaps git is not available
[2025-07-18 13:04:56,042][2140324] Loading existing experiment configuration from /home/isaacp/repos/rl_class/train_dir/default_experiment/config.json
[2025-07-18 13:04:56,043][2140324] Overriding arg 'num_workers' with value 1 passed from command line
[2025-07-18 13:04:56,043][2140324] Adding new argument 'no_render'=True that is not in the saved config file!
[2025-07-18 13:04:56,043][2140324] Adding new argument 'save_video'=True that is not in the saved config file!
[2025-07-18 13:04:56,043][2140324] Adding new argument 'video_frames'=1000000000.0 that is not in the saved config file!
[2025-07-18 13:04:56,043][2140324] Adding new argument 'video_name'=None that is not in the saved config file!
[2025-07-18 13:04:56,044][2140324] Adding new argument 'max_num_frames'=1000000000.0 that is not in the saved config file!
[2025-07-18 13:04:56,044][2140324] Adding new argument 'max_num_episodes'=10 that is not in the saved config file!
[2025-07-18 13:04:56,044][2140324] Adding new argument 'push_to_hub'=False that is not in the saved config file!
[2025-07-18 13:04:56,044][2140324] Adding new argument 'hf_repository'=None that is not in the saved config file!
[2025-07-18 13:04:56,044][2140324] Adding new argument 'policy_index'=0 that is not in the saved config file!
[2025-07-18 13:04:56,045][2140324] Adding new argument 'eval_deterministic'=False that is not in the saved config file!
[2025-07-18 13:04:56,045][2140324] Adding new argument 'train_script'=None that is not in the saved config file!
[2025-07-18 13:04:56,045][2140324] Adding new argument 'enjoy_script'=None that is not in the saved config file!
[2025-07-18 13:04:56,045][2140324] Using frameskip 1 and render_action_repeat=4 for evaluation
[2025-07-18 13:04:56,057][2140324] RunningMeanStd input shape: (3, 72, 128)
[2025-07-18 13:04:56,058][2140324] RunningMeanStd input shape: (1,)
[2025-07-18 13:04:56,062][2140324] ConvEncoder: input_channels=3
[2025-07-18 13:04:56,077][2140324] Conv encoder output size: 512
[2025-07-18 13:04:56,078][2140324] Policy head output size: 512
[2025-07-18 13:04:56,094][2140324] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 13:04:56,095][2140324] Could not load from checkpoint, attempt 0
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device, weights_only=True)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 13:04:56,095][2140324] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 13:04:56,096][2140324] Could not load from checkpoint, attempt 1
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device, weights_only=True)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 13:04:56,096][2140324] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 13:04:56,097][2140324] Could not load from checkpoint, attempt 2
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device, weights_only=True)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 13:06:05,456][2140324] Could not query the git revision for the logs, perhaps git is not available
[2025-07-18 13:06:05,456][2140324] Loading existing experiment configuration from /home/isaacp/repos/rl_class/train_dir/default_experiment/config.json
[2025-07-18 13:06:05,457][2140324] Overriding arg 'num_workers' with value 1 passed from command line
[2025-07-18 13:06:05,457][2140324] Adding new argument 'no_render'=True that is not in the saved config file!
[2025-07-18 13:06:05,457][2140324] Adding new argument 'save_video'=True that is not in the saved config file!
[2025-07-18 13:06:05,457][2140324] Adding new argument 'video_frames'=1000000000.0 that is not in the saved config file!
[2025-07-18 13:06:05,457][2140324] Adding new argument 'video_name'=None that is not in the saved config file!
[2025-07-18 13:06:05,458][2140324] Adding new argument 'max_num_frames'=1000000000.0 that is not in the saved config file!
[2025-07-18 13:06:05,458][2140324] Adding new argument 'max_num_episodes'=10 that is not in the saved config file!
[2025-07-18 13:06:05,458][2140324] Adding new argument 'push_to_hub'=False that is not in the saved config file!
[2025-07-18 13:06:05,458][2140324] Adding new argument 'hf_repository'=None that is not in the saved config file!
[2025-07-18 13:06:05,458][2140324] Adding new argument 'policy_index'=0 that is not in the saved config file!
[2025-07-18 13:06:05,458][2140324] Adding new argument 'eval_deterministic'=False that is not in the saved config file!
[2025-07-18 13:06:05,458][2140324] Adding new argument 'train_script'=None that is not in the saved config file!
[2025-07-18 13:06:05,459][2140324] Adding new argument 'enjoy_script'=None that is not in the saved config file!
[2025-07-18 13:06:05,459][2140324] Using frameskip 1 and render_action_repeat=4 for evaluation
[2025-07-18 13:06:05,469][2140324] RunningMeanStd input shape: (3, 72, 128)
[2025-07-18 13:06:05,470][2140324] RunningMeanStd input shape: (1,)
[2025-07-18 13:06:05,474][2140324] ConvEncoder: input_channels=3
[2025-07-18 13:06:05,488][2140324] Conv encoder output size: 512
[2025-07-18 13:06:05,489][2140324] Policy head output size: 512
[2025-07-18 13:06:05,511][2140324] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 13:06:05,512][2140324] Could not load from checkpoint, attempt 0
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device, weights_only=False)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 13:06:05,512][2140324] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 13:06:05,512][2140324] Could not load from checkpoint, attempt 1
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device, weights_only=False)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 13:06:05,513][2140324] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 13:06:05,513][2140324] Could not load from checkpoint, attempt 2
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device, weights_only=False)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 13:09:07,992][2144738] Saving configuration to /home/isaacp/repos/rl_class/train_dir/default_experiment/config.json...
[2025-07-18 13:09:07,995][2144738] Rollout worker 0 uses device cpu
[2025-07-18 13:09:07,995][2144738] Rollout worker 1 uses device cpu
[2025-07-18 13:09:07,995][2144738] Rollout worker 2 uses device cpu
[2025-07-18 13:09:07,996][2144738] Rollout worker 3 uses device cpu
[2025-07-18 13:09:07,996][2144738] Rollout worker 4 uses device cpu
[2025-07-18 13:09:07,996][2144738] Rollout worker 5 uses device cpu
[2025-07-18 13:09:07,996][2144738] Rollout worker 6 uses device cpu
[2025-07-18 13:09:07,996][2144738] Rollout worker 7 uses device cpu
[2025-07-18 13:09:08,022][2144738] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 13:09:08,023][2144738] InferenceWorker_p0-w0: min num requests: 2
[2025-07-18 13:09:08,044][2144738] Starting all processes...
[2025-07-18 13:09:08,044][2144738] Starting process learner_proc0
[2025-07-18 13:09:08,094][2144738] Starting all processes...
[2025-07-18 13:09:08,096][2144738] Starting process inference_proc0-0
[2025-07-18 13:09:08,097][2144738] Starting process rollout_proc0
[2025-07-18 13:09:08,097][2144738] Starting process rollout_proc1
[2025-07-18 13:09:08,097][2144738] Starting process rollout_proc2
[2025-07-18 13:09:08,098][2144738] Starting process rollout_proc3
[2025-07-18 13:09:08,098][2144738] Starting process rollout_proc4
[2025-07-18 13:09:08,098][2144738] Starting process rollout_proc5
[2025-07-18 13:09:08,098][2144738] Starting process rollout_proc6
[2025-07-18 13:09:08,098][2144738] Starting process rollout_proc7
[2025-07-18 13:09:09,006][2144895] Worker 0 uses CPU cores [0, 1, 2, 3]
[2025-07-18 13:09:09,006][2144897] Worker 2 uses CPU cores [8, 9, 10, 11]
[2025-07-18 13:09:09,028][2144898] Worker 3 uses CPU cores [12, 13, 14, 15]
[2025-07-18 13:09:09,036][2144881] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 13:09:09,036][2144881] Set environment var CUDA_VISIBLE_DEVICES to '0' (GPU indices [0]) for learning process 0
[2025-07-18 13:09:09,038][2144896] Worker 1 uses CPU cores [4, 5, 6, 7]
[2025-07-18 13:09:09,039][2144902] Worker 7 uses CPU cores [28, 29, 30, 31]
[2025-07-18 13:09:09,047][2144899] Worker 6 uses CPU cores [24, 25, 26, 27]
[2025-07-18 13:09:09,048][2144881] Num visible devices: 1
[2025-07-18 13:09:09,048][2144881] Starting seed is not provided
[2025-07-18 13:09:09,048][2144881] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 13:09:09,048][2144881] Initializing actor-critic model on device cuda:0
[2025-07-18 13:09:09,049][2144881] RunningMeanStd input shape: (3, 72, 128)
[2025-07-18 13:09:09,049][2144881] RunningMeanStd input shape: (1,)
[2025-07-18 13:09:09,052][2144894] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 13:09:09,052][2144894] Set environment var CUDA_VISIBLE_DEVICES to '0' (GPU indices [0]) for inference process 0
[2025-07-18 13:09:09,055][2144881] ConvEncoder: input_channels=3
[2025-07-18 13:09:09,063][2144894] Num visible devices: 1
[2025-07-18 13:09:09,066][2144901] Worker 5 uses CPU cores [20, 21, 22, 23]
[2025-07-18 13:09:09,066][2144900] Worker 4 uses CPU cores [16, 17, 18, 19]
[2025-07-18 13:09:09,094][2144881] Conv encoder output size: 512
[2025-07-18 13:09:09,094][2144881] Policy head output size: 512
[2025-07-18 13:09:09,100][2144881] Created Actor Critic model with architecture:
[2025-07-18 13:09:09,100][2144881] ActorCriticSharedWeights(
  (obs_normalizer): ObservationNormalizer(
    (running_mean_std): RunningMeanStdDictInPlace(
      (running_mean_std): ModuleDict(
        (obs): RunningMeanStdInPlace()
      )
    )
  )
  (returns_normalizer): RecursiveScriptModule(original_name=RunningMeanStdInPlace)
  (encoder): VizdoomEncoder(
    (basic_encoder): ConvEncoder(
      (enc): RecursiveScriptModule(
        original_name=ConvEncoderImpl
        (conv_head): RecursiveScriptModule(
          original_name=Sequential
          (0): RecursiveScriptModule(original_name=Conv2d)
          (1): RecursiveScriptModule(original_name=ELU)
          (2): RecursiveScriptModule(original_name=Conv2d)
          (3): RecursiveScriptModule(original_name=ELU)
          (4): RecursiveScriptModule(original_name=Conv2d)
          (5): RecursiveScriptModule(original_name=ELU)
        )
        (mlp_layers): RecursiveScriptModule(
          original_name=Sequential
          (0): RecursiveScriptModule(original_name=Linear)
          (1): RecursiveScriptModule(original_name=ELU)
        )
      )
    )
  )
  (core): ModelCoreRNN(
    (core): GRU(512, 512)
  )
  (decoder): MlpDecoder(
    (mlp): Identity()
  )
  (critic_linear): Linear(in_features=512, out_features=1, bias=True)
  (action_parameterization): ActionParameterizationDefault(
    (distribution_linear): Linear(in_features=512, out_features=5, bias=True)
  )
)
[2025-07-18 13:09:09,192][2144881] Using optimizer <class 'torch.optim.adam.Adam'>
[2025-07-18 13:09:09,688][2144881] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 13:09:09,689][2144881] Could not load from checkpoint, attempt 0
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 13:09:09,689][2144881] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 13:09:09,690][2144881] Could not load from checkpoint, attempt 1
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 13:09:09,690][2144881] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 13:09:09,690][2144881] Could not load from checkpoint, attempt 2
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 13:09:09,690][2144881] Did not load from checkpoint, starting from scratch!
[2025-07-18 13:09:09,690][2144881] Initialized policy 0 weights for model version 0
[2025-07-18 13:09:09,691][2144881] LearnerWorker_p0 finished initialization!
[2025-07-18 13:09:09,691][2144881] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 13:09:09,740][2144894] RunningMeanStd input shape: (3, 72, 128)
[2025-07-18 13:09:09,741][2144894] RunningMeanStd input shape: (1,)
[2025-07-18 13:09:09,746][2144894] ConvEncoder: input_channels=3
[2025-07-18 13:09:09,784][2144894] Conv encoder output size: 512
[2025-07-18 13:09:09,784][2144894] Policy head output size: 512
[2025-07-18 13:09:09,803][2144738] Inference worker 0-0 is ready!
[2025-07-18 13:09:09,803][2144738] All inference workers are ready! Signal rollout workers to start!
[2025-07-18 13:09:09,819][2144901] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:09:09,819][2144898] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:09:09,819][2144902] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:09:09,819][2144896] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:09:09,820][2144899] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:09:09,820][2144900] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:09:09,821][2144895] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:09:09,821][2144897] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:09:09,961][2144900] Decorrelating experience for 0 frames...
[2025-07-18 13:09:09,963][2144896] Decorrelating experience for 0 frames...
[2025-07-18 13:09:09,965][2144899] Decorrelating experience for 0 frames...
[2025-07-18 13:09:09,965][2144898] Decorrelating experience for 0 frames...
[2025-07-18 13:09:09,966][2144902] Decorrelating experience for 0 frames...
[2025-07-18 13:09:09,967][2144901] Decorrelating experience for 0 frames...
[2025-07-18 13:09:10,088][2144900] Decorrelating experience for 32 frames...
[2025-07-18 13:09:10,093][2144899] Decorrelating experience for 32 frames...
[2025-07-18 13:09:10,093][2144902] Decorrelating experience for 32 frames...
[2025-07-18 13:09:10,093][2144898] Decorrelating experience for 32 frames...
[2025-07-18 13:09:10,116][2144895] Decorrelating experience for 0 frames...
[2025-07-18 13:09:10,119][2144897] Decorrelating experience for 0 frames...
[2025-07-18 13:09:10,239][2144896] Decorrelating experience for 32 frames...
[2025-07-18 13:09:10,239][2144895] Decorrelating experience for 32 frames...
[2025-07-18 13:09:10,253][2144902] Decorrelating experience for 64 frames...
[2025-07-18 13:09:10,253][2144898] Decorrelating experience for 64 frames...
[2025-07-18 13:09:10,394][2144900] Decorrelating experience for 64 frames...
[2025-07-18 13:09:10,395][2144895] Decorrelating experience for 64 frames...
[2025-07-18 13:09:10,396][2144896] Decorrelating experience for 64 frames...
[2025-07-18 13:09:10,396][2144901] Decorrelating experience for 32 frames...
[2025-07-18 13:09:10,398][2144902] Decorrelating experience for 96 frames...
[2025-07-18 13:09:10,536][2144900] Decorrelating experience for 96 frames...
[2025-07-18 13:09:10,536][2144895] Decorrelating experience for 96 frames...
[2025-07-18 13:09:10,536][2144896] Decorrelating experience for 96 frames...
[2025-07-18 13:09:10,554][2144897] Decorrelating experience for 32 frames...
[2025-07-18 13:09:10,557][2144899] Decorrelating experience for 64 frames...
[2025-07-18 13:09:10,558][2144901] Decorrelating experience for 64 frames...
[2025-07-18 13:09:10,698][2144901] Decorrelating experience for 96 frames...
[2025-07-18 13:09:10,702][2144898] Decorrelating experience for 96 frames...
[2025-07-18 13:09:10,716][2144897] Decorrelating experience for 64 frames...
[2025-07-18 13:09:10,851][2144899] Decorrelating experience for 96 frames...
[2025-07-18 13:09:10,859][2144897] Decorrelating experience for 96 frames...
[2025-07-18 13:09:10,903][2144881] Signal inference workers to stop experience collection...
[2025-07-18 13:09:10,904][2144894] InferenceWorker_p0-w0: stopping experience collection
[2025-07-18 13:09:11,263][2144738] Fps is (10 sec: nan, 60 sec: nan, 300 sec: nan). Total num frames: 0. Throughput: 0: nan. Samples: 0. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[2025-07-18 13:09:11,263][2144738] Avg episode reward: [(0, '2.780')]
[2025-07-18 13:09:11,384][2144881] Signal inference workers to resume experience collection...
[2025-07-18 13:09:11,384][2144894] InferenceWorker_p0-w0: resuming experience collection
[2025-07-18 13:09:11,929][2144894] Updated weights for policy 0, policy_version 10 (0.0033)
[2025-07-18 13:09:12,555][2144894] Updated weights for policy 0, policy_version 20 (0.0003)
[2025-07-18 13:09:13,186][2144894] Updated weights for policy 0, policy_version 30 (0.0004)
[2025-07-18 13:09:13,781][2144894] Updated weights for policy 0, policy_version 40 (0.0003)
[2025-07-18 13:09:14,405][2144894] Updated weights for policy 0, policy_version 50 (0.0003)
[2025-07-18 13:09:15,027][2144894] Updated weights for policy 0, policy_version 60 (0.0003)
[2025-07-18 13:09:15,614][2144894] Updated weights for policy 0, policy_version 70 (0.0003)
[2025-07-18 13:09:16,227][2144894] Updated weights for policy 0, policy_version 80 (0.0003)
[2025-07-18 13:09:16,263][2144738] Fps is (10 sec: 65536.7, 60 sec: 65536.7, 300 sec: 65536.7). Total num frames: 327680. Throughput: 0: 15135.8. Samples: 75678. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2025-07-18 13:09:16,263][2144738] Avg episode reward: [(0, '4.357')]
[2025-07-18 13:09:16,265][2144881] Saving new best policy, reward=4.357!
[2025-07-18 13:09:16,863][2144894] Updated weights for policy 0, policy_version 90 (0.0003)
[2025-07-18 13:09:17,487][2144894] Updated weights for policy 0, policy_version 100 (0.0003)
[2025-07-18 13:09:18,111][2144894] Updated weights for policy 0, policy_version 110 (0.0003)
[2025-07-18 13:09:18,739][2144894] Updated weights for policy 0, policy_version 120 (0.0004)
[2025-07-18 13:09:19,339][2144894] Updated weights for policy 0, policy_version 130 (0.0003)
[2025-07-18 13:09:19,952][2144894] Updated weights for policy 0, policy_version 140 (0.0003)
[2025-07-18 13:09:20,557][2144894] Updated weights for policy 0, policy_version 150 (0.0003)
[2025-07-18 13:09:21,166][2144894] Updated weights for policy 0, policy_version 160 (0.0003)
[2025-07-18 13:09:21,263][2144738] Fps is (10 sec: 65946.1, 60 sec: 65946.1, 300 sec: 65946.1). Total num frames: 659456. Throughput: 0: 12482.1. Samples: 124820. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)
[2025-07-18 13:09:21,263][2144738] Avg episode reward: [(0, '4.387')]
[2025-07-18 13:09:21,264][2144881] Saving new best policy, reward=4.387!
[2025-07-18 13:09:21,749][2144894] Updated weights for policy 0, policy_version 170 (0.0003)
[2025-07-18 13:09:22,359][2144894] Updated weights for policy 0, policy_version 180 (0.0003)
[2025-07-18 13:09:22,987][2144894] Updated weights for policy 0, policy_version 190 (0.0003)
[2025-07-18 13:09:23,611][2144894] Updated weights for policy 0, policy_version 200 (0.0004)
[2025-07-18 13:09:24,229][2144894] Updated weights for policy 0, policy_version 210 (0.0003)
[2025-07-18 13:09:24,834][2144894] Updated weights for policy 0, policy_version 220 (0.0003)
[2025-07-18 13:09:25,432][2144894] Updated weights for policy 0, policy_version 230 (0.0003)
[2025-07-18 13:09:26,037][2144894] Updated weights for policy 0, policy_version 240 (0.0003)
[2025-07-18 13:09:26,263][2144738] Fps is (10 sec: 66764.6, 60 sec: 66355.3, 300 sec: 66355.3). Total num frames: 995328. Throughput: 0: 15030.6. Samples: 225458. Policy #0 lag: (min: 0.0, avg: 0.4, max: 1.0)
[2025-07-18 13:09:26,263][2144738] Avg episode reward: [(0, '5.056')]
[2025-07-18 13:09:26,265][2144881] Saving new best policy, reward=5.056!
[2025-07-18 13:09:26,649][2144894] Updated weights for policy 0, policy_version 250 (0.0003)
[2025-07-18 13:09:27,272][2144894] Updated weights for policy 0, policy_version 260 (0.0003)
[2025-07-18 13:09:27,888][2144894] Updated weights for policy 0, policy_version 270 (0.0003)
[2025-07-18 13:09:28,017][2144738] Heartbeat connected on Batcher_0
[2025-07-18 13:09:28,019][2144738] Heartbeat connected on LearnerWorker_p0
[2025-07-18 13:09:28,023][2144738] Heartbeat connected on InferenceWorker_p0-w0
[2025-07-18 13:09:28,026][2144738] Heartbeat connected on RolloutWorker_w0
[2025-07-18 13:09:28,028][2144738] Heartbeat connected on RolloutWorker_w1
[2025-07-18 13:09:28,031][2144738] Heartbeat connected on RolloutWorker_w2
[2025-07-18 13:09:28,033][2144738] Heartbeat connected on RolloutWorker_w3
[2025-07-18 13:09:28,036][2144738] Heartbeat connected on RolloutWorker_w4
[2025-07-18 13:09:28,039][2144738] Heartbeat connected on RolloutWorker_w5
[2025-07-18 13:09:28,041][2144738] Heartbeat connected on RolloutWorker_w6
[2025-07-18 13:09:28,044][2144738] Heartbeat connected on RolloutWorker_w7
[2025-07-18 13:09:28,498][2144894] Updated weights for policy 0, policy_version 280 (0.0003)
[2025-07-18 13:09:29,132][2144894] Updated weights for policy 0, policy_version 290 (0.0003)
[2025-07-18 13:09:29,749][2144894] Updated weights for policy 0, policy_version 300 (0.0003)
[2025-07-18 13:09:30,381][2144894] Updated weights for policy 0, policy_version 310 (0.0003)
[2025-07-18 13:09:30,981][2144894] Updated weights for policy 0, policy_version 320 (0.0003)
[2025-07-18 13:09:31,263][2144738] Fps is (10 sec: 66764.5, 60 sec: 66355.3, 300 sec: 66355.3). Total num frames: 1327104. Throughput: 0: 16272.4. Samples: 325448. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)
[2025-07-18 13:09:31,263][2144738] Avg episode reward: [(0, '6.425')]
[2025-07-18 13:09:31,264][2144881] Saving new best policy, reward=6.425!
[2025-07-18 13:09:31,594][2144894] Updated weights for policy 0, policy_version 330 (0.0003)
[2025-07-18 13:09:32,198][2144894] Updated weights for policy 0, policy_version 340 (0.0003)
[2025-07-18 13:09:32,815][2144894] Updated weights for policy 0, policy_version 350 (0.0003)
[2025-07-18 13:09:33,414][2144894] Updated weights for policy 0, policy_version 360 (0.0003)
[2025-07-18 13:09:34,044][2144894] Updated weights for policy 0, policy_version 370 (0.0003)
[2025-07-18 13:09:34,657][2144894] Updated weights for policy 0, policy_version 380 (0.0003)
[2025-07-18 13:09:35,278][2144894] Updated weights for policy 0, policy_version 390 (0.0003)
[2025-07-18 13:09:35,899][2144894] Updated weights for policy 0, policy_version 400 (0.0003)
[2025-07-18 13:09:36,263][2144738] Fps is (10 sec: 66765.0, 60 sec: 66519.2, 300 sec: 66519.2). Total num frames: 1662976. Throughput: 0: 15031.1. Samples: 375778. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2025-07-18 13:09:36,263][2144738] Avg episode reward: [(0, '8.078')]
[2025-07-18 13:09:36,265][2144881] Saving new best policy, reward=8.078!
[2025-07-18 13:09:36,502][2144894] Updated weights for policy 0, policy_version 410 (0.0003)
[2025-07-18 13:09:37,130][2144894] Updated weights for policy 0, policy_version 420 (0.0004)
[2025-07-18 13:09:37,736][2144894] Updated weights for policy 0, policy_version 430 (0.0003)
[2025-07-18 13:09:38,354][2144894] Updated weights for policy 0, policy_version 440 (0.0004)
[2025-07-18 13:09:38,971][2144894] Updated weights for policy 0, policy_version 450 (0.0003)
[2025-07-18 13:09:39,586][2144894] Updated weights for policy 0, policy_version 460 (0.0003)
[2025-07-18 13:09:40,210][2144894] Updated weights for policy 0, policy_version 470 (0.0004)
[2025-07-18 13:09:40,830][2144894] Updated weights for policy 0, policy_version 480 (0.0003)
[2025-07-18 13:09:41,263][2144738] Fps is (10 sec: 66764.8, 60 sec: 66491.8, 300 sec: 66491.8). Total num frames: 1994752. Throughput: 0: 15846.9. Samples: 475406. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)
[2025-07-18 13:09:41,263][2144738] Avg episode reward: [(0, '11.808')]
[2025-07-18 13:09:41,264][2144881] Saving new best policy, reward=11.808!
[2025-07-18 13:09:41,432][2144894] Updated weights for policy 0, policy_version 490 (0.0003)
[2025-07-18 13:09:42,083][2144894] Updated weights for policy 0, policy_version 500 (0.0004)
[2025-07-18 13:09:42,699][2144894] Updated weights for policy 0, policy_version 510 (0.0003)
[2025-07-18 13:09:43,305][2144894] Updated weights for policy 0, policy_version 520 (0.0003)
[2025-07-18 13:09:43,931][2144894] Updated weights for policy 0, policy_version 530 (0.0003)
[2025-07-18 13:09:44,549][2144894] Updated weights for policy 0, policy_version 540 (0.0003)
[2025-07-18 13:09:45,137][2144894] Updated weights for policy 0, policy_version 550 (0.0003)
[2025-07-18 13:09:45,745][2144894] Updated weights for policy 0, policy_version 560 (0.0003)
[2025-07-18 13:09:46,263][2144738] Fps is (10 sec: 66354.7, 60 sec: 66472.2, 300 sec: 66472.2). Total num frames: 2326528. Throughput: 0: 16433.0. Samples: 575154. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2025-07-18 13:09:46,263][2144738] Avg episode reward: [(0, '14.576')]
[2025-07-18 13:09:46,265][2144881] Saving new best policy, reward=14.576!
[2025-07-18 13:09:46,354][2144894] Updated weights for policy 0, policy_version 570 (0.0003)
[2025-07-18 13:09:46,948][2144894] Updated weights for policy 0, policy_version 580 (0.0003)
[2025-07-18 13:09:47,540][2144894] Updated weights for policy 0, policy_version 590 (0.0003)
[2025-07-18 13:09:48,133][2144894] Updated weights for policy 0, policy_version 600 (0.0003)
[2025-07-18 13:09:48,733][2144894] Updated weights for policy 0, policy_version 610 (0.0003)
[2025-07-18 13:09:49,351][2144894] Updated weights for policy 0, policy_version 620 (0.0003)
[2025-07-18 13:09:49,968][2144894] Updated weights for policy 0, policy_version 630 (0.0003)
[2025-07-18 13:09:50,595][2144894] Updated weights for policy 0, policy_version 640 (0.0004)
[2025-07-18 13:09:51,204][2144894] Updated weights for policy 0, policy_version 650 (0.0003)
[2025-07-18 13:09:51,263][2144738] Fps is (10 sec: 67174.5, 60 sec: 66662.5, 300 sec: 66662.5). Total num frames: 2666496. Throughput: 0: 15656.1. Samples: 626242. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)
[2025-07-18 13:09:51,263][2144738] Avg episode reward: [(0, '17.739')]
[2025-07-18 13:09:51,264][2144881] Saving new best policy, reward=17.739!
[2025-07-18 13:09:51,810][2144894] Updated weights for policy 0, policy_version 660 (0.0003)
[2025-07-18 13:09:52,419][2144894] Updated weights for policy 0, policy_version 670 (0.0003)
[2025-07-18 13:09:53,044][2144894] Updated weights for policy 0, policy_version 680 (0.0003)
[2025-07-18 13:09:53,677][2144894] Updated weights for policy 0, policy_version 690 (0.0003)
[2025-07-18 13:09:54,285][2144894] Updated weights for policy 0, policy_version 700 (0.0003)
[2025-07-18 13:09:54,906][2144894] Updated weights for policy 0, policy_version 710 (0.0004)
[2025-07-18 13:09:55,528][2144894] Updated weights for policy 0, policy_version 720 (0.0003)
[2025-07-18 13:09:56,133][2144894] Updated weights for policy 0, policy_version 730 (0.0003)
[2025-07-18 13:09:56,263][2144738] Fps is (10 sec: 67174.4, 60 sec: 66628.2, 300 sec: 66628.2). Total num frames: 2998272. Throughput: 0: 16139.3. Samples: 726268. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)
[2025-07-18 13:09:56,263][2144738] Avg episode reward: [(0, '18.747')]
[2025-07-18 13:09:56,265][2144881] Saving new best policy, reward=18.747!
[2025-07-18 13:09:56,739][2144894] Updated weights for policy 0, policy_version 740 (0.0003)
[2025-07-18 13:09:57,368][2144894] Updated weights for policy 0, policy_version 750 (0.0004)
[2025-07-18 13:09:57,967][2144894] Updated weights for policy 0, policy_version 760 (0.0003)
[2025-07-18 13:09:58,583][2144894] Updated weights for policy 0, policy_version 770 (0.0003)
[2025-07-18 13:09:59,190][2144894] Updated weights for policy 0, policy_version 780 (0.0003)
[2025-07-18 13:09:59,823][2144894] Updated weights for policy 0, policy_version 790 (0.0004)
[2025-07-18 13:10:00,448][2144894] Updated weights for policy 0, policy_version 800 (0.0003)
[2025-07-18 13:10:01,071][2144894] Updated weights for policy 0, policy_version 810 (0.0003)
[2025-07-18 13:10:01,263][2144738] Fps is (10 sec: 66354.0, 60 sec: 66600.8, 300 sec: 66600.8). Total num frames: 3330048. Throughput: 0: 16673.0. Samples: 825968. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2025-07-18 13:10:01,263][2144738] Avg episode reward: [(0, '21.001')]
[2025-07-18 13:10:01,264][2144881] Saving new best policy, reward=21.001!
[2025-07-18 13:10:01,682][2144894] Updated weights for policy 0, policy_version 820 (0.0003)
[2025-07-18 13:10:02,292][2144894] Updated weights for policy 0, policy_version 830 (0.0003)
[2025-07-18 13:10:02,905][2144894] Updated weights for policy 0, policy_version 840 (0.0003)
[2025-07-18 13:10:03,513][2144894] Updated weights for policy 0, policy_version 850 (0.0004)
[2025-07-18 13:10:04,129][2144894] Updated weights for policy 0, policy_version 860 (0.0003)
[2025-07-18 13:10:04,750][2144894] Updated weights for policy 0, policy_version 870 (0.0003)
[2025-07-18 13:10:05,359][2144894] Updated weights for policy 0, policy_version 880 (0.0003)
[2025-07-18 13:10:05,990][2144894] Updated weights for policy 0, policy_version 890 (0.0004)
[2025-07-18 13:10:06,263][2144738] Fps is (10 sec: 66355.7, 60 sec: 66578.7, 300 sec: 66578.7). Total num frames: 3661824. Throughput: 0: 16697.9. Samples: 876224. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2025-07-18 13:10:06,263][2144738] Avg episode reward: [(0, '22.175')]
[2025-07-18 13:10:06,265][2144881] Saving new best policy, reward=22.175!
[2025-07-18 13:10:06,605][2144894] Updated weights for policy 0, policy_version 900 (0.0003)
[2025-07-18 13:10:07,216][2144894] Updated weights for policy 0, policy_version 910 (0.0003)
[2025-07-18 13:10:07,816][2144894] Updated weights for policy 0, policy_version 920 (0.0003)
[2025-07-18 13:10:08,432][2144894] Updated weights for policy 0, policy_version 930 (0.0003)
[2025-07-18 13:10:09,034][2144894] Updated weights for policy 0, policy_version 940 (0.0003)
[2025-07-18 13:10:09,649][2144894] Updated weights for policy 0, policy_version 950 (0.0003)
[2025-07-18 13:10:10,251][2144894] Updated weights for policy 0, policy_version 960 (0.0003)
[2025-07-18 13:10:10,829][2144894] Updated weights for policy 0, policy_version 970 (0.0003)
[2025-07-18 13:10:11,263][2144738] Fps is (10 sec: 66765.9, 60 sec: 66628.3, 300 sec: 66628.3). Total num frames: 3997696. Throughput: 0: 16686.3. Samples: 976340. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2025-07-18 13:10:11,263][2144738] Avg episode reward: [(0, '22.564')]
[2025-07-18 13:10:11,264][2144881] Saving new best policy, reward=22.564!
[2025-07-18 13:10:11,323][2144881] Stopping Batcher_0...
[2025-07-18 13:10:11,323][2144881] Saving /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 13:10:11,323][2144738] Component Batcher_0 stopped!
[2025-07-18 13:10:11,324][2144881] Loop batcher_evt_loop terminating...
[2025-07-18 13:10:11,335][2144894] Weights refcount: 2 0
[2025-07-18 13:10:11,336][2144894] Stopping InferenceWorker_p0-w0...
[2025-07-18 13:10:11,336][2144894] Loop inference_proc0-0_evt_loop terminating...
[2025-07-18 13:10:11,336][2144738] Component InferenceWorker_p0-w0 stopped!
[2025-07-18 13:10:11,343][2144901] Stopping RolloutWorker_w5...
[2025-07-18 13:10:11,343][2144901] Loop rollout_proc5_evt_loop terminating...
[2025-07-18 13:10:11,343][2144738] Component RolloutWorker_w5 stopped!
[2025-07-18 13:10:11,343][2144898] Stopping RolloutWorker_w3...
[2025-07-18 13:10:11,344][2144738] Component RolloutWorker_w3 stopped!
[2025-07-18 13:10:11,344][2144898] Loop rollout_proc3_evt_loop terminating...
[2025-07-18 13:10:11,344][2144899] Stopping RolloutWorker_w6...
[2025-07-18 13:10:11,344][2144899] Loop rollout_proc6_evt_loop terminating...
[2025-07-18 13:10:11,344][2144738] Component RolloutWorker_w6 stopped!
[2025-07-18 13:10:11,344][2144896] Stopping RolloutWorker_w1...
[2025-07-18 13:10:11,344][2144738] Component RolloutWorker_w1 stopped!
[2025-07-18 13:10:11,344][2144896] Loop rollout_proc1_evt_loop terminating...
[2025-07-18 13:10:11,344][2144897] Stopping RolloutWorker_w2...
[2025-07-18 13:10:11,345][2144897] Loop rollout_proc2_evt_loop terminating...
[2025-07-18 13:10:11,345][2144738] Component RolloutWorker_w2 stopped!
[2025-07-18 13:10:11,345][2144900] Stopping RolloutWorker_w4...
[2025-07-18 13:10:11,345][2144900] Loop rollout_proc4_evt_loop terminating...
[2025-07-18 13:10:11,345][2144738] Component RolloutWorker_w4 stopped!
[2025-07-18 13:10:11,345][2144902] Stopping RolloutWorker_w7...
[2025-07-18 13:10:11,346][2144902] Loop rollout_proc7_evt_loop terminating...
[2025-07-18 13:10:11,345][2144738] Component RolloutWorker_w7 stopped!
[2025-07-18 13:10:11,346][2144895] Stopping RolloutWorker_w0...
[2025-07-18 13:10:11,346][2144738] Component RolloutWorker_w0 stopped!
[2025-07-18 13:10:11,346][2144895] Loop rollout_proc0_evt_loop terminating...
[2025-07-18 13:10:11,367][2144881] Saving /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 13:10:11,408][2144881] Stopping LearnerWorker_p0...
[2025-07-18 13:10:11,408][2144881] Loop learner_proc0_evt_loop terminating...
[2025-07-18 13:10:11,408][2144738] Component LearnerWorker_p0 stopped!
[2025-07-18 13:10:11,409][2144738] Waiting for process learner_proc0 to stop...
[2025-07-18 13:10:11,955][2144738] Waiting for process inference_proc0-0 to join...
[2025-07-18 13:10:11,956][2144738] Waiting for process rollout_proc0 to join...
[2025-07-18 13:10:11,956][2144738] Waiting for process rollout_proc1 to join...
[2025-07-18 13:10:11,956][2144738] Waiting for process rollout_proc2 to join...
[2025-07-18 13:10:11,957][2144738] Waiting for process rollout_proc3 to join...
[2025-07-18 13:10:11,957][2144738] Waiting for process rollout_proc4 to join...
[2025-07-18 13:10:11,957][2144738] Waiting for process rollout_proc5 to join...
[2025-07-18 13:10:11,958][2144738] Waiting for process rollout_proc6 to join...
[2025-07-18 13:10:11,958][2144738] Waiting for process rollout_proc7 to join...
[2025-07-18 13:10:11,958][2144738] Batcher 0 profile tree view:
batching: 4.6288, releasing_batches: 0.0094
[2025-07-18 13:10:11,958][2144738] InferenceWorker_p0-w0 profile tree view:
wait_policy: 0.0000
  wait_policy_total: 1.2672
update_model: 0.8702
  weight_update: 0.0003
one_step: 0.0007
  handle_policy_step: 56.5256
    deserialize: 2.4277, stack: 0.2747, obs_to_device_normalize: 14.9283, forward: 23.3722, send_messages: 3.6659
    prepare_outputs: 9.8054
      to_cpu: 6.9806
[2025-07-18 13:10:11,959][2144738] Learner 0 profile tree view:
misc: 0.0028, prepare_batch: 2.9622
train: 7.7196
  epoch_init: 0.0019, minibatch_init: 0.0021, losses_postprocess: 0.1649, kl_divergence: 0.1563, after_optimizer: 1.6970
  calculate_losses: 3.1233
    losses_init: 0.0011, forward_head: 0.2274, bptt_initial: 1.8360, tail: 0.2064, advantages_returns: 0.0525, losses: 0.4120
    bptt: 0.3391
      bptt_forward_core: 0.3249
  update: 2.4460
    clip: 0.2716
[2025-07-18 13:10:11,959][2144738] RolloutWorker_w0 profile tree view:
wait_for_trajectories: 0.0433, enqueue_policy_requests: 2.2137, env_step: 33.2899, overhead: 1.5026, complete_rollouts: 0.0647
save_policy_outputs: 2.5872
  split_output_tensors: 0.8604
[2025-07-18 13:10:11,959][2144738] RolloutWorker_w7 profile tree view:
wait_for_trajectories: 0.0439, enqueue_policy_requests: 2.2206, env_step: 32.7494, overhead: 1.4770, complete_rollouts: 0.0657
save_policy_outputs: 2.5886
  split_output_tensors: 0.8684
[2025-07-18 13:10:11,959][2144738] Loop Runner_EvtLoop terminating...
[2025-07-18 13:10:11,959][2144738] Runner profile tree view:
main_loop: 63.9157
[2025-07-18 13:10:11,960][2144738] Collected {0: 4005888}, FPS: 62674.5
[2025-07-18 13:10:12,026][2144738] Could not query the git revision for the logs, perhaps git is not available
[2025-07-18 13:10:12,026][2144738] Loading existing experiment configuration from /home/isaacp/repos/rl_class/train_dir/default_experiment/config.json
[2025-07-18 13:10:12,026][2144738] Overriding arg 'num_workers' with value 1 passed from command line
[2025-07-18 13:10:12,027][2144738] Adding new argument 'no_render'=True that is not in the saved config file!
[2025-07-18 13:10:12,027][2144738] Adding new argument 'save_video'=True that is not in the saved config file!
[2025-07-18 13:10:12,027][2144738] Adding new argument 'video_frames'=1000000000.0 that is not in the saved config file!
[2025-07-18 13:10:12,027][2144738] Adding new argument 'video_name'=None that is not in the saved config file!
[2025-07-18 13:10:12,027][2144738] Adding new argument 'max_num_frames'=1000000000.0 that is not in the saved config file!
[2025-07-18 13:10:12,028][2144738] Adding new argument 'max_num_episodes'=10 that is not in the saved config file!
[2025-07-18 13:10:12,028][2144738] Adding new argument 'push_to_hub'=False that is not in the saved config file!
[2025-07-18 13:10:12,028][2144738] Adding new argument 'hf_repository'=None that is not in the saved config file!
[2025-07-18 13:10:12,029][2144738] Adding new argument 'policy_index'=0 that is not in the saved config file!
[2025-07-18 13:10:12,029][2144738] Adding new argument 'eval_deterministic'=False that is not in the saved config file!
[2025-07-18 13:10:12,029][2144738] Adding new argument 'train_script'=None that is not in the saved config file!
[2025-07-18 13:10:12,029][2144738] Adding new argument 'enjoy_script'=None that is not in the saved config file!
[2025-07-18 13:10:12,030][2144738] Using frameskip 1 and render_action_repeat=4 for evaluation
[2025-07-18 13:10:12,041][2144738] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:10:12,042][2144738] RunningMeanStd input shape: (3, 72, 128)
[2025-07-18 13:10:12,043][2144738] RunningMeanStd input shape: (1,)
[2025-07-18 13:10:12,048][2144738] ConvEncoder: input_channels=3
[2025-07-18 13:10:12,089][2144738] Conv encoder output size: 512
[2025-07-18 13:10:12,089][2144738] Policy head output size: 512
[2025-07-18 13:10:12,190][2144738] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 13:10:12,191][2144738] Could not load from checkpoint, attempt 0
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.dtype was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.dtype])` or the `torch.serialization.safe_globals([numpy.dtype])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 13:10:12,192][2144738] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 13:10:12,193][2144738] Could not load from checkpoint, attempt 1
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.dtype was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.dtype])` or the `torch.serialization.safe_globals([numpy.dtype])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 13:10:12,193][2144738] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 13:10:12,193][2144738] Could not load from checkpoint, attempt 2
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.dtype was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.dtype])` or the `torch.serialization.safe_globals([numpy.dtype])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 13:11:42,398][2145685] Saving configuration to /home/isaacp/repos/rl_class/train_dir/default_experiment/config.json...
[2025-07-18 13:11:42,400][2145685] Rollout worker 0 uses device cpu
[2025-07-18 13:11:42,401][2145685] Rollout worker 1 uses device cpu
[2025-07-18 13:11:42,401][2145685] Rollout worker 2 uses device cpu
[2025-07-18 13:11:42,401][2145685] Rollout worker 3 uses device cpu
[2025-07-18 13:11:42,401][2145685] Rollout worker 4 uses device cpu
[2025-07-18 13:11:42,401][2145685] Rollout worker 5 uses device cpu
[2025-07-18 13:11:42,401][2145685] Rollout worker 6 uses device cpu
[2025-07-18 13:11:42,402][2145685] Rollout worker 7 uses device cpu
[2025-07-18 13:11:42,429][2145685] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 13:11:42,429][2145685] InferenceWorker_p0-w0: min num requests: 2
[2025-07-18 13:11:42,450][2145685] Starting all processes...
[2025-07-18 13:11:42,451][2145685] Starting process learner_proc0
[2025-07-18 13:11:42,500][2145685] Starting all processes...
[2025-07-18 13:11:42,503][2145685] Starting process inference_proc0-0
[2025-07-18 13:11:42,503][2145685] Starting process rollout_proc0
[2025-07-18 13:11:42,503][2145685] Starting process rollout_proc1
[2025-07-18 13:11:42,504][2145685] Starting process rollout_proc2
[2025-07-18 13:11:42,504][2145685] Starting process rollout_proc3
[2025-07-18 13:11:42,504][2145685] Starting process rollout_proc4
[2025-07-18 13:11:42,504][2145685] Starting process rollout_proc5
[2025-07-18 13:11:42,506][2145685] Starting process rollout_proc6
[2025-07-18 13:11:42,506][2145685] Starting process rollout_proc7
[2025-07-18 13:11:43,397][2145836] Worker 3 uses CPU cores [12, 13, 14, 15]
[2025-07-18 13:11:43,397][2145835] Worker 2 uses CPU cores [8, 9, 10, 11]
[2025-07-18 13:11:43,397][2145819] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 13:11:43,397][2145819] Set environment var CUDA_VISIBLE_DEVICES to '0' (GPU indices [0]) for learning process 0
[2025-07-18 13:11:43,402][2145834] Worker 1 uses CPU cores [4, 5, 6, 7]
[2025-07-18 13:11:43,408][2145819] Num visible devices: 1
[2025-07-18 13:11:43,409][2145819] Starting seed is not provided
[2025-07-18 13:11:43,409][2145819] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 13:11:43,409][2145819] Initializing actor-critic model on device cuda:0
[2025-07-18 13:11:43,409][2145819] RunningMeanStd input shape: (3, 72, 128)
[2025-07-18 13:11:43,410][2145819] RunningMeanStd input shape: (1,)
[2025-07-18 13:11:43,415][2145819] ConvEncoder: input_channels=3
[2025-07-18 13:11:43,420][2145840] Worker 7 uses CPU cores [28, 29, 30, 31]
[2025-07-18 13:11:43,420][2145837] Worker 4 uses CPU cores [16, 17, 18, 19]
[2025-07-18 13:11:43,420][2145832] Worker 0 uses CPU cores [0, 1, 2, 3]
[2025-07-18 13:11:43,431][2145839] Worker 6 uses CPU cores [24, 25, 26, 27]
[2025-07-18 13:11:43,438][2145838] Worker 5 uses CPU cores [20, 21, 22, 23]
[2025-07-18 13:11:43,438][2145833] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 13:11:43,438][2145833] Set environment var CUDA_VISIBLE_DEVICES to '0' (GPU indices [0]) for inference process 0
[2025-07-18 13:11:43,449][2145833] Num visible devices: 1
[2025-07-18 13:11:43,454][2145819] Conv encoder output size: 512
[2025-07-18 13:11:43,454][2145819] Policy head output size: 512
[2025-07-18 13:11:43,460][2145819] Created Actor Critic model with architecture:
[2025-07-18 13:11:43,460][2145819] ActorCriticSharedWeights(
  (obs_normalizer): ObservationNormalizer(
    (running_mean_std): RunningMeanStdDictInPlace(
      (running_mean_std): ModuleDict(
        (obs): RunningMeanStdInPlace()
      )
    )
  )
  (returns_normalizer): RecursiveScriptModule(original_name=RunningMeanStdInPlace)
  (encoder): VizdoomEncoder(
    (basic_encoder): ConvEncoder(
      (enc): RecursiveScriptModule(
        original_name=ConvEncoderImpl
        (conv_head): RecursiveScriptModule(
          original_name=Sequential
          (0): RecursiveScriptModule(original_name=Conv2d)
          (1): RecursiveScriptModule(original_name=ELU)
          (2): RecursiveScriptModule(original_name=Conv2d)
          (3): RecursiveScriptModule(original_name=ELU)
          (4): RecursiveScriptModule(original_name=Conv2d)
          (5): RecursiveScriptModule(original_name=ELU)
        )
        (mlp_layers): RecursiveScriptModule(
          original_name=Sequential
          (0): RecursiveScriptModule(original_name=Linear)
          (1): RecursiveScriptModule(original_name=ELU)
        )
      )
    )
  )
  (core): ModelCoreRNN(
    (core): GRU(512, 512)
  )
  (decoder): MlpDecoder(
    (mlp): Identity()
  )
  (critic_linear): Linear(in_features=512, out_features=1, bias=True)
  (action_parameterization): ActionParameterizationDefault(
    (distribution_linear): Linear(in_features=512, out_features=5, bias=True)
  )
)
[2025-07-18 13:11:43,546][2145819] Using optimizer <class 'torch.optim.adam.Adam'>
[2025-07-18 13:11:44,043][2145819] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 13:11:44,044][2145819] Could not load from checkpoint, attempt 0
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 13:11:44,044][2145819] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 13:11:44,044][2145819] Could not load from checkpoint, attempt 1
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 13:11:44,045][2145819] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 13:11:44,045][2145819] Could not load from checkpoint, attempt 2
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 13:11:44,045][2145819] Did not load from checkpoint, starting from scratch!
[2025-07-18 13:11:44,045][2145819] Initialized policy 0 weights for model version 0
[2025-07-18 13:11:44,046][2145819] LearnerWorker_p0 finished initialization!
[2025-07-18 13:11:44,046][2145819] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 13:11:44,100][2145833] RunningMeanStd input shape: (3, 72, 128)
[2025-07-18 13:11:44,101][2145833] RunningMeanStd input shape: (1,)
[2025-07-18 13:11:44,106][2145833] ConvEncoder: input_channels=3
[2025-07-18 13:11:44,144][2145833] Conv encoder output size: 512
[2025-07-18 13:11:44,144][2145833] Policy head output size: 512
[2025-07-18 13:11:44,163][2145685] Inference worker 0-0 is ready!
[2025-07-18 13:11:44,164][2145685] All inference workers are ready! Signal rollout workers to start!
[2025-07-18 13:11:44,179][2145832] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:11:44,180][2145837] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:11:44,180][2145840] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:11:44,180][2145835] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:11:44,180][2145839] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:11:44,180][2145836] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:11:44,181][2145838] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:11:44,181][2145834] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:11:44,321][2145837] Decorrelating experience for 0 frames...
[2025-07-18 13:11:44,322][2145832] Decorrelating experience for 0 frames...
[2025-07-18 13:11:44,322][2145838] Decorrelating experience for 0 frames...
[2025-07-18 13:11:44,323][2145834] Decorrelating experience for 0 frames...
[2025-07-18 13:11:44,326][2145840] Decorrelating experience for 0 frames...
[2025-07-18 13:11:44,329][2145835] Decorrelating experience for 0 frames...
[2025-07-18 13:11:44,446][2145837] Decorrelating experience for 32 frames...
[2025-07-18 13:11:44,446][2145832] Decorrelating experience for 32 frames...
[2025-07-18 13:11:44,455][2145840] Decorrelating experience for 32 frames...
[2025-07-18 13:11:44,459][2145835] Decorrelating experience for 32 frames...
[2025-07-18 13:11:44,471][2145836] Decorrelating experience for 0 frames...
[2025-07-18 13:11:44,597][2145838] Decorrelating experience for 32 frames...
[2025-07-18 13:11:44,600][2145834] Decorrelating experience for 32 frames...
[2025-07-18 13:11:44,605][2145836] Decorrelating experience for 32 frames...
[2025-07-18 13:11:44,605][2145832] Decorrelating experience for 64 frames...
[2025-07-18 13:11:44,621][2145835] Decorrelating experience for 64 frames...
[2025-07-18 13:11:44,622][2145839] Decorrelating experience for 0 frames...
[2025-07-18 13:11:44,744][2145832] Decorrelating experience for 96 frames...
[2025-07-18 13:11:44,753][2145838] Decorrelating experience for 64 frames...
[2025-07-18 13:11:44,754][2145834] Decorrelating experience for 64 frames...
[2025-07-18 13:11:44,754][2145837] Decorrelating experience for 64 frames...
[2025-07-18 13:11:44,765][2145835] Decorrelating experience for 96 frames...
[2025-07-18 13:11:44,765][2145836] Decorrelating experience for 64 frames...
[2025-07-18 13:11:44,766][2145840] Decorrelating experience for 64 frames...
[2025-07-18 13:11:44,896][2145838] Decorrelating experience for 96 frames...
[2025-07-18 13:11:44,896][2145834] Decorrelating experience for 96 frames...
[2025-07-18 13:11:44,896][2145837] Decorrelating experience for 96 frames...
[2025-07-18 13:11:44,902][2145839] Decorrelating experience for 32 frames...
[2025-07-18 13:11:44,907][2145836] Decorrelating experience for 96 frames...
[2025-07-18 13:11:44,908][2145840] Decorrelating experience for 96 frames...
[2025-07-18 13:11:45,056][2145839] Decorrelating experience for 64 frames...
[2025-07-18 13:11:45,199][2145839] Decorrelating experience for 96 frames...
[2025-07-18 13:11:45,279][2145819] Signal inference workers to stop experience collection...
[2025-07-18 13:11:45,280][2145833] InferenceWorker_p0-w0: stopping experience collection
[2025-07-18 13:11:45,669][2145685] Fps is (10 sec: nan, 60 sec: nan, 300 sec: nan). Total num frames: 0. Throughput: 0: nan. Samples: 0. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[2025-07-18 13:11:45,670][2145685] Avg episode reward: [(0, '2.656')]
[2025-07-18 13:11:45,759][2145819] Signal inference workers to resume experience collection...
[2025-07-18 13:11:45,759][2145833] InferenceWorker_p0-w0: resuming experience collection
[2025-07-18 13:11:46,272][2145833] Updated weights for policy 0, policy_version 10 (0.0032)
[2025-07-18 13:11:46,903][2145833] Updated weights for policy 0, policy_version 20 (0.0003)
[2025-07-18 13:11:47,535][2145833] Updated weights for policy 0, policy_version 30 (0.0003)
[2025-07-18 13:11:48,154][2145833] Updated weights for policy 0, policy_version 40 (0.0003)
[2025-07-18 13:11:48,766][2145833] Updated weights for policy 0, policy_version 50 (0.0003)
[2025-07-18 13:11:49,371][2145833] Updated weights for policy 0, policy_version 60 (0.0003)
[2025-07-18 13:11:49,981][2145833] Updated weights for policy 0, policy_version 70 (0.0003)
[2025-07-18 13:11:50,591][2145833] Updated weights for policy 0, policy_version 80 (0.0003)
[2025-07-18 13:11:50,669][2145685] Fps is (10 sec: 66354.9, 60 sec: 66354.9, 300 sec: 66354.9). Total num frames: 331776. Throughput: 0: 15095.5. Samples: 75478. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)
[2025-07-18 13:11:50,670][2145685] Avg episode reward: [(0, '4.363')]
[2025-07-18 13:11:50,671][2145819] Saving new best policy, reward=4.363!
[2025-07-18 13:11:51,206][2145833] Updated weights for policy 0, policy_version 90 (0.0003)
[2025-07-18 13:11:51,811][2145833] Updated weights for policy 0, policy_version 100 (0.0003)
[2025-07-18 13:11:52,451][2145833] Updated weights for policy 0, policy_version 110 (0.0003)
[2025-07-18 13:11:53,089][2145833] Updated weights for policy 0, policy_version 120 (0.0003)
[2025-07-18 13:11:53,693][2145833] Updated weights for policy 0, policy_version 130 (0.0003)
[2025-07-18 13:11:54,308][2145833] Updated weights for policy 0, policy_version 140 (0.0003)
[2025-07-18 13:11:54,911][2145833] Updated weights for policy 0, policy_version 150 (0.0003)
[2025-07-18 13:11:55,517][2145833] Updated weights for policy 0, policy_version 160 (0.0003)
[2025-07-18 13:11:55,669][2145685] Fps is (10 sec: 66355.2, 60 sec: 66355.2, 300 sec: 66355.2). Total num frames: 663552. Throughput: 0: 12499.4. Samples: 124994. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2025-07-18 13:11:55,670][2145685] Avg episode reward: [(0, '4.407')]
[2025-07-18 13:11:55,670][2145819] Saving new best policy, reward=4.407!
[2025-07-18 13:11:56,151][2145833] Updated weights for policy 0, policy_version 170 (0.0003)
[2025-07-18 13:11:56,771][2145833] Updated weights for policy 0, policy_version 180 (0.0003)
[2025-07-18 13:11:57,386][2145833] Updated weights for policy 0, policy_version 190 (0.0003)
[2025-07-18 13:11:58,013][2145833] Updated weights for policy 0, policy_version 200 (0.0003)
[2025-07-18 13:11:58,629][2145833] Updated weights for policy 0, policy_version 210 (0.0003)
[2025-07-18 13:11:59,251][2145833] Updated weights for policy 0, policy_version 220 (0.0003)
[2025-07-18 13:11:59,847][2145833] Updated weights for policy 0, policy_version 230 (0.0003)
[2025-07-18 13:12:00,458][2145833] Updated weights for policy 0, policy_version 240 (0.0003)
[2025-07-18 13:12:00,669][2145685] Fps is (10 sec: 66355.3, 60 sec: 66355.2, 300 sec: 66355.2). Total num frames: 995328. Throughput: 0: 14987.9. Samples: 224818. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2025-07-18 13:12:00,670][2145685] Avg episode reward: [(0, '4.947')]
[2025-07-18 13:12:00,671][2145819] Saving new best policy, reward=4.947!
[2025-07-18 13:12:01,084][2145833] Updated weights for policy 0, policy_version 250 (0.0003)
[2025-07-18 13:12:01,689][2145833] Updated weights for policy 0, policy_version 260 (0.0003)
[2025-07-18 13:12:02,308][2145833] Updated weights for policy 0, policy_version 270 (0.0003)
[2025-07-18 13:12:02,426][2145685] Heartbeat connected on Batcher_0
[2025-07-18 13:12:02,431][2145685] Heartbeat connected on InferenceWorker_p0-w0
[2025-07-18 13:12:02,432][2145685] Heartbeat connected on RolloutWorker_w0
[2025-07-18 13:12:02,435][2145685] Heartbeat connected on RolloutWorker_w1
[2025-07-18 13:12:02,437][2145685] Heartbeat connected on LearnerWorker_p0
[2025-07-18 13:12:02,438][2145685] Heartbeat connected on RolloutWorker_w2
[2025-07-18 13:12:02,440][2145685] Heartbeat connected on RolloutWorker_w3
[2025-07-18 13:12:02,443][2145685] Heartbeat connected on RolloutWorker_w4
[2025-07-18 13:12:02,445][2145685] Heartbeat connected on RolloutWorker_w5
[2025-07-18 13:12:02,448][2145685] Heartbeat connected on RolloutWorker_w6
[2025-07-18 13:12:02,450][2145685] Heartbeat connected on RolloutWorker_w7
[2025-07-18 13:12:02,931][2145833] Updated weights for policy 0, policy_version 280 (0.0003)
[2025-07-18 13:12:03,531][2145833] Updated weights for policy 0, policy_version 290 (0.0003)
[2025-07-18 13:12:04,157][2145833] Updated weights for policy 0, policy_version 300 (0.0003)
[2025-07-18 13:12:04,785][2145833] Updated weights for policy 0, policy_version 310 (0.0004)
[2025-07-18 13:12:05,418][2145833] Updated weights for policy 0, policy_version 320 (0.0003)
[2025-07-18 13:12:05,669][2145685] Fps is (10 sec: 65945.7, 60 sec: 66150.4, 300 sec: 66150.4). Total num frames: 1323008. Throughput: 0: 16224.8. Samples: 324496. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)
[2025-07-18 13:12:05,670][2145685] Avg episode reward: [(0, '6.519')]
[2025-07-18 13:12:05,677][2145819] Saving new best policy, reward=6.519!
[2025-07-18 13:12:06,050][2145833] Updated weights for policy 0, policy_version 330 (0.0004)
[2025-07-18 13:12:06,655][2145833] Updated weights for policy 0, policy_version 340 (0.0003)
[2025-07-18 13:12:07,279][2145833] Updated weights for policy 0, policy_version 350 (0.0004)
[2025-07-18 13:12:07,906][2145833] Updated weights for policy 0, policy_version 360 (0.0003)
[2025-07-18 13:12:08,513][2145833] Updated weights for policy 0, policy_version 370 (0.0003)
[2025-07-18 13:12:09,137][2145833] Updated weights for policy 0, policy_version 380 (0.0003)
[2025-07-18 13:12:09,760][2145833] Updated weights for policy 0, policy_version 390 (0.0003)
[2025-07-18 13:12:10,422][2145833] Updated weights for policy 0, policy_version 400 (0.0004)
[2025-07-18 13:12:10,669][2145685] Fps is (10 sec: 65945.6, 60 sec: 66191.4, 300 sec: 66191.4). Total num frames: 1654784. Throughput: 0: 14954.4. Samples: 373860. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)
[2025-07-18 13:12:10,670][2145685] Avg episode reward: [(0, '7.979')]
[2025-07-18 13:12:10,671][2145819] Saving new best policy, reward=7.979!
[2025-07-18 13:12:11,029][2145833] Updated weights for policy 0, policy_version 410 (0.0003)
[2025-07-18 13:12:11,642][2145833] Updated weights for policy 0, policy_version 420 (0.0003)
[2025-07-18 13:12:12,262][2145833] Updated weights for policy 0, policy_version 430 (0.0003)
[2025-07-18 13:12:12,863][2145833] Updated weights for policy 0, policy_version 440 (0.0003)
[2025-07-18 13:12:13,462][2145833] Updated weights for policy 0, policy_version 450 (0.0003)
[2025-07-18 13:12:14,084][2145833] Updated weights for policy 0, policy_version 460 (0.0003)
[2025-07-18 13:12:14,729][2145833] Updated weights for policy 0, policy_version 470 (0.0003)
[2025-07-18 13:12:15,341][2145833] Updated weights for policy 0, policy_version 480 (0.0003)
[2025-07-18 13:12:15,669][2145685] Fps is (10 sec: 66354.9, 60 sec: 66218.6, 300 sec: 66218.6). Total num frames: 1986560. Throughput: 0: 15756.9. Samples: 472706. Policy #0 lag: (min: 0.0, avg: 0.5, max: 1.0)
[2025-07-18 13:12:15,670][2145685] Avg episode reward: [(0, '11.652')]
[2025-07-18 13:12:15,670][2145819] Saving new best policy, reward=11.652!
[2025-07-18 13:12:15,965][2145833] Updated weights for policy 0, policy_version 490 (0.0003)
[2025-07-18 13:12:16,583][2145833] Updated weights for policy 0, policy_version 500 (0.0003)
[2025-07-18 13:12:17,194][2145833] Updated weights for policy 0, policy_version 510 (0.0003)
[2025-07-18 13:12:17,803][2145833] Updated weights for policy 0, policy_version 520 (0.0003)
[2025-07-18 13:12:18,401][2145833] Updated weights for policy 0, policy_version 530 (0.0003)
[2025-07-18 13:12:19,023][2145833] Updated weights for policy 0, policy_version 540 (0.0003)
[2025-07-18 13:12:19,635][2145833] Updated weights for policy 0, policy_version 550 (0.0003)
[2025-07-18 13:12:20,259][2145833] Updated weights for policy 0, policy_version 560 (0.0003)
[2025-07-18 13:12:20,669][2145685] Fps is (10 sec: 66355.3, 60 sec: 66238.2, 300 sec: 66238.2). Total num frames: 2318336. Throughput: 0: 16369.1. Samples: 572920. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2025-07-18 13:12:20,670][2145685] Avg episode reward: [(0, '15.578')]
[2025-07-18 13:12:20,671][2145819] Saving new best policy, reward=15.578!
[2025-07-18 13:12:20,872][2145833] Updated weights for policy 0, policy_version 570 (0.0003)
[2025-07-18 13:12:21,494][2145833] Updated weights for policy 0, policy_version 580 (0.0003)
[2025-07-18 13:12:22,111][2145833] Updated weights for policy 0, policy_version 590 (0.0004)
[2025-07-18 13:12:22,732][2145833] Updated weights for policy 0, policy_version 600 (0.0003)
[2025-07-18 13:12:23,347][2145833] Updated weights for policy 0, policy_version 610 (0.0003)
[2025-07-18 13:12:23,952][2145833] Updated weights for policy 0, policy_version 620 (0.0003)
[2025-07-18 13:12:24,563][2145833] Updated weights for policy 0, policy_version 630 (0.0004)
[2025-07-18 13:12:25,190][2145833] Updated weights for policy 0, policy_version 640 (0.0003)
[2025-07-18 13:12:25,669][2145685] Fps is (10 sec: 66355.4, 60 sec: 66252.8, 300 sec: 66252.8). Total num frames: 2650112. Throughput: 0: 15561.1. Samples: 622444. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)
[2025-07-18 13:12:25,670][2145685] Avg episode reward: [(0, '16.325')]
[2025-07-18 13:12:25,680][2145819] Saving new best policy, reward=16.325!
[2025-07-18 13:12:25,809][2145833] Updated weights for policy 0, policy_version 650 (0.0003)
[2025-07-18 13:12:26,434][2145833] Updated weights for policy 0, policy_version 660 (0.0003)
[2025-07-18 13:12:27,065][2145833] Updated weights for policy 0, policy_version 670 (0.0003)
[2025-07-18 13:12:27,671][2145833] Updated weights for policy 0, policy_version 680 (0.0003)
[2025-07-18 13:12:28,269][2145833] Updated weights for policy 0, policy_version 690 (0.0003)
[2025-07-18 13:12:28,894][2145833] Updated weights for policy 0, policy_version 700 (0.0003)
[2025-07-18 13:12:29,499][2145833] Updated weights for policy 0, policy_version 710 (0.0003)
[2025-07-18 13:12:30,089][2145833] Updated weights for policy 0, policy_version 720 (0.0003)
[2025-07-18 13:12:30,669][2145685] Fps is (10 sec: 66764.6, 60 sec: 66355.2, 300 sec: 66355.2). Total num frames: 2985984. Throughput: 0: 16048.0. Samples: 722162. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)
[2025-07-18 13:12:30,670][2145685] Avg episode reward: [(0, '20.892')]
[2025-07-18 13:12:30,671][2145819] Saving new best policy, reward=20.892!
[2025-07-18 13:12:30,717][2145833] Updated weights for policy 0, policy_version 730 (0.0003)
[2025-07-18 13:12:31,314][2145833] Updated weights for policy 0, policy_version 740 (0.0003)
[2025-07-18 13:12:31,931][2145833] Updated weights for policy 0, policy_version 750 (0.0003)
[2025-07-18 13:12:32,541][2145833] Updated weights for policy 0, policy_version 760 (0.0003)
[2025-07-18 13:12:33,150][2145833] Updated weights for policy 0, policy_version 770 (0.0003)
[2025-07-18 13:12:33,776][2145833] Updated weights for policy 0, policy_version 780 (0.0003)
[2025-07-18 13:12:34,400][2145833] Updated weights for policy 0, policy_version 790 (0.0003)
[2025-07-18 13:12:35,015][2145833] Updated weights for policy 0, policy_version 800 (0.0003)
[2025-07-18 13:12:35,626][2145833] Updated weights for policy 0, policy_version 810 (0.0003)
[2025-07-18 13:12:35,669][2145685] Fps is (10 sec: 66764.7, 60 sec: 66355.2, 300 sec: 66355.2). Total num frames: 3317760. Throughput: 0: 16604.8. Samples: 822694. Policy #0 lag: (min: 0.0, avg: 0.5, max: 1.0)
[2025-07-18 13:12:35,670][2145685] Avg episode reward: [(0, '21.900')]
[2025-07-18 13:12:35,670][2145819] Saving new best policy, reward=21.900!
[2025-07-18 13:12:36,241][2145833] Updated weights for policy 0, policy_version 820 (0.0003)
[2025-07-18 13:12:36,849][2145833] Updated weights for policy 0, policy_version 830 (0.0003)
[2025-07-18 13:12:37,461][2145833] Updated weights for policy 0, policy_version 840 (0.0004)
[2025-07-18 13:12:38,066][2145833] Updated weights for policy 0, policy_version 850 (0.0003)
[2025-07-18 13:12:38,682][2145833] Updated weights for policy 0, policy_version 860 (0.0003)
[2025-07-18 13:12:39,272][2145833] Updated weights for policy 0, policy_version 870 (0.0003)
[2025-07-18 13:12:39,863][2145833] Updated weights for policy 0, policy_version 880 (0.0003)
[2025-07-18 13:12:40,487][2145833] Updated weights for policy 0, policy_version 890 (0.0004)
[2025-07-18 13:12:40,669][2145685] Fps is (10 sec: 66764.8, 60 sec: 66429.7, 300 sec: 66429.7). Total num frames: 3653632. Throughput: 0: 16614.4. Samples: 872640. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)
[2025-07-18 13:12:40,670][2145685] Avg episode reward: [(0, '23.615')]
[2025-07-18 13:12:40,681][2145819] Saving new best policy, reward=23.615!
[2025-07-18 13:12:41,126][2145833] Updated weights for policy 0, policy_version 900 (0.0004)
[2025-07-18 13:12:41,736][2145833] Updated weights for policy 0, policy_version 910 (0.0003)
[2025-07-18 13:12:42,352][2145833] Updated weights for policy 0, policy_version 920 (0.0003)
[2025-07-18 13:12:42,957][2145833] Updated weights for policy 0, policy_version 930 (0.0003)
[2025-07-18 13:12:43,605][2145833] Updated weights for policy 0, policy_version 940 (0.0004)
[2025-07-18 13:12:44,224][2145833] Updated weights for policy 0, policy_version 950 (0.0003)
[2025-07-18 13:12:44,831][2145833] Updated weights for policy 0, policy_version 960 (0.0003)
[2025-07-18 13:12:45,462][2145833] Updated weights for policy 0, policy_version 970 (0.0004)
[2025-07-18 13:12:45,669][2145685] Fps is (10 sec: 66764.9, 60 sec: 66423.5, 300 sec: 66423.5). Total num frames: 3985408. Throughput: 0: 16619.8. Samples: 972710. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)
[2025-07-18 13:12:45,670][2145685] Avg episode reward: [(0, '21.497')]
[2025-07-18 13:12:45,955][2145819] Stopping Batcher_0...
[2025-07-18 13:12:45,955][2145819] Saving /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 13:12:45,955][2145685] Component Batcher_0 stopped!
[2025-07-18 13:12:45,955][2145819] Loop batcher_evt_loop terminating...
[2025-07-18 13:12:45,969][2145833] Weights refcount: 2 0
[2025-07-18 13:12:45,970][2145833] Stopping InferenceWorker_p0-w0...
[2025-07-18 13:12:45,970][2145833] Loop inference_proc0-0_evt_loop terminating...
[2025-07-18 13:12:45,970][2145685] Component InferenceWorker_p0-w0 stopped!
[2025-07-18 13:12:45,975][2145837] Stopping RolloutWorker_w4...
[2025-07-18 13:12:45,975][2145837] Loop rollout_proc4_evt_loop terminating...
[2025-07-18 13:12:45,975][2145685] Component RolloutWorker_w4 stopped!
[2025-07-18 13:12:45,977][2145832] Stopping RolloutWorker_w0...
[2025-07-18 13:12:45,977][2145685] Component RolloutWorker_w0 stopped!
[2025-07-18 13:12:45,977][2145832] Loop rollout_proc0_evt_loop terminating...
[2025-07-18 13:12:45,978][2145685] Component RolloutWorker_w1 stopped!
[2025-07-18 13:12:45,978][2145834] Stopping RolloutWorker_w1...
[2025-07-18 13:12:45,978][2145834] Loop rollout_proc1_evt_loop terminating...
[2025-07-18 13:12:45,978][2145840] Stopping RolloutWorker_w7...
[2025-07-18 13:12:45,978][2145685] Component RolloutWorker_w7 stopped!
[2025-07-18 13:12:45,978][2145840] Loop rollout_proc7_evt_loop terminating...
[2025-07-18 13:12:45,979][2145836] Stopping RolloutWorker_w3...
[2025-07-18 13:12:45,979][2145838] Stopping RolloutWorker_w5...
[2025-07-18 13:12:45,979][2145839] Stopping RolloutWorker_w6...
[2025-07-18 13:12:45,979][2145835] Stopping RolloutWorker_w2...
[2025-07-18 13:12:45,979][2145836] Loop rollout_proc3_evt_loop terminating...
[2025-07-18 13:12:45,979][2145838] Loop rollout_proc5_evt_loop terminating...
[2025-07-18 13:12:45,979][2145685] Component RolloutWorker_w3 stopped!
[2025-07-18 13:12:45,979][2145839] Loop rollout_proc6_evt_loop terminating...
[2025-07-18 13:12:45,979][2145835] Loop rollout_proc2_evt_loop terminating...
[2025-07-18 13:12:45,979][2145685] Component RolloutWorker_w2 stopped!
[2025-07-18 13:12:45,979][2145685] Component RolloutWorker_w6 stopped!
[2025-07-18 13:12:45,979][2145685] Component RolloutWorker_w5 stopped!
[2025-07-18 13:12:45,998][2145819] Saving /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 13:12:46,038][2145819] Stopping LearnerWorker_p0...
[2025-07-18 13:12:46,039][2145819] Loop learner_proc0_evt_loop terminating...
[2025-07-18 13:12:46,039][2145685] Component LearnerWorker_p0 stopped!
[2025-07-18 13:12:46,039][2145685] Waiting for process learner_proc0 to stop...
[2025-07-18 13:12:46,581][2145685] Waiting for process inference_proc0-0 to join...
[2025-07-18 13:12:46,582][2145685] Waiting for process rollout_proc0 to join...
[2025-07-18 13:12:46,582][2145685] Waiting for process rollout_proc1 to join...
[2025-07-18 13:12:46,583][2145685] Waiting for process rollout_proc2 to join...
[2025-07-18 13:12:46,583][2145685] Waiting for process rollout_proc3 to join...
[2025-07-18 13:12:46,583][2145685] Waiting for process rollout_proc4 to join...
[2025-07-18 13:12:46,583][2145685] Waiting for process rollout_proc5 to join...
[2025-07-18 13:12:46,584][2145685] Waiting for process rollout_proc6 to join...
[2025-07-18 13:12:46,584][2145685] Waiting for process rollout_proc7 to join...
[2025-07-18 13:12:46,584][2145685] Batcher 0 profile tree view:
batching: 5.6899, releasing_batches: 0.0085
[2025-07-18 13:12:46,584][2145685] InferenceWorker_p0-w0 profile tree view:
wait_policy: 0.0000
  wait_policy_total: 1.2456
update_model: 0.8744
  weight_update: 0.0003
one_step: 0.0007
  handle_policy_step: 56.8214
    deserialize: 2.4465, stack: 0.2781, obs_to_device_normalize: 15.0203, forward: 23.2993, send_messages: 3.7053
    prepare_outputs: 10.0253
      to_cpu: 7.2384
[2025-07-18 13:12:46,585][2145685] Learner 0 profile tree view:
misc: 0.0023, prepare_batch: 2.9650
train: 7.6407
  epoch_init: 0.0017, minibatch_init: 0.0021, losses_postprocess: 0.1662, kl_divergence: 0.1580, after_optimizer: 1.6987
  calculate_losses: 3.0387
    losses_init: 0.0011, forward_head: 0.2309, bptt_initial: 1.7458, tail: 0.2059, advantages_returns: 0.0516, losses: 0.4161
    bptt: 0.3387
      bptt_forward_core: 0.3248
  update: 2.4478
    clip: 0.2681
[2025-07-18 13:12:46,585][2145685] RolloutWorker_w0 profile tree view:
wait_for_trajectories: 0.0420, enqueue_policy_requests: 2.2052, env_step: 33.2261, overhead: 1.5083, complete_rollouts: 0.0628
save_policy_outputs: 2.5872
  split_output_tensors: 0.8604
[2025-07-18 13:12:46,585][2145685] RolloutWorker_w7 profile tree view:
wait_for_trajectories: 0.0435, enqueue_policy_requests: 2.2304, env_step: 33.1857, overhead: 1.4919, complete_rollouts: 0.0639
save_policy_outputs: 2.5813
  split_output_tensors: 0.8617
[2025-07-18 13:12:46,585][2145685] Loop Runner_EvtLoop terminating...
[2025-07-18 13:12:46,585][2145685] Runner profile tree view:
main_loop: 64.1352
[2025-07-18 13:12:46,586][2145685] Collected {0: 4005888}, FPS: 62460.0
[2025-07-18 13:12:46,647][2145685] Could not query the git revision for the logs, perhaps git is not available
[2025-07-18 13:12:46,648][2145685] Loading existing experiment configuration from /home/isaacp/repos/rl_class/train_dir/default_experiment/config.json
[2025-07-18 13:12:46,648][2145685] Overriding arg 'num_workers' with value 1 passed from command line
[2025-07-18 13:12:46,649][2145685] Adding new argument 'no_render'=True that is not in the saved config file!
[2025-07-18 13:12:46,649][2145685] Adding new argument 'save_video'=True that is not in the saved config file!
[2025-07-18 13:12:46,649][2145685] Adding new argument 'video_frames'=1000000000.0 that is not in the saved config file!
[2025-07-18 13:12:46,649][2145685] Adding new argument 'video_name'=None that is not in the saved config file!
[2025-07-18 13:12:46,649][2145685] Adding new argument 'max_num_frames'=1000000000.0 that is not in the saved config file!
[2025-07-18 13:12:46,650][2145685] Adding new argument 'max_num_episodes'=10 that is not in the saved config file!
[2025-07-18 13:12:46,650][2145685] Adding new argument 'push_to_hub'=False that is not in the saved config file!
[2025-07-18 13:12:46,650][2145685] Adding new argument 'hf_repository'=None that is not in the saved config file!
[2025-07-18 13:12:46,650][2145685] Adding new argument 'policy_index'=0 that is not in the saved config file!
[2025-07-18 13:12:46,651][2145685] Adding new argument 'eval_deterministic'=False that is not in the saved config file!
[2025-07-18 13:12:46,651][2145685] Adding new argument 'train_script'=None that is not in the saved config file!
[2025-07-18 13:12:46,651][2145685] Adding new argument 'enjoy_script'=None that is not in the saved config file!
[2025-07-18 13:12:46,651][2145685] Using frameskip 1 and render_action_repeat=4 for evaluation
[2025-07-18 13:12:46,663][2145685] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:12:46,664][2145685] RunningMeanStd input shape: (3, 72, 128)
[2025-07-18 13:12:46,665][2145685] RunningMeanStd input shape: (1,)
[2025-07-18 13:12:46,670][2145685] ConvEncoder: input_channels=3
[2025-07-18 13:12:46,711][2145685] Conv encoder output size: 512
[2025-07-18 13:12:46,712][2145685] Policy head output size: 512
[2025-07-18 13:12:46,805][2145685] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 13:12:46,807][2145685] Could not load from checkpoint, attempt 0
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.dtype was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.dtype])` or the `torch.serialization.safe_globals([numpy.dtype])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 13:12:46,808][2145685] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 13:12:46,808][2145685] Could not load from checkpoint, attempt 1
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.dtype was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.dtype])` or the `torch.serialization.safe_globals([numpy.dtype])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 13:12:46,809][2145685] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 13:12:46,809][2145685] Could not load from checkpoint, attempt 2
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.dtype was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.dtype])` or the `torch.serialization.safe_globals([numpy.dtype])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 13:15:05,131][2145685] Could not query the git revision for the logs, perhaps git is not available
[2025-07-18 13:15:23,527][2145685] Could not query the git revision for the logs, perhaps git is not available
[2025-07-18 13:15:35,040][2145685] Could not query the git revision for the logs, perhaps git is not available
[2025-07-18 13:18:07,173][2145685] Could not query the git revision for the logs, perhaps git is not available
[2025-07-18 13:18:19,810][2145685] Could not query the git revision for the logs, perhaps git is not available
[2025-07-18 13:18:19,811][2145685] Loading existing experiment configuration from /home/isaacp/repos/rl_class/train_dir/default_experiment/config.json
[2025-07-18 13:18:19,811][2145685] Overriding arg 'num_workers' with value 1 passed from command line
[2025-07-18 13:18:19,811][2145685] Adding new argument 'no_render'=True that is not in the saved config file!
[2025-07-18 13:18:19,811][2145685] Adding new argument 'save_video'=True that is not in the saved config file!
[2025-07-18 13:18:19,812][2145685] Adding new argument 'video_frames'=1000000000.0 that is not in the saved config file!
[2025-07-18 13:18:19,812][2145685] Adding new argument 'video_name'=None that is not in the saved config file!
[2025-07-18 13:18:19,812][2145685] Adding new argument 'max_num_frames'=1000000000.0 that is not in the saved config file!
[2025-07-18 13:18:19,812][2145685] Adding new argument 'max_num_episodes'=10 that is not in the saved config file!
[2025-07-18 13:18:19,813][2145685] Adding new argument 'push_to_hub'=False that is not in the saved config file!
[2025-07-18 13:18:19,813][2145685] Adding new argument 'hf_repository'=None that is not in the saved config file!
[2025-07-18 13:18:19,813][2145685] Adding new argument 'policy_index'=0 that is not in the saved config file!
[2025-07-18 13:18:19,813][2145685] Adding new argument 'eval_deterministic'=False that is not in the saved config file!
[2025-07-18 13:18:19,814][2145685] Adding new argument 'train_script'=None that is not in the saved config file!
[2025-07-18 13:18:19,814][2145685] Adding new argument 'enjoy_script'=None that is not in the saved config file!
[2025-07-18 13:18:19,814][2145685] Using frameskip 1 and render_action_repeat=4 for evaluation
[2025-07-18 13:18:19,827][2145685] RunningMeanStd input shape: (3, 72, 128)
[2025-07-18 13:18:19,828][2145685] RunningMeanStd input shape: (1,)
[2025-07-18 13:18:19,832][2145685] ConvEncoder: input_channels=3
[2025-07-18 13:18:19,848][2145685] Conv encoder output size: 512
[2025-07-18 13:18:19,848][2145685] Policy head output size: 512
[2025-07-18 13:18:19,869][2145685] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 13:18:19,869][2145685] Could not load from checkpoint, attempt 0
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    log.warning("Loading state from checkpoint %s...", latest_checkpoint)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.dtype was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.dtype])` or the `torch.serialization.safe_globals([numpy.dtype])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 13:18:19,870][2145685] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 13:18:19,870][2145685] Could not load from checkpoint, attempt 1
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    log.warning("Loading state from checkpoint %s...", latest_checkpoint)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.dtype was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.dtype])` or the `torch.serialization.safe_globals([numpy.dtype])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 13:18:19,871][2145685] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 13:18:19,871][2145685] Could not load from checkpoint, attempt 2
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    log.warning("Loading state from checkpoint %s...", latest_checkpoint)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.dtype was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.dtype])` or the `torch.serialization.safe_globals([numpy.dtype])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 13:20:35,105][2147157] Saving configuration to /home/isaacp/repos/rl_class/train_dir/default_experiment/config.json...
[2025-07-18 13:20:35,107][2147157] Rollout worker 0 uses device cpu
[2025-07-18 13:20:35,108][2147157] Rollout worker 1 uses device cpu
[2025-07-18 13:20:35,108][2147157] Rollout worker 2 uses device cpu
[2025-07-18 13:20:35,108][2147157] Rollout worker 3 uses device cpu
[2025-07-18 13:20:35,108][2147157] Rollout worker 4 uses device cpu
[2025-07-18 13:20:35,108][2147157] Rollout worker 5 uses device cpu
[2025-07-18 13:20:35,109][2147157] Rollout worker 6 uses device cpu
[2025-07-18 13:20:35,109][2147157] Rollout worker 7 uses device cpu
[2025-07-18 13:20:35,135][2147157] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 13:20:35,135][2147157] InferenceWorker_p0-w0: min num requests: 2
[2025-07-18 13:20:35,155][2147157] Starting all processes...
[2025-07-18 13:20:35,155][2147157] Starting process learner_proc0
[2025-07-18 13:20:35,205][2147157] Starting all processes...
[2025-07-18 13:20:35,207][2147157] Starting process inference_proc0-0
[2025-07-18 13:20:35,207][2147157] Starting process rollout_proc0
[2025-07-18 13:20:35,208][2147157] Starting process rollout_proc1
[2025-07-18 13:20:35,208][2147157] Starting process rollout_proc2
[2025-07-18 13:20:35,208][2147157] Starting process rollout_proc3
[2025-07-18 13:20:35,209][2147157] Starting process rollout_proc4
[2025-07-18 13:20:35,209][2147157] Starting process rollout_proc5
[2025-07-18 13:20:35,209][2147157] Starting process rollout_proc6
[2025-07-18 13:20:35,210][2147157] Starting process rollout_proc7
[2025-07-18 13:20:36,076][2147291] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 13:20:36,076][2147291] Set environment var CUDA_VISIBLE_DEVICES to '0' (GPU indices [0]) for learning process 0
[2025-07-18 13:20:36,088][2147291] Num visible devices: 1
[2025-07-18 13:20:36,088][2147291] Starting seed is not provided
[2025-07-18 13:20:36,088][2147291] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 13:20:36,088][2147291] Initializing actor-critic model on device cuda:0
[2025-07-18 13:20:36,089][2147291] RunningMeanStd input shape: (3, 72, 128)
[2025-07-18 13:20:36,089][2147291] RunningMeanStd input shape: (1,)
[2025-07-18 13:20:36,093][2147312] Worker 7 uses CPU cores [28, 29, 30, 31]
[2025-07-18 13:20:36,094][2147291] ConvEncoder: input_channels=3
[2025-07-18 13:20:36,104][2147310] Worker 5 uses CPU cores [20, 21, 22, 23]
[2025-07-18 13:20:36,112][2147309] Worker 4 uses CPU cores [16, 17, 18, 19]
[2025-07-18 13:20:36,132][2147305] Worker 0 uses CPU cores [0, 1, 2, 3]
[2025-07-18 13:20:36,132][2147307] Worker 3 uses CPU cores [12, 13, 14, 15]
[2025-07-18 13:20:36,133][2147306] Worker 1 uses CPU cores [4, 5, 6, 7]
[2025-07-18 13:20:36,136][2147291] Conv encoder output size: 512
[2025-07-18 13:20:36,136][2147291] Policy head output size: 512
[2025-07-18 13:20:36,136][2147311] Worker 6 uses CPU cores [24, 25, 26, 27]
[2025-07-18 13:20:36,136][2147308] Worker 2 uses CPU cores [8, 9, 10, 11]
[2025-07-18 13:20:36,142][2147291] Created Actor Critic model with architecture:
[2025-07-18 13:20:36,142][2147291] ActorCriticSharedWeights(
  (obs_normalizer): ObservationNormalizer(
    (running_mean_std): RunningMeanStdDictInPlace(
      (running_mean_std): ModuleDict(
        (obs): RunningMeanStdInPlace()
      )
    )
  )
  (returns_normalizer): RecursiveScriptModule(original_name=RunningMeanStdInPlace)
  (encoder): VizdoomEncoder(
    (basic_encoder): ConvEncoder(
      (enc): RecursiveScriptModule(
        original_name=ConvEncoderImpl
        (conv_head): RecursiveScriptModule(
          original_name=Sequential
          (0): RecursiveScriptModule(original_name=Conv2d)
          (1): RecursiveScriptModule(original_name=ELU)
          (2): RecursiveScriptModule(original_name=Conv2d)
          (3): RecursiveScriptModule(original_name=ELU)
          (4): RecursiveScriptModule(original_name=Conv2d)
          (5): RecursiveScriptModule(original_name=ELU)
        )
        (mlp_layers): RecursiveScriptModule(
          original_name=Sequential
          (0): RecursiveScriptModule(original_name=Linear)
          (1): RecursiveScriptModule(original_name=ELU)
        )
      )
    )
  )
  (core): ModelCoreRNN(
    (core): GRU(512, 512)
  )
  (decoder): MlpDecoder(
    (mlp): Identity()
  )
  (critic_linear): Linear(in_features=512, out_features=1, bias=True)
  (action_parameterization): ActionParameterizationDefault(
    (distribution_linear): Linear(in_features=512, out_features=5, bias=True)
  )
)
[2025-07-18 13:20:36,151][2147304] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 13:20:36,152][2147304] Set environment var CUDA_VISIBLE_DEVICES to '0' (GPU indices [0]) for inference process 0
[2025-07-18 13:20:36,170][2147304] Num visible devices: 1
[2025-07-18 13:20:36,223][2147291] Using optimizer <class 'torch.optim.adam.Adam'>
[2025-07-18 13:20:36,725][2147291] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 13:20:36,726][2147291] Could not load from checkpoint, attempt 0
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner.py", line 282, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 13:20:36,727][2147291] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 13:20:36,727][2147291] Could not load from checkpoint, attempt 1
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner.py", line 282, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 13:20:36,727][2147291] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 13:20:36,727][2147291] Could not load from checkpoint, attempt 2
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner.py", line 282, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 13:20:36,727][2147291] Did not load from checkpoint, starting from scratch!
[2025-07-18 13:20:36,727][2147291] Initialized policy 0 weights for model version 0
[2025-07-18 13:20:36,728][2147291] LearnerWorker_p0 finished initialization!
[2025-07-18 13:20:36,728][2147291] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 13:20:36,778][2147304] RunningMeanStd input shape: (3, 72, 128)
[2025-07-18 13:20:36,779][2147304] RunningMeanStd input shape: (1,)
[2025-07-18 13:20:36,784][2147304] ConvEncoder: input_channels=3
[2025-07-18 13:20:36,822][2147304] Conv encoder output size: 512
[2025-07-18 13:20:36,822][2147304] Policy head output size: 512
[2025-07-18 13:20:36,842][2147157] Inference worker 0-0 is ready!
[2025-07-18 13:20:36,843][2147157] All inference workers are ready! Signal rollout workers to start!
[2025-07-18 13:20:36,858][2147312] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:20:36,858][2147309] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:20:36,859][2147310] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:20:36,859][2147306] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:20:36,859][2147307] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:20:36,859][2147305] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:20:36,865][2147311] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:20:36,865][2147308] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:20:36,998][2147309] Decorrelating experience for 0 frames...
[2025-07-18 13:20:37,001][2147312] Decorrelating experience for 0 frames...
[2025-07-18 13:20:37,001][2147307] Decorrelating experience for 0 frames...
[2025-07-18 13:20:37,006][2147308] Decorrelating experience for 0 frames...
[2025-07-18 13:20:37,006][2147311] Decorrelating experience for 0 frames...
[2025-07-18 13:20:37,126][2147307] Decorrelating experience for 32 frames...
[2025-07-18 13:20:37,127][2147312] Decorrelating experience for 32 frames...
[2025-07-18 13:20:37,131][2147311] Decorrelating experience for 32 frames...
[2025-07-18 13:20:37,147][2147305] Decorrelating experience for 0 frames...
[2025-07-18 13:20:37,149][2147306] Decorrelating experience for 0 frames...
[2025-07-18 13:20:37,149][2147310] Decorrelating experience for 0 frames...
[2025-07-18 13:20:37,272][2147309] Decorrelating experience for 32 frames...
[2025-07-18 13:20:37,272][2147310] Decorrelating experience for 32 frames...
[2025-07-18 13:20:37,273][2147305] Decorrelating experience for 32 frames...
[2025-07-18 13:20:37,286][2147308] Decorrelating experience for 32 frames...
[2025-07-18 13:20:37,423][2147310] Decorrelating experience for 64 frames...
[2025-07-18 13:20:37,426][2147306] Decorrelating experience for 32 frames...
[2025-07-18 13:20:37,427][2147309] Decorrelating experience for 64 frames...
[2025-07-18 13:20:37,427][2147305] Decorrelating experience for 64 frames...
[2025-07-18 13:20:37,442][2147311] Decorrelating experience for 64 frames...
[2025-07-18 13:20:37,446][2147308] Decorrelating experience for 64 frames...
[2025-07-18 13:20:37,563][2147309] Decorrelating experience for 96 frames...
[2025-07-18 13:20:37,581][2147306] Decorrelating experience for 64 frames...
[2025-07-18 13:20:37,594][2147307] Decorrelating experience for 64 frames...
[2025-07-18 13:20:37,612][2147310] Decorrelating experience for 96 frames...
[2025-07-18 13:20:37,690][2147308] Decorrelating experience for 96 frames...
[2025-07-18 13:20:37,722][2147306] Decorrelating experience for 96 frames...
[2025-07-18 13:20:37,741][2147307] Decorrelating experience for 96 frames...
[2025-07-18 13:20:37,748][2147312] Decorrelating experience for 64 frames...
[2025-07-18 13:20:37,763][2147305] Decorrelating experience for 96 frames...
[2025-07-18 13:20:37,889][2147312] Decorrelating experience for 96 frames...
[2025-07-18 13:20:37,892][2147311] Decorrelating experience for 96 frames...
[2025-07-18 13:20:38,060][2147291] Signal inference workers to stop experience collection...
[2025-07-18 13:20:38,068][2147304] InferenceWorker_p0-w0: stopping experience collection
[2025-07-18 13:20:38,389][2147157] Fps is (10 sec: nan, 60 sec: nan, 300 sec: nan). Total num frames: 0. Throughput: 0: nan. Samples: 0. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[2025-07-18 13:20:38,390][2147157] Avg episode reward: [(0, '2.328')]
[2025-07-18 13:20:38,561][2147291] Signal inference workers to resume experience collection...
[2025-07-18 13:20:38,562][2147304] InferenceWorker_p0-w0: resuming experience collection
[2025-07-18 13:20:39,092][2147304] Updated weights for policy 0, policy_version 10 (0.0032)
[2025-07-18 13:20:39,722][2147304] Updated weights for policy 0, policy_version 20 (0.0004)
[2025-07-18 13:20:40,340][2147304] Updated weights for policy 0, policy_version 30 (0.0003)
[2025-07-18 13:20:40,961][2147304] Updated weights for policy 0, policy_version 40 (0.0003)
[2025-07-18 13:20:41,584][2147304] Updated weights for policy 0, policy_version 50 (0.0003)
[2025-07-18 13:20:42,199][2147304] Updated weights for policy 0, policy_version 60 (0.0003)
[2025-07-18 13:20:42,808][2147304] Updated weights for policy 0, policy_version 70 (0.0003)
[2025-07-18 13:20:43,389][2147157] Fps is (10 sec: 64717.5, 60 sec: 64717.5, 300 sec: 64717.5). Total num frames: 323584. Throughput: 0: 14608.2. Samples: 73040. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)
[2025-07-18 13:20:43,389][2147157] Avg episode reward: [(0, '4.281')]
[2025-07-18 13:20:43,400][2147291] Saving new best policy, reward=4.281!
[2025-07-18 13:20:43,401][2147304] Updated weights for policy 0, policy_version 80 (0.0003)
[2025-07-18 13:20:44,013][2147304] Updated weights for policy 0, policy_version 90 (0.0003)
[2025-07-18 13:20:44,607][2147304] Updated weights for policy 0, policy_version 100 (0.0003)
[2025-07-18 13:20:45,214][2147304] Updated weights for policy 0, policy_version 110 (0.0003)
[2025-07-18 13:20:45,838][2147304] Updated weights for policy 0, policy_version 120 (0.0003)
[2025-07-18 13:20:46,447][2147304] Updated weights for policy 0, policy_version 130 (0.0003)
[2025-07-18 13:20:47,058][2147304] Updated weights for policy 0, policy_version 140 (0.0003)
[2025-07-18 13:20:47,649][2147304] Updated weights for policy 0, policy_version 150 (0.0003)
[2025-07-18 13:20:48,248][2147304] Updated weights for policy 0, policy_version 160 (0.0003)
[2025-07-18 13:20:48,389][2147157] Fps is (10 sec: 66355.7, 60 sec: 66355.7, 300 sec: 66355.7). Total num frames: 663552. Throughput: 0: 12375.9. Samples: 123758. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)
[2025-07-18 13:20:48,389][2147157] Avg episode reward: [(0, '4.643')]
[2025-07-18 13:20:48,390][2147291] Saving new best policy, reward=4.643!
[2025-07-18 13:20:48,856][2147304] Updated weights for policy 0, policy_version 170 (0.0003)
[2025-07-18 13:20:49,460][2147304] Updated weights for policy 0, policy_version 180 (0.0003)
[2025-07-18 13:20:50,070][2147304] Updated weights for policy 0, policy_version 190 (0.0003)
[2025-07-18 13:20:50,668][2147304] Updated weights for policy 0, policy_version 200 (0.0003)
[2025-07-18 13:20:51,285][2147304] Updated weights for policy 0, policy_version 210 (0.0003)
[2025-07-18 13:20:51,890][2147304] Updated weights for policy 0, policy_version 220 (0.0003)
[2025-07-18 13:20:52,525][2147304] Updated weights for policy 0, policy_version 230 (0.0003)
[2025-07-18 13:20:53,136][2147304] Updated weights for policy 0, policy_version 240 (0.0003)
[2025-07-18 13:20:53,389][2147157] Fps is (10 sec: 67584.2, 60 sec: 66628.7, 300 sec: 66628.7). Total num frames: 999424. Throughput: 0: 15018.2. Samples: 225272. Policy #0 lag: (min: 0.0, avg: 0.7, max: 1.0)
[2025-07-18 13:20:53,389][2147157] Avg episode reward: [(0, '4.502')]
[2025-07-18 13:20:53,765][2147304] Updated weights for policy 0, policy_version 250 (0.0003)
[2025-07-18 13:20:54,366][2147304] Updated weights for policy 0, policy_version 260 (0.0003)
[2025-07-18 13:20:54,965][2147304] Updated weights for policy 0, policy_version 270 (0.0003)
[2025-07-18 13:20:55,129][2147157] Heartbeat connected on Batcher_0
[2025-07-18 13:20:55,132][2147157] Heartbeat connected on LearnerWorker_p0
[2025-07-18 13:20:55,136][2147157] Heartbeat connected on InferenceWorker_p0-w0
[2025-07-18 13:20:55,138][2147157] Heartbeat connected on RolloutWorker_w0
[2025-07-18 13:20:55,140][2147157] Heartbeat connected on RolloutWorker_w1
[2025-07-18 13:20:55,143][2147157] Heartbeat connected on RolloutWorker_w2
[2025-07-18 13:20:55,145][2147157] Heartbeat connected on RolloutWorker_w3
[2025-07-18 13:20:55,148][2147157] Heartbeat connected on RolloutWorker_w4
[2025-07-18 13:20:55,150][2147157] Heartbeat connected on RolloutWorker_w5
[2025-07-18 13:20:55,153][2147157] Heartbeat connected on RolloutWorker_w6
[2025-07-18 13:20:55,156][2147157] Heartbeat connected on RolloutWorker_w7
[2025-07-18 13:20:55,565][2147304] Updated weights for policy 0, policy_version 280 (0.0003)
[2025-07-18 13:20:56,156][2147304] Updated weights for policy 0, policy_version 290 (0.0003)
[2025-07-18 13:20:56,782][2147304] Updated weights for policy 0, policy_version 300 (0.0003)
[2025-07-18 13:20:57,405][2147304] Updated weights for policy 0, policy_version 310 (0.0003)
[2025-07-18 13:20:58,044][2147304] Updated weights for policy 0, policy_version 320 (0.0004)
[2025-07-18 13:20:58,389][2147157] Fps is (10 sec: 66764.7, 60 sec: 66560.2, 300 sec: 66560.2). Total num frames: 1331200. Throughput: 0: 16264.6. Samples: 325290. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)
[2025-07-18 13:20:58,389][2147157] Avg episode reward: [(0, '4.919')]
[2025-07-18 13:20:58,390][2147291] Saving new best policy, reward=4.919!
[2025-07-18 13:20:58,661][2147304] Updated weights for policy 0, policy_version 330 (0.0004)
[2025-07-18 13:20:59,281][2147304] Updated weights for policy 0, policy_version 340 (0.0003)
[2025-07-18 13:20:59,895][2147304] Updated weights for policy 0, policy_version 350 (0.0003)
[2025-07-18 13:21:00,507][2147304] Updated weights for policy 0, policy_version 360 (0.0003)
[2025-07-18 13:21:01,112][2147304] Updated weights for policy 0, policy_version 370 (0.0003)
[2025-07-18 13:21:01,708][2147304] Updated weights for policy 0, policy_version 380 (0.0003)
[2025-07-18 13:21:02,302][2147304] Updated weights for policy 0, policy_version 390 (0.0003)
[2025-07-18 13:21:02,931][2147304] Updated weights for policy 0, policy_version 400 (0.0003)
[2025-07-18 13:21:03,389][2147157] Fps is (10 sec: 66764.7, 60 sec: 66683.1, 300 sec: 66683.1). Total num frames: 1667072. Throughput: 0: 15017.5. Samples: 375436. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2025-07-18 13:21:03,389][2147157] Avg episode reward: [(0, '5.018')]
[2025-07-18 13:21:03,391][2147291] Saving new best policy, reward=5.018!
[2025-07-18 13:21:03,550][2147304] Updated weights for policy 0, policy_version 410 (0.0004)
[2025-07-18 13:21:04,150][2147304] Updated weights for policy 0, policy_version 420 (0.0003)
[2025-07-18 13:21:04,746][2147304] Updated weights for policy 0, policy_version 430 (0.0003)
[2025-07-18 13:21:05,369][2147304] Updated weights for policy 0, policy_version 440 (0.0003)
[2025-07-18 13:21:05,972][2147304] Updated weights for policy 0, policy_version 450 (0.0003)
[2025-07-18 13:21:06,605][2147304] Updated weights for policy 0, policy_version 460 (0.0004)
[2025-07-18 13:21:07,232][2147304] Updated weights for policy 0, policy_version 470 (0.0003)
[2025-07-18 13:21:07,881][2147304] Updated weights for policy 0, policy_version 480 (0.0004)
[2025-07-18 13:21:08,389][2147157] Fps is (10 sec: 66355.3, 60 sec: 66491.9, 300 sec: 66491.9). Total num frames: 1994752. Throughput: 0: 15860.8. Samples: 475824. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2025-07-18 13:21:08,389][2147157] Avg episode reward: [(0, '5.122')]
[2025-07-18 13:21:08,390][2147291] Saving new best policy, reward=5.122!
[2025-07-18 13:21:08,508][2147304] Updated weights for policy 0, policy_version 490 (0.0003)
[2025-07-18 13:21:09,112][2147304] Updated weights for policy 0, policy_version 500 (0.0003)
[2025-07-18 13:21:09,745][2147304] Updated weights for policy 0, policy_version 510 (0.0003)
[2025-07-18 13:21:10,377][2147304] Updated weights for policy 0, policy_version 520 (0.0004)
[2025-07-18 13:21:10,990][2147304] Updated weights for policy 0, policy_version 530 (0.0004)
[2025-07-18 13:21:11,621][2147304] Updated weights for policy 0, policy_version 540 (0.0004)
[2025-07-18 13:21:12,255][2147304] Updated weights for policy 0, policy_version 550 (0.0003)
[2025-07-18 13:21:12,879][2147304] Updated weights for policy 0, policy_version 560 (0.0003)
[2025-07-18 13:21:13,389][2147157] Fps is (10 sec: 65945.4, 60 sec: 66472.3, 300 sec: 66472.3). Total num frames: 2326528. Throughput: 0: 16391.6. Samples: 573706. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)
[2025-07-18 13:21:13,389][2147157] Avg episode reward: [(0, '6.305')]
[2025-07-18 13:21:13,391][2147291] Saving new best policy, reward=6.305!
[2025-07-18 13:21:13,503][2147304] Updated weights for policy 0, policy_version 570 (0.0003)
[2025-07-18 13:21:14,122][2147304] Updated weights for policy 0, policy_version 580 (0.0004)
[2025-07-18 13:21:14,731][2147304] Updated weights for policy 0, policy_version 590 (0.0003)
[2025-07-18 13:21:15,350][2147304] Updated weights for policy 0, policy_version 600 (0.0003)
[2025-07-18 13:21:15,974][2147304] Updated weights for policy 0, policy_version 610 (0.0003)
[2025-07-18 13:21:16,598][2147304] Updated weights for policy 0, policy_version 620 (0.0003)
[2025-07-18 13:21:17,217][2147304] Updated weights for policy 0, policy_version 630 (0.0003)
[2025-07-18 13:21:17,832][2147304] Updated weights for policy 0, policy_version 640 (0.0003)
[2025-07-18 13:21:18,389][2147157] Fps is (10 sec: 65945.3, 60 sec: 66355.3, 300 sec: 66355.3). Total num frames: 2654208. Throughput: 0: 15580.9. Samples: 623234. Policy #0 lag: (min: 0.0, avg: 0.6, max: 1.0)
[2025-07-18 13:21:18,389][2147157] Avg episode reward: [(0, '7.409')]
[2025-07-18 13:21:18,395][2147291] Saving new best policy, reward=7.409!
[2025-07-18 13:21:18,456][2147304] Updated weights for policy 0, policy_version 650 (0.0004)
[2025-07-18 13:21:19,083][2147304] Updated weights for policy 0, policy_version 660 (0.0004)
[2025-07-18 13:21:19,696][2147304] Updated weights for policy 0, policy_version 670 (0.0003)
[2025-07-18 13:21:20,331][2147304] Updated weights for policy 0, policy_version 680 (0.0004)
[2025-07-18 13:21:20,953][2147304] Updated weights for policy 0, policy_version 690 (0.0004)
[2025-07-18 13:21:21,545][2147304] Updated weights for policy 0, policy_version 700 (0.0003)
[2025-07-18 13:21:22,146][2147304] Updated weights for policy 0, policy_version 710 (0.0003)
[2025-07-18 13:21:22,765][2147304] Updated weights for policy 0, policy_version 720 (0.0004)
[2025-07-18 13:21:23,353][2147304] Updated weights for policy 0, policy_version 730 (0.0003)
[2025-07-18 13:21:23,389][2147157] Fps is (10 sec: 66355.2, 60 sec: 66446.3, 300 sec: 66446.3). Total num frames: 2990080. Throughput: 0: 16057.4. Samples: 722580. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2025-07-18 13:21:23,390][2147157] Avg episode reward: [(0, '8.941')]
[2025-07-18 13:21:23,391][2147291] Saving new best policy, reward=8.941!
[2025-07-18 13:21:23,974][2147304] Updated weights for policy 0, policy_version 740 (0.0004)
[2025-07-18 13:21:24,574][2147304] Updated weights for policy 0, policy_version 750 (0.0003)
[2025-07-18 13:21:25,170][2147304] Updated weights for policy 0, policy_version 760 (0.0003)
[2025-07-18 13:21:25,793][2147304] Updated weights for policy 0, policy_version 770 (0.0003)
[2025-07-18 13:21:26,408][2147304] Updated weights for policy 0, policy_version 780 (0.0003)
[2025-07-18 13:21:27,011][2147304] Updated weights for policy 0, policy_version 790 (0.0003)
[2025-07-18 13:21:27,632][2147304] Updated weights for policy 0, policy_version 800 (0.0003)
[2025-07-18 13:21:28,238][2147304] Updated weights for policy 0, policy_version 810 (0.0003)
[2025-07-18 13:21:28,389][2147157] Fps is (10 sec: 67174.4, 60 sec: 66519.1, 300 sec: 66519.1). Total num frames: 3325952. Throughput: 0: 16673.1. Samples: 823328. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)
[2025-07-18 13:21:28,389][2147157] Avg episode reward: [(0, '12.853')]
[2025-07-18 13:21:28,390][2147291] Saving new best policy, reward=12.853!
[2025-07-18 13:21:28,848][2147304] Updated weights for policy 0, policy_version 820 (0.0003)
[2025-07-18 13:21:29,462][2147304] Updated weights for policy 0, policy_version 830 (0.0003)
[2025-07-18 13:21:30,066][2147304] Updated weights for policy 0, policy_version 840 (0.0003)
[2025-07-18 13:21:30,664][2147304] Updated weights for policy 0, policy_version 850 (0.0003)
[2025-07-18 13:21:31,278][2147304] Updated weights for policy 0, policy_version 860 (0.0003)
[2025-07-18 13:21:31,888][2147304] Updated weights for policy 0, policy_version 870 (0.0003)
[2025-07-18 13:21:32,490][2147304] Updated weights for policy 0, policy_version 880 (0.0003)
[2025-07-18 13:21:33,091][2147304] Updated weights for policy 0, policy_version 890 (0.0003)
[2025-07-18 13:21:33,389][2147157] Fps is (10 sec: 67174.6, 60 sec: 66578.7, 300 sec: 66578.7). Total num frames: 3661824. Throughput: 0: 16675.7. Samples: 874164. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2025-07-18 13:21:33,389][2147157] Avg episode reward: [(0, '14.195')]
[2025-07-18 13:21:33,391][2147291] Saving new best policy, reward=14.195!
[2025-07-18 13:21:33,734][2147304] Updated weights for policy 0, policy_version 900 (0.0004)
[2025-07-18 13:21:34,351][2147304] Updated weights for policy 0, policy_version 910 (0.0004)
[2025-07-18 13:21:34,952][2147304] Updated weights for policy 0, policy_version 920 (0.0003)
[2025-07-18 13:21:35,556][2147304] Updated weights for policy 0, policy_version 930 (0.0003)
[2025-07-18 13:21:36,157][2147304] Updated weights for policy 0, policy_version 940 (0.0003)
[2025-07-18 13:21:36,790][2147304] Updated weights for policy 0, policy_version 950 (0.0003)
[2025-07-18 13:21:37,406][2147304] Updated weights for policy 0, policy_version 960 (0.0003)
[2025-07-18 13:21:38,024][2147304] Updated weights for policy 0, policy_version 970 (0.0004)
[2025-07-18 13:21:38,389][2147157] Fps is (10 sec: 67175.0, 60 sec: 66628.4, 300 sec: 66628.4). Total num frames: 3997696. Throughput: 0: 16652.7. Samples: 974644. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2025-07-18 13:21:38,389][2147157] Avg episode reward: [(0, '14.954')]
[2025-07-18 13:21:38,390][2147291] Saving new best policy, reward=14.954!
[2025-07-18 13:21:38,512][2147291] Stopping Batcher_0...
[2025-07-18 13:21:38,512][2147291] Saving /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 13:21:38,512][2147157] Component Batcher_0 stopped!
[2025-07-18 13:21:38,513][2147291] Loop batcher_evt_loop terminating...
[2025-07-18 13:21:38,526][2147304] Weights refcount: 2 0
[2025-07-18 13:21:38,527][2147304] Stopping InferenceWorker_p0-w0...
[2025-07-18 13:21:38,527][2147304] Loop inference_proc0-0_evt_loop terminating...
[2025-07-18 13:21:38,527][2147157] Component InferenceWorker_p0-w0 stopped!
[2025-07-18 13:21:38,532][2147305] Stopping RolloutWorker_w0...
[2025-07-18 13:21:38,532][2147305] Loop rollout_proc0_evt_loop terminating...
[2025-07-18 13:21:38,532][2147310] Stopping RolloutWorker_w5...
[2025-07-18 13:21:38,532][2147157] Component RolloutWorker_w0 stopped!
[2025-07-18 13:21:38,532][2147310] Loop rollout_proc5_evt_loop terminating...
[2025-07-18 13:21:38,533][2147309] Stopping RolloutWorker_w4...
[2025-07-18 13:21:38,533][2147157] Component RolloutWorker_w5 stopped!
[2025-07-18 13:21:38,533][2147309] Loop rollout_proc4_evt_loop terminating...
[2025-07-18 13:21:38,533][2147157] Component RolloutWorker_w4 stopped!
[2025-07-18 13:21:38,535][2147308] Stopping RolloutWorker_w2...
[2025-07-18 13:21:38,535][2147312] Stopping RolloutWorker_w7...
[2025-07-18 13:21:38,535][2147306] Stopping RolloutWorker_w1...
[2025-07-18 13:21:38,535][2147312] Loop rollout_proc7_evt_loop terminating...
[2025-07-18 13:21:38,535][2147308] Loop rollout_proc2_evt_loop terminating...
[2025-07-18 13:21:38,535][2147306] Loop rollout_proc1_evt_loop terminating...
[2025-07-18 13:21:38,535][2147157] Component RolloutWorker_w2 stopped!
[2025-07-18 13:21:38,535][2147311] Stopping RolloutWorker_w6...
[2025-07-18 13:21:38,535][2147157] Component RolloutWorker_w7 stopped!
[2025-07-18 13:21:38,535][2147157] Component RolloutWorker_w1 stopped!
[2025-07-18 13:21:38,535][2147311] Loop rollout_proc6_evt_loop terminating...
[2025-07-18 13:21:38,536][2147157] Component RolloutWorker_w6 stopped!
[2025-07-18 13:21:38,536][2147307] Stopping RolloutWorker_w3...
[2025-07-18 13:21:38,536][2147157] Component RolloutWorker_w3 stopped!
[2025-07-18 13:21:38,536][2147307] Loop rollout_proc3_evt_loop terminating...
[2025-07-18 13:21:38,554][2147291] Saving new best policy, reward=15.337!
[2025-07-18 13:21:38,590][2147291] Saving /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 13:21:38,629][2147291] Stopping LearnerWorker_p0...
[2025-07-18 13:21:38,630][2147291] Loop learner_proc0_evt_loop terminating...
[2025-07-18 13:21:38,629][2147157] Component LearnerWorker_p0 stopped!
[2025-07-18 13:21:38,630][2147157] Waiting for process learner_proc0 to stop...
[2025-07-18 13:21:39,170][2147157] Waiting for process inference_proc0-0 to join...
[2025-07-18 13:21:39,171][2147157] Waiting for process rollout_proc0 to join...
[2025-07-18 13:21:39,171][2147157] Waiting for process rollout_proc1 to join...
[2025-07-18 13:21:39,172][2147157] Waiting for process rollout_proc2 to join...
[2025-07-18 13:21:39,172][2147157] Waiting for process rollout_proc3 to join...
[2025-07-18 13:21:39,172][2147157] Waiting for process rollout_proc4 to join...
[2025-07-18 13:21:39,172][2147157] Waiting for process rollout_proc5 to join...
[2025-07-18 13:21:39,173][2147157] Waiting for process rollout_proc6 to join...
[2025-07-18 13:21:39,173][2147157] Waiting for process rollout_proc7 to join...
[2025-07-18 13:21:39,173][2147157] Batcher 0 profile tree view:
batching: 5.4762, releasing_batches: 0.0089
[2025-07-18 13:21:39,174][2147157] InferenceWorker_p0-w0 profile tree view:
wait_policy: 0.0000
  wait_policy_total: 1.3889
update_model: 0.8750
  weight_update: 0.0003
one_step: 0.0007
  handle_policy_step: 56.5497
    deserialize: 2.4514, stack: 0.2763, obs_to_device_normalize: 14.9017, forward: 23.3329, send_messages: 3.6635
    prepare_outputs: 9.8779
      to_cpu: 7.0270
[2025-07-18 13:21:39,174][2147157] Learner 0 profile tree view:
misc: 0.0022, prepare_batch: 2.9088
train: 7.6652
  epoch_init: 0.0018, minibatch_init: 0.0022, losses_postprocess: 0.1680, kl_divergence: 0.1547, after_optimizer: 1.6659
  calculate_losses: 3.1251
    losses_init: 0.0013, forward_head: 0.2315, bptt_initial: 1.8331, tail: 0.2077, advantages_returns: 0.0521, losses: 0.4075
    bptt: 0.3428
      bptt_forward_core: 0.3286
  update: 2.4197
    clip: 0.2703
[2025-07-18 13:21:39,174][2147157] RolloutWorker_w0 profile tree view:
wait_for_trajectories: 0.0431, enqueue_policy_requests: 2.1950, env_step: 33.4471, overhead: 1.4840, complete_rollouts: 0.0631
save_policy_outputs: 2.5574
  split_output_tensors: 0.8645
[2025-07-18 13:21:39,174][2147157] RolloutWorker_w7 profile tree view:
wait_for_trajectories: 0.0437, enqueue_policy_requests: 2.2374, env_step: 33.0313, overhead: 1.4855, complete_rollouts: 0.0654
save_policy_outputs: 2.5673
  split_output_tensors: 0.8608
[2025-07-18 13:21:39,174][2147157] Loop Runner_EvtLoop terminating...
[2025-07-18 13:21:39,175][2147157] Runner profile tree view:
main_loop: 64.0200
[2025-07-18 13:21:39,175][2147157] Collected {0: 4005888}, FPS: 62572.4
[2025-07-18 13:21:39,236][2147157] Could not query the git revision for the logs, perhaps git is not available
[2025-07-18 13:21:39,237][2147157] Loading existing experiment configuration from /home/isaacp/repos/rl_class/train_dir/default_experiment/config.json
[2025-07-18 13:21:39,237][2147157] Overriding arg 'num_workers' with value 1 passed from command line
[2025-07-18 13:21:39,237][2147157] Adding new argument 'no_render'=True that is not in the saved config file!
[2025-07-18 13:21:39,238][2147157] Adding new argument 'save_video'=True that is not in the saved config file!
[2025-07-18 13:21:39,238][2147157] Adding new argument 'video_frames'=1000000000.0 that is not in the saved config file!
[2025-07-18 13:21:39,238][2147157] Adding new argument 'video_name'=None that is not in the saved config file!
[2025-07-18 13:21:39,238][2147157] Adding new argument 'max_num_frames'=1000000000.0 that is not in the saved config file!
[2025-07-18 13:21:39,239][2147157] Adding new argument 'max_num_episodes'=10 that is not in the saved config file!
[2025-07-18 13:21:39,239][2147157] Adding new argument 'push_to_hub'=False that is not in the saved config file!
[2025-07-18 13:21:39,239][2147157] Adding new argument 'hf_repository'=None that is not in the saved config file!
[2025-07-18 13:21:39,239][2147157] Adding new argument 'policy_index'=0 that is not in the saved config file!
[2025-07-18 13:21:39,239][2147157] Adding new argument 'eval_deterministic'=False that is not in the saved config file!
[2025-07-18 13:21:39,240][2147157] Adding new argument 'train_script'=None that is not in the saved config file!
[2025-07-18 13:21:39,240][2147157] Adding new argument 'enjoy_script'=None that is not in the saved config file!
[2025-07-18 13:21:39,240][2147157] Using frameskip 1 and render_action_repeat=4 for evaluation
[2025-07-18 13:21:39,251][2147157] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:21:39,252][2147157] RunningMeanStd input shape: (3, 72, 128)
[2025-07-18 13:21:39,253][2147157] RunningMeanStd input shape: (1,)
[2025-07-18 13:21:39,257][2147157] ConvEncoder: input_channels=3
[2025-07-18 13:21:39,297][2147157] Conv encoder output size: 512
[2025-07-18 13:21:39,297][2147157] Policy head output size: 512
[2025-07-18 13:21:39,399][2147157] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 13:21:39,400][2147157] Could not load from checkpoint, attempt 0
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner.py", line 282, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
Please file an issue with the following so that we can make `weights_only=True` compatible with your use case: WeightsUnpickler error: Can only build Tensor, Parameter, OrderedDict or types allowlisted via `add_safe_globals`, but got <class 'numpy.dtypes.Float64DType'>

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 13:21:39,401][2147157] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 13:21:39,402][2147157] Could not load from checkpoint, attempt 1
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner.py", line 282, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
Please file an issue with the following so that we can make `weights_only=True` compatible with your use case: WeightsUnpickler error: Can only build Tensor, Parameter, OrderedDict or types allowlisted via `add_safe_globals`, but got <class 'numpy.dtypes.Float64DType'>

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 13:21:39,402][2147157] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 13:21:39,402][2147157] Could not load from checkpoint, attempt 2
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner.py", line 282, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
Please file an issue with the following so that we can make `weights_only=True` compatible with your use case: WeightsUnpickler error: Can only build Tensor, Parameter, OrderedDict or types allowlisted via `add_safe_globals`, but got <class 'numpy.dtypes.Float64DType'>

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 13:24:56,987][2147157] Could not query the git revision for the logs, perhaps git is not available
[2025-07-18 13:24:56,988][2147157] Loading existing experiment configuration from /home/isaacp/repos/rl_class/train_dir/default_experiment/config.json
[2025-07-18 13:24:56,988][2147157] Overriding arg 'num_workers' with value 1 passed from command line
[2025-07-18 13:24:56,989][2147157] Adding new argument 'no_render'=True that is not in the saved config file!
[2025-07-18 13:24:56,989][2147157] Adding new argument 'save_video'=True that is not in the saved config file!
[2025-07-18 13:24:56,989][2147157] Adding new argument 'video_frames'=1000000000.0 that is not in the saved config file!
[2025-07-18 13:24:56,989][2147157] Adding new argument 'video_name'=None that is not in the saved config file!
[2025-07-18 13:24:56,990][2147157] Adding new argument 'max_num_frames'=1000000000.0 that is not in the saved config file!
[2025-07-18 13:24:56,990][2147157] Adding new argument 'max_num_episodes'=10 that is not in the saved config file!
[2025-07-18 13:24:56,991][2147157] Adding new argument 'push_to_hub'=False that is not in the saved config file!
[2025-07-18 13:24:56,991][2147157] Adding new argument 'hf_repository'=None that is not in the saved config file!
[2025-07-18 13:24:56,991][2147157] Adding new argument 'policy_index'=0 that is not in the saved config file!
[2025-07-18 13:24:56,991][2147157] Adding new argument 'eval_deterministic'=False that is not in the saved config file!
[2025-07-18 13:24:56,992][2147157] Adding new argument 'train_script'=None that is not in the saved config file!
[2025-07-18 13:24:56,992][2147157] Adding new argument 'enjoy_script'=None that is not in the saved config file!
[2025-07-18 13:24:56,992][2147157] Using frameskip 1 and render_action_repeat=4 for evaluation
[2025-07-18 13:24:57,006][2147157] RunningMeanStd input shape: (3, 72, 128)
[2025-07-18 13:24:57,006][2147157] RunningMeanStd input shape: (1,)
[2025-07-18 13:24:57,010][2147157] ConvEncoder: input_channels=3
[2025-07-18 13:24:57,027][2147157] Conv encoder output size: 512
[2025-07-18 13:24:57,027][2147157] Policy head output size: 512
[2025-07-18 13:24:57,052][2147157] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 13:24:57,053][2147157] Could not load from checkpoint, attempt 0
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner.py", line 282, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device, weights_only=False)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
Please file an issue with the following so that we can make `weights_only=True` compatible with your use case: WeightsUnpickler error: Can only build Tensor, Parameter, OrderedDict or types allowlisted via `add_safe_globals`, but got <class 'numpy.dtypes.Float64DType'>

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 13:24:57,053][2147157] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 13:24:57,054][2147157] Could not load from checkpoint, attempt 1
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner.py", line 282, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device, weights_only=False)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
Please file an issue with the following so that we can make `weights_only=True` compatible with your use case: WeightsUnpickler error: Can only build Tensor, Parameter, OrderedDict or types allowlisted via `add_safe_globals`, but got <class 'numpy.dtypes.Float64DType'>

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 13:24:57,054][2147157] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 13:24:57,055][2147157] Could not load from checkpoint, attempt 2
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner.py", line 282, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device, weights_only=False)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/torch/serialization.py", line 1524, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
Please file an issue with the following so that we can make `weights_only=True` compatible with your use case: WeightsUnpickler error: Can only build Tensor, Parameter, OrderedDict or types allowlisted via `add_safe_globals`, but got <class 'numpy.dtypes.Float64DType'>

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2025-07-18 13:27:13,389][2147157] Environment doom_basic already registered, overwriting...
[2025-07-18 13:27:13,389][2147157] Environment doom_two_colors_easy already registered, overwriting...
[2025-07-18 13:27:13,389][2147157] Environment doom_two_colors_hard already registered, overwriting...
[2025-07-18 13:27:13,390][2147157] Environment doom_dm already registered, overwriting...
[2025-07-18 13:27:13,390][2147157] Environment doom_dwango5 already registered, overwriting...
[2025-07-18 13:27:13,390][2147157] Environment doom_my_way_home_flat_actions already registered, overwriting...
[2025-07-18 13:27:13,390][2147157] Environment doom_defend_the_center_flat_actions already registered, overwriting...
[2025-07-18 13:27:13,391][2147157] Environment doom_my_way_home already registered, overwriting...
[2025-07-18 13:27:13,391][2147157] Environment doom_deadly_corridor already registered, overwriting...
[2025-07-18 13:27:13,391][2147157] Environment doom_defend_the_center already registered, overwriting...
[2025-07-18 13:27:13,391][2147157] Environment doom_defend_the_line already registered, overwriting...
[2025-07-18 13:27:13,391][2147157] Environment doom_health_gathering already registered, overwriting...
[2025-07-18 13:27:13,391][2147157] Environment doom_health_gathering_supreme already registered, overwriting...
[2025-07-18 13:27:13,392][2147157] Environment doom_battle already registered, overwriting...
[2025-07-18 13:27:13,392][2147157] Environment doom_battle2 already registered, overwriting...
[2025-07-18 13:27:13,392][2147157] Environment doom_duel_bots already registered, overwriting...
[2025-07-18 13:27:13,392][2147157] Environment doom_deathmatch_bots already registered, overwriting...
[2025-07-18 13:27:13,392][2147157] Environment doom_duel already registered, overwriting...
[2025-07-18 13:27:13,392][2147157] Environment doom_deathmatch_full already registered, overwriting...
[2025-07-18 13:27:13,392][2147157] Environment doom_benchmark already registered, overwriting...
[2025-07-18 13:27:13,393][2147157] register_encoder_factory: <function make_vizdoom_encoder at 0x7c64701a1440>
[2025-07-18 13:27:13,397][2147157] Could not query the git revision for the logs, perhaps git is not available
[2025-07-18 13:27:13,398][2147157] Loading existing experiment configuration from /home/isaacp/repos/rl_class/train_dir/default_experiment/config.json
[2025-07-18 13:27:13,401][2147157] Experiment dir /home/isaacp/repos/rl_class/train_dir/default_experiment already exists!
[2025-07-18 13:27:13,401][2147157] Resuming existing experiment from /home/isaacp/repos/rl_class/train_dir/default_experiment...
[2025-07-18 13:27:13,401][2147157] Weights and Biases integration disabled
[2025-07-18 13:27:13,402][2147157] Environment var CUDA_VISIBLE_DEVICES is 0

[2025-07-18 13:27:14,335][2147157] Starting experiment with the following configuration:
help=False
algo=APPO
env=doom_health_gathering_supreme
experiment=default_experiment
train_dir=/home/isaacp/repos/rl_class/train_dir
restart_behavior=resume
device=gpu
seed=None
num_policies=1
async_rl=True
serial_mode=False
batched_sampling=False
num_batches_to_accumulate=2
worker_num_splits=2
policy_workers_per_policy=1
max_policy_lag=1000
num_workers=8
num_envs_per_worker=4
batch_size=1024
num_batches_per_epoch=1
num_epochs=1
rollout=32
recurrence=32
shuffle_minibatches=False
gamma=0.99
reward_scale=1.0
reward_clip=1000.0
value_bootstrap=False
normalize_returns=True
exploration_loss_coeff=0.001
value_loss_coeff=0.5
kl_loss_coeff=0.0
exploration_loss=symmetric_kl
gae_lambda=0.95
ppo_clip_ratio=0.1
ppo_clip_value=0.2
with_vtrace=False
vtrace_rho=1.0
vtrace_c=1.0
optimizer=adam
adam_eps=1e-06
adam_beta1=0.9
adam_beta2=0.999
max_grad_norm=4.0
learning_rate=0.0001
lr_schedule=constant
lr_schedule_kl_threshold=0.008
lr_adaptive_min=1e-06
lr_adaptive_max=0.01
obs_subtract_mean=0.0
obs_scale=255.0
normalize_input=True
normalize_input_keys=None
decorrelate_experience_max_seconds=0
decorrelate_envs_on_one_worker=True
actor_worker_gpus=[]
set_workers_cpu_affinity=True
force_envs_single_thread=False
default_niceness=0
log_to_file=True
experiment_summaries_interval=10
flush_summaries_interval=30
stats_avg=100
summaries_use_frameskip=True
heartbeat_interval=20
heartbeat_reporting_interval=600
train_for_env_steps=4000000
train_for_seconds=10000000000
save_every_sec=120
keep_checkpoints=2
load_checkpoint_kind=latest
save_milestones_sec=-1
save_best_every_sec=5
save_best_metric=reward
save_best_after=100000
benchmark=False
encoder_mlp_layers=[512, 512]
encoder_conv_architecture=convnet_simple
encoder_conv_mlp_layers=[512]
use_rnn=True
rnn_size=512
rnn_type=gru
rnn_num_layers=1
decoder_mlp_layers=[]
nonlinearity=elu
policy_initialization=orthogonal
policy_init_gain=1.0
actor_critic_share_weights=True
adaptive_stddev=True
continuous_tanh_scale=0.0
initial_stddev=1.0
use_env_info_cache=False
env_gpu_actions=False
env_gpu_observations=True
env_frameskip=4
env_framestack=1
pixel_format=CHW
use_record_episode_statistics=False
with_wandb=False
wandb_user=None
wandb_project=sample_factory
wandb_group=None
wandb_job_type=SF
wandb_tags=[]
with_pbt=False
pbt_mix_policies_in_one_env=True
pbt_period_env_steps=5000000
pbt_start_mutation=20000000
pbt_replace_fraction=0.3
pbt_mutation_rate=0.15
pbt_replace_reward_gap=0.1
pbt_replace_reward_gap_absolute=1e-06
pbt_optimize_gamma=False
pbt_target_objective=true_objective
pbt_perturb_min=1.1
pbt_perturb_max=1.5
num_agents=-1
num_humans=0
num_bots=-1
start_bot_difficulty=None
timelimit=None
res_w=128
res_h=72
wide_aspect_ratio=False
eval_env_frameskip=1
fps=35
command_line=--env=doom_health_gathering_supreme --num_workers=8 --num_envs_per_worker=4 --train_for_env_steps=4000000
cli_args={'env': 'doom_health_gathering_supreme', 'num_workers': 8, 'num_envs_per_worker': 4, 'train_for_env_steps': 4000000}
git_hash=unknown
git_repo_name=not a git repository
[2025-07-18 13:27:14,336][2147157] Saving configuration to /home/isaacp/repos/rl_class/train_dir/default_experiment/config.json...
[2025-07-18 13:27:14,338][2147157] Rollout worker 0 uses device cpu
[2025-07-18 13:27:14,339][2147157] Rollout worker 1 uses device cpu
[2025-07-18 13:27:14,339][2147157] Rollout worker 2 uses device cpu
[2025-07-18 13:27:14,339][2147157] Rollout worker 3 uses device cpu
[2025-07-18 13:27:14,339][2147157] Rollout worker 4 uses device cpu
[2025-07-18 13:27:14,339][2147157] Rollout worker 5 uses device cpu
[2025-07-18 13:27:14,340][2147157] Rollout worker 6 uses device cpu
[2025-07-18 13:27:14,340][2147157] Rollout worker 7 uses device cpu
[2025-07-18 13:27:14,364][2147157] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 13:27:14,364][2147157] InferenceWorker_p0-w0: min num requests: 2
[2025-07-18 13:27:14,384][2147157] Starting all processes...
[2025-07-18 13:27:14,385][2147157] Starting process learner_proc0
[2025-07-18 13:27:14,434][2147157] Starting all processes...
[2025-07-18 13:27:14,436][2147157] Starting process inference_proc0-0
[2025-07-18 13:27:14,436][2147157] Starting process rollout_proc0
[2025-07-18 13:27:14,437][2147157] Starting process rollout_proc1
[2025-07-18 13:27:14,437][2147157] Starting process rollout_proc2
[2025-07-18 13:27:14,437][2147157] Starting process rollout_proc3
[2025-07-18 13:27:14,437][2147157] Starting process rollout_proc4
[2025-07-18 13:27:14,438][2147157] Starting process rollout_proc5
[2025-07-18 13:27:14,438][2147157] Starting process rollout_proc6
[2025-07-18 13:27:14,439][2147157] Starting process rollout_proc7
[2025-07-18 13:27:15,334][2149303] Worker 4 uses CPU cores [16, 17, 18, 19]
[2025-07-18 13:27:15,345][2149299] Worker 3 uses CPU cores [12, 13, 14, 15]
[2025-07-18 13:27:15,349][2149282] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 13:27:15,349][2149282] Set environment var CUDA_VISIBLE_DEVICES to '0' (GPU indices [0]) for learning process 0
[2025-07-18 13:27:15,360][2149302] Worker 6 uses CPU cores [24, 25, 26, 27]
[2025-07-18 13:27:15,361][2149282] Num visible devices: 1
[2025-07-18 13:27:15,361][2149282] Starting seed is not provided
[2025-07-18 13:27:15,361][2149282] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 13:27:15,361][2149282] Initializing actor-critic model on device cuda:0
[2025-07-18 13:27:15,361][2149282] RunningMeanStd input shape: (3, 72, 128)
[2025-07-18 13:27:15,362][2149296] Worker 1 uses CPU cores [4, 5, 6, 7]
[2025-07-18 13:27:15,362][2149301] Worker 7 uses CPU cores [28, 29, 30, 31]
[2025-07-18 13:27:15,362][2149297] Worker 0 uses CPU cores [0, 1, 2, 3]
[2025-07-18 13:27:15,362][2149282] RunningMeanStd input shape: (1,)
[2025-07-18 13:27:15,365][2149298] Worker 2 uses CPU cores [8, 9, 10, 11]
[2025-07-18 13:27:15,367][2149282] ConvEncoder: input_channels=3
[2025-07-18 13:27:15,371][2149300] Worker 5 uses CPU cores [20, 21, 22, 23]
[2025-07-18 13:27:15,394][2149295] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 13:27:15,394][2149295] Set environment var CUDA_VISIBLE_DEVICES to '0' (GPU indices [0]) for inference process 0
[2025-07-18 13:27:15,406][2149295] Num visible devices: 1
[2025-07-18 13:27:15,407][2149282] Conv encoder output size: 512
[2025-07-18 13:27:15,407][2149282] Policy head output size: 512
[2025-07-18 13:27:15,414][2149282] Created Actor Critic model with architecture:
[2025-07-18 13:27:15,414][2149282] ActorCriticSharedWeights(
  (obs_normalizer): ObservationNormalizer(
    (running_mean_std): RunningMeanStdDictInPlace(
      (running_mean_std): ModuleDict(
        (obs): RunningMeanStdInPlace()
      )
    )
  )
  (returns_normalizer): RecursiveScriptModule(original_name=RunningMeanStdInPlace)
  (encoder): VizdoomEncoder(
    (basic_encoder): ConvEncoder(
      (enc): RecursiveScriptModule(
        original_name=ConvEncoderImpl
        (conv_head): RecursiveScriptModule(
          original_name=Sequential
          (0): RecursiveScriptModule(original_name=Conv2d)
          (1): RecursiveScriptModule(original_name=ELU)
          (2): RecursiveScriptModule(original_name=Conv2d)
          (3): RecursiveScriptModule(original_name=ELU)
          (4): RecursiveScriptModule(original_name=Conv2d)
          (5): RecursiveScriptModule(original_name=ELU)
        )
        (mlp_layers): RecursiveScriptModule(
          original_name=Sequential
          (0): RecursiveScriptModule(original_name=Linear)
          (1): RecursiveScriptModule(original_name=ELU)
        )
      )
    )
  )
  (core): ModelCoreRNN(
    (core): GRU(512, 512)
  )
  (decoder): MlpDecoder(
    (mlp): Identity()
  )
  (critic_linear): Linear(in_features=512, out_features=1, bias=True)
  (action_parameterization): ActionParameterizationDefault(
    (distribution_linear): Linear(in_features=512, out_features=5, bias=True)
  )
)
[2025-07-18 13:27:15,485][2149282] Using optimizer <class 'torch.optim.adam.Adam'>
[2025-07-18 13:27:16,004][2149282] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2025-07-18 13:27:16,020][2149282] Loading model from checkpoint
[2025-07-18 13:27:16,021][2149282] Loaded experiment state at self.train_step=978, self.env_steps=4005888
[2025-07-18 13:27:16,021][2149282] Initialized policy 0 weights for model version 978
[2025-07-18 13:27:16,022][2149282] LearnerWorker_p0 finished initialization!
[2025-07-18 13:27:16,022][2149282] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 13:27:16,069][2149295] RunningMeanStd input shape: (3, 72, 128)
[2025-07-18 13:27:16,070][2149295] RunningMeanStd input shape: (1,)
[2025-07-18 13:27:16,074][2149295] ConvEncoder: input_channels=3
[2025-07-18 13:27:16,113][2149295] Conv encoder output size: 512
[2025-07-18 13:27:16,113][2149295] Policy head output size: 512
[2025-07-18 13:27:16,132][2147157] Inference worker 0-0 is ready!
[2025-07-18 13:27:16,133][2147157] All inference workers are ready! Signal rollout workers to start!
[2025-07-18 13:27:16,149][2149297] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:27:16,149][2149303] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:27:16,149][2149300] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:27:16,149][2149299] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:27:16,149][2149301] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:27:16,149][2149296] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:27:16,149][2149302] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:27:16,150][2149298] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:27:16,294][2149300] Decorrelating experience for 0 frames...
[2025-07-18 13:27:16,296][2149297] Decorrelating experience for 0 frames...
[2025-07-18 13:27:16,297][2149303] Decorrelating experience for 0 frames...
[2025-07-18 13:27:16,298][2149302] Decorrelating experience for 0 frames...
[2025-07-18 13:27:16,300][2149298] Decorrelating experience for 0 frames...
[2025-07-18 13:27:16,300][2149301] Decorrelating experience for 0 frames...
[2025-07-18 13:27:16,300][2149299] Decorrelating experience for 0 frames...
[2025-07-18 13:27:16,424][2149300] Decorrelating experience for 32 frames...
[2025-07-18 13:27:16,429][2149298] Decorrelating experience for 32 frames...
[2025-07-18 13:27:16,429][2149301] Decorrelating experience for 32 frames...
[2025-07-18 13:27:16,429][2149299] Decorrelating experience for 32 frames...
[2025-07-18 13:27:16,429][2149302] Decorrelating experience for 32 frames...
[2025-07-18 13:27:16,440][2149296] Decorrelating experience for 0 frames...
[2025-07-18 13:27:16,565][2149296] Decorrelating experience for 32 frames...
[2025-07-18 13:27:16,573][2149297] Decorrelating experience for 32 frames...
[2025-07-18 13:27:16,578][2149300] Decorrelating experience for 64 frames...
[2025-07-18 13:27:16,592][2149299] Decorrelating experience for 64 frames...
[2025-07-18 13:27:16,592][2149301] Decorrelating experience for 64 frames...
[2025-07-18 13:27:16,592][2149302] Decorrelating experience for 64 frames...
[2025-07-18 13:27:16,719][2149300] Decorrelating experience for 96 frames...
[2025-07-18 13:27:16,720][2149296] Decorrelating experience for 64 frames...
[2025-07-18 13:27:16,722][2149303] Decorrelating experience for 32 frames...
[2025-07-18 13:27:16,727][2149297] Decorrelating experience for 64 frames...
[2025-07-18 13:27:16,735][2149299] Decorrelating experience for 96 frames...
[2025-07-18 13:27:16,735][2149302] Decorrelating experience for 96 frames...
[2025-07-18 13:27:16,859][2149296] Decorrelating experience for 96 frames...
[2025-07-18 13:27:16,863][2149297] Decorrelating experience for 96 frames...
[2025-07-18 13:27:16,879][2149303] Decorrelating experience for 64 frames...
[2025-07-18 13:27:16,886][2149301] Decorrelating experience for 96 frames...
[2025-07-18 13:27:16,990][2149298] Decorrelating experience for 64 frames...
[2025-07-18 13:27:17,014][2149303] Decorrelating experience for 96 frames...
[2025-07-18 13:27:17,144][2149298] Decorrelating experience for 96 frames...
[2025-07-18 13:27:17,211][2149282] Signal inference workers to stop experience collection...
[2025-07-18 13:27:17,224][2149295] InferenceWorker_p0-w0: stopping experience collection
[2025-07-18 13:27:17,693][2149282] Signal inference workers to resume experience collection...
[2025-07-18 13:27:17,693][2149282] Stopping Batcher_0...
[2025-07-18 13:27:17,693][2149282] Loop batcher_evt_loop terminating...
[2025-07-18 13:27:17,695][2147157] Component Batcher_0 stopped!
[2025-07-18 13:27:17,703][2149295] Weights refcount: 2 0
[2025-07-18 13:27:17,704][2149295] Stopping InferenceWorker_p0-w0...
[2025-07-18 13:27:17,704][2149295] Loop inference_proc0-0_evt_loop terminating...
[2025-07-18 13:27:17,704][2147157] Component InferenceWorker_p0-w0 stopped!
[2025-07-18 13:27:17,709][2149303] Stopping RolloutWorker_w4...
[2025-07-18 13:27:17,710][2149297] Stopping RolloutWorker_w0...
[2025-07-18 13:27:17,710][2149303] Loop rollout_proc4_evt_loop terminating...
[2025-07-18 13:27:17,710][2149297] Loop rollout_proc0_evt_loop terminating...
[2025-07-18 13:27:17,710][2149300] Stopping RolloutWorker_w5...
[2025-07-18 13:27:17,710][2147157] Component RolloutWorker_w4 stopped!
[2025-07-18 13:27:17,710][2149300] Loop rollout_proc5_evt_loop terminating...
[2025-07-18 13:27:17,710][2147157] Component RolloutWorker_w0 stopped!
[2025-07-18 13:27:17,711][2149296] Stopping RolloutWorker_w1...
[2025-07-18 13:27:17,711][2149296] Loop rollout_proc1_evt_loop terminating...
[2025-07-18 13:27:17,711][2147157] Component RolloutWorker_w5 stopped!
[2025-07-18 13:27:17,711][2149301] Stopping RolloutWorker_w7...
[2025-07-18 13:27:17,711][2149298] Stopping RolloutWorker_w2...
[2025-07-18 13:27:17,711][2149301] Loop rollout_proc7_evt_loop terminating...
[2025-07-18 13:27:17,711][2149298] Loop rollout_proc2_evt_loop terminating...
[2025-07-18 13:27:17,711][2149302] Stopping RolloutWorker_w6...
[2025-07-18 13:27:17,711][2149302] Loop rollout_proc6_evt_loop terminating...
[2025-07-18 13:27:17,711][2147157] Component RolloutWorker_w1 stopped!
[2025-07-18 13:27:17,712][2149299] Stopping RolloutWorker_w3...
[2025-07-18 13:27:17,712][2147157] Component RolloutWorker_w7 stopped!
[2025-07-18 13:27:17,712][2149299] Loop rollout_proc3_evt_loop terminating...
[2025-07-18 13:27:17,712][2147157] Component RolloutWorker_w2 stopped!
[2025-07-18 13:27:17,712][2147157] Component RolloutWorker_w6 stopped!
[2025-07-18 13:27:17,713][2147157] Component RolloutWorker_w3 stopped!
[2025-07-18 13:27:17,781][2149282] Saving /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000980_4014080.pth...
[2025-07-18 13:27:17,811][2149282] Saving /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000980_4014080.pth...
[2025-07-18 13:27:17,848][2149282] Stopping LearnerWorker_p0...
[2025-07-18 13:27:17,848][2149282] Loop learner_proc0_evt_loop terminating...
[2025-07-18 13:27:17,848][2147157] Component LearnerWorker_p0 stopped!
[2025-07-18 13:27:17,849][2147157] Waiting for process learner_proc0 to stop...
[2025-07-18 13:27:18,345][2147157] Waiting for process inference_proc0-0 to join...
[2025-07-18 13:27:18,346][2147157] Waiting for process rollout_proc0 to join...
[2025-07-18 13:27:18,347][2147157] Waiting for process rollout_proc1 to join...
[2025-07-18 13:27:18,347][2147157] Waiting for process rollout_proc2 to join...
[2025-07-18 13:27:18,348][2147157] Waiting for process rollout_proc3 to join...
[2025-07-18 13:27:18,348][2147157] Waiting for process rollout_proc4 to join...
[2025-07-18 13:27:18,348][2147157] Waiting for process rollout_proc5 to join...
[2025-07-18 13:27:18,349][2147157] Waiting for process rollout_proc6 to join...
[2025-07-18 13:27:18,349][2147157] Waiting for process rollout_proc7 to join...
[2025-07-18 13:27:18,350][2147157] Batcher 0 profile tree view:
batching: 0.0290, releasing_batches: 0.0002
[2025-07-18 13:27:18,350][2147157] InferenceWorker_p0-w0 profile tree view:
update_model: 0.0019
wait_policy: 0.0000
  wait_policy_total: 0.6105
one_step: 0.0008
  handle_policy_step: 0.4677
    deserialize: 0.0107, stack: 0.0010, obs_to_device_normalize: 0.0826, forward: 0.3302, send_messages: 0.0121
    prepare_outputs: 0.0241
      to_cpu: 0.0143
[2025-07-18 13:27:18,350][2147157] Learner 0 profile tree view:
misc: 0.0000, prepare_batch: 0.3398
train: 0.3190
  epoch_init: 0.0000, minibatch_init: 0.0000, losses_postprocess: 0.0001, kl_divergence: 0.0046, after_optimizer: 0.0172
  calculate_losses: 0.1073
    losses_init: 0.0000, forward_head: 0.0457, bptt_initial: 0.0400, tail: 0.0088, advantages_returns: 0.0004, losses: 0.0110
    bptt: 0.0013
      bptt_forward_core: 0.0013
  update: 0.1893
    clip: 0.0176
[2025-07-18 13:27:18,350][2147157] RolloutWorker_w0 profile tree view:
wait_for_trajectories: 0.0003, enqueue_policy_requests: 0.0100, env_step: 0.0991, overhead: 0.0047, complete_rollouts: 0.0002
save_policy_outputs: 0.0085
  split_output_tensors: 0.0028
[2025-07-18 13:27:18,350][2147157] RolloutWorker_w7 profile tree view:
wait_for_trajectories: 0.0003, enqueue_policy_requests: 0.0101, env_step: 0.1100, overhead: 0.0051, complete_rollouts: 0.0002
save_policy_outputs: 0.0086
  split_output_tensors: 0.0029
[2025-07-18 13:27:18,351][2147157] Loop Runner_EvtLoop terminating...
[2025-07-18 13:27:18,351][2147157] Runner profile tree view:
main_loop: 3.9667
[2025-07-18 13:27:18,351][2147157] Collected {0: 4014080}, FPS: 2065.2
[2025-07-18 13:27:18,360][2147157] Could not query the git revision for the logs, perhaps git is not available
[2025-07-18 13:27:18,361][2147157] Loading existing experiment configuration from /home/isaacp/repos/rl_class/train_dir/default_experiment/config.json
[2025-07-18 13:27:18,361][2147157] Overriding arg 'num_workers' with value 1 passed from command line
[2025-07-18 13:27:18,361][2147157] Adding new argument 'no_render'=True that is not in the saved config file!
[2025-07-18 13:27:18,361][2147157] Adding new argument 'save_video'=True that is not in the saved config file!
[2025-07-18 13:27:18,362][2147157] Adding new argument 'video_frames'=1000000000.0 that is not in the saved config file!
[2025-07-18 13:27:18,362][2147157] Adding new argument 'video_name'=None that is not in the saved config file!
[2025-07-18 13:27:18,362][2147157] Adding new argument 'max_num_frames'=1000000000.0 that is not in the saved config file!
[2025-07-18 13:27:18,362][2147157] Adding new argument 'max_num_episodes'=10 that is not in the saved config file!
[2025-07-18 13:27:18,362][2147157] Adding new argument 'push_to_hub'=False that is not in the saved config file!
[2025-07-18 13:27:18,363][2147157] Adding new argument 'hf_repository'=None that is not in the saved config file!
[2025-07-18 13:27:18,363][2147157] Adding new argument 'policy_index'=0 that is not in the saved config file!
[2025-07-18 13:27:18,363][2147157] Adding new argument 'eval_deterministic'=False that is not in the saved config file!
[2025-07-18 13:27:18,363][2147157] Adding new argument 'train_script'=None that is not in the saved config file!
[2025-07-18 13:27:18,363][2147157] Adding new argument 'enjoy_script'=None that is not in the saved config file!
[2025-07-18 13:27:18,363][2147157] Using frameskip 1 and render_action_repeat=4 for evaluation
[2025-07-18 13:27:18,373][2147157] RunningMeanStd input shape: (3, 72, 128)
[2025-07-18 13:27:18,374][2147157] RunningMeanStd input shape: (1,)
[2025-07-18 13:27:18,378][2147157] ConvEncoder: input_channels=3
[2025-07-18 13:27:18,392][2147157] Conv encoder output size: 512
[2025-07-18 13:27:18,392][2147157] Policy head output size: 512
[2025-07-18 13:27:18,402][2147157] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000980_4014080.pth...
[2025-07-18 13:27:18,788][2147157] Num frames 100...
[2025-07-18 13:27:18,832][2147157] Num frames 200...
[2025-07-18 13:27:18,875][2147157] Num frames 300...
[2025-07-18 13:27:18,918][2147157] Num frames 400...
[2025-07-18 13:27:18,961][2147157] Num frames 500...
[2025-07-18 13:27:19,005][2147157] Num frames 600...
[2025-07-18 13:27:19,050][2147157] Num frames 700...
[2025-07-18 13:27:19,096][2147157] Num frames 800...
[2025-07-18 13:27:19,152][2147157] Avg episode rewards: #0: 14.070, true rewards: #0: 8.070
[2025-07-18 13:27:19,152][2147157] Avg episode reward: 14.070, avg true_objective: 8.070
[2025-07-18 13:27:19,194][2147157] Num frames 900...
[2025-07-18 13:27:19,239][2147157] Num frames 1000...
[2025-07-18 13:27:19,285][2147157] Num frames 1100...
[2025-07-18 13:27:19,331][2147157] Num frames 1200...
[2025-07-18 13:27:19,394][2147157] Avg episode rewards: #0: 10.115, true rewards: #0: 6.115
[2025-07-18 13:27:19,395][2147157] Avg episode reward: 10.115, avg true_objective: 6.115
[2025-07-18 13:27:19,433][2147157] Num frames 1300...
[2025-07-18 13:27:19,479][2147157] Num frames 1400...
[2025-07-18 13:27:19,525][2147157] Num frames 1500...
[2025-07-18 13:27:19,571][2147157] Num frames 1600...
[2025-07-18 13:27:19,617][2147157] Num frames 1700...
[2025-07-18 13:27:19,663][2147157] Num frames 1800...
[2025-07-18 13:27:19,709][2147157] Num frames 1900...
[2025-07-18 13:27:19,756][2147157] Num frames 2000...
[2025-07-18 13:27:19,802][2147157] Num frames 2100...
[2025-07-18 13:27:19,847][2147157] Num frames 2200...
[2025-07-18 13:27:19,893][2147157] Num frames 2300...
[2025-07-18 13:27:19,939][2147157] Num frames 2400...
[2025-07-18 13:27:19,985][2147157] Num frames 2500...
[2025-07-18 13:27:20,067][2147157] Avg episode rewards: #0: 16.223, true rewards: #0: 8.557
[2025-07-18 13:27:20,068][2147157] Avg episode reward: 16.223, avg true_objective: 8.557
[2025-07-18 13:27:20,084][2147157] Num frames 2600...
[2025-07-18 13:27:20,130][2147157] Num frames 2700...
[2025-07-18 13:27:20,176][2147157] Num frames 2800...
[2025-07-18 13:27:20,222][2147157] Num frames 2900...
[2025-07-18 13:27:20,265][2147157] Num frames 3000...
[2025-07-18 13:27:20,330][2147157] Avg episode rewards: #0: 14.070, true rewards: #0: 7.570
[2025-07-18 13:27:20,331][2147157] Avg episode reward: 14.070, avg true_objective: 7.570
[2025-07-18 13:27:20,364][2147157] Num frames 3100...
[2025-07-18 13:27:20,408][2147157] Num frames 3200...
[2025-07-18 13:27:20,454][2147157] Num frames 3300...
[2025-07-18 13:27:20,500][2147157] Num frames 3400...
[2025-07-18 13:27:20,545][2147157] Num frames 3500...
[2025-07-18 13:27:20,591][2147157] Num frames 3600...
[2025-07-18 13:27:20,637][2147157] Num frames 3700...
[2025-07-18 13:27:20,717][2147157] Avg episode rewards: #0: 13.928, true rewards: #0: 7.528
[2025-07-18 13:27:20,718][2147157] Avg episode reward: 13.928, avg true_objective: 7.528
[2025-07-18 13:27:20,735][2147157] Num frames 3800...
[2025-07-18 13:27:20,778][2147157] Num frames 3900...
[2025-07-18 13:27:20,821][2147157] Num frames 4000...
[2025-07-18 13:27:20,864][2147157] Num frames 4100...
[2025-07-18 13:27:20,907][2147157] Num frames 4200...
[2025-07-18 13:27:20,949][2147157] Num frames 4300...
[2025-07-18 13:27:21,005][2147157] Avg episode rewards: #0: 13.180, true rewards: #0: 7.180
[2025-07-18 13:27:21,005][2147157] Avg episode reward: 13.180, avg true_objective: 7.180
[2025-07-18 13:27:21,045][2147157] Num frames 4400...
[2025-07-18 13:27:21,088][2147157] Num frames 4500...
[2025-07-18 13:27:21,130][2147157] Num frames 4600...
[2025-07-18 13:27:21,172][2147157] Num frames 4700...
[2025-07-18 13:27:21,215][2147157] Num frames 4800...
[2025-07-18 13:27:21,258][2147157] Num frames 4900...
[2025-07-18 13:27:21,300][2147157] Num frames 5000...
[2025-07-18 13:27:21,357][2147157] Avg episode rewards: #0: 13.303, true rewards: #0: 7.160
[2025-07-18 13:27:21,357][2147157] Avg episode reward: 13.303, avg true_objective: 7.160
[2025-07-18 13:27:21,394][2147157] Num frames 5100...
[2025-07-18 13:27:21,436][2147157] Num frames 5200...
[2025-07-18 13:27:21,481][2147157] Num frames 5300...
[2025-07-18 13:27:21,527][2147157] Num frames 5400...
[2025-07-18 13:27:21,571][2147157] Num frames 5500...
[2025-07-18 13:27:21,647][2147157] Avg episode rewards: #0: 12.820, true rewards: #0: 6.945
[2025-07-18 13:27:21,647][2147157] Avg episode reward: 12.820, avg true_objective: 6.945
[2025-07-18 13:27:21,667][2147157] Num frames 5600...
[2025-07-18 13:27:21,712][2147157] Num frames 5700...
[2025-07-18 13:27:21,757][2147157] Num frames 5800...
[2025-07-18 13:27:21,803][2147157] Num frames 5900...
[2025-07-18 13:27:21,849][2147157] Num frames 6000...
[2025-07-18 13:27:21,895][2147157] Num frames 6100...
[2025-07-18 13:27:21,941][2147157] Num frames 6200...
[2025-07-18 13:27:21,986][2147157] Num frames 6300...
[2025-07-18 13:27:22,030][2147157] Num frames 6400...
[2025-07-18 13:27:22,076][2147157] Num frames 6500...
[2025-07-18 13:27:22,163][2147157] Avg episode rewards: #0: 13.978, true rewards: #0: 7.311
[2025-07-18 13:27:22,164][2147157] Avg episode reward: 13.978, avg true_objective: 7.311
[2025-07-18 13:27:22,173][2147157] Num frames 6600...
[2025-07-18 13:27:22,219][2147157] Num frames 6700...
[2025-07-18 13:27:22,264][2147157] Num frames 6800...
[2025-07-18 13:27:22,309][2147157] Num frames 6900...
[2025-07-18 13:27:22,354][2147157] Num frames 7000...
[2025-07-18 13:27:22,399][2147157] Num frames 7100...
[2025-07-18 13:27:22,489][2147157] Avg episode rewards: #0: 13.488, true rewards: #0: 7.188
[2025-07-18 13:27:22,490][2147157] Avg episode reward: 13.488, avg true_objective: 7.188
[2025-07-18 13:27:28,336][2147157] Replay video saved to /home/isaacp/repos/rl_class/train_dir/default_experiment/replay.mp4!
[2025-07-18 13:34:05,393][2151665] Saving configuration to /home/isaacp/repos/rl_class/train_dir/default_experiment/config.json...
[2025-07-18 13:34:05,395][2151665] Rollout worker 0 uses device cpu
[2025-07-18 13:34:05,395][2151665] Rollout worker 1 uses device cpu
[2025-07-18 13:34:05,396][2151665] Rollout worker 2 uses device cpu
[2025-07-18 13:34:05,396][2151665] Rollout worker 3 uses device cpu
[2025-07-18 13:34:05,396][2151665] Rollout worker 4 uses device cpu
[2025-07-18 13:34:05,396][2151665] Rollout worker 5 uses device cpu
[2025-07-18 13:34:05,396][2151665] Rollout worker 6 uses device cpu
[2025-07-18 13:34:05,397][2151665] Rollout worker 7 uses device cpu
[2025-07-18 13:34:05,421][2151665] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 13:34:05,422][2151665] InferenceWorker_p0-w0: min num requests: 2
[2025-07-18 13:34:05,440][2151665] Starting all processes...
[2025-07-18 13:34:05,441][2151665] Starting process learner_proc0
[2025-07-18 13:34:05,490][2151665] Starting all processes...
[2025-07-18 13:34:05,493][2151665] Starting process inference_proc0-0
[2025-07-18 13:34:05,493][2151665] Starting process rollout_proc0
[2025-07-18 13:34:05,493][2151665] Starting process rollout_proc1
[2025-07-18 13:34:05,493][2151665] Starting process rollout_proc2
[2025-07-18 13:34:05,493][2151665] Starting process rollout_proc3
[2025-07-18 13:34:05,494][2151665] Starting process rollout_proc4
[2025-07-18 13:34:05,494][2151665] Starting process rollout_proc5
[2025-07-18 13:34:05,495][2151665] Starting process rollout_proc6
[2025-07-18 13:34:05,495][2151665] Starting process rollout_proc7
[2025-07-18 13:34:06,344][2151802] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 13:34:06,344][2151802] Set environment var CUDA_VISIBLE_DEVICES to '0' (GPU indices [0]) for learning process 0
[2025-07-18 13:34:06,355][2151802] Num visible devices: 1
[2025-07-18 13:34:06,356][2151802] Starting seed is not provided
[2025-07-18 13:34:06,356][2151802] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 13:34:06,356][2151802] Initializing actor-critic model on device cuda:0
[2025-07-18 13:34:06,356][2151802] RunningMeanStd input shape: (3, 72, 128)
[2025-07-18 13:34:06,357][2151802] RunningMeanStd input shape: (1,)
[2025-07-18 13:34:06,362][2151802] ConvEncoder: input_channels=3
[2025-07-18 13:34:06,371][2151821] Worker 5 uses CPU cores [20, 21, 22, 23]
[2025-07-18 13:34:06,373][2151823] Worker 6 uses CPU cores [24, 25, 26, 27]
[2025-07-18 13:34:06,377][2151820] Worker 4 uses CPU cores [16, 17, 18, 19]
[2025-07-18 13:34:06,383][2151816] Worker 0 uses CPU cores [0, 1, 2, 3]
[2025-07-18 13:34:06,383][2151822] Worker 7 uses CPU cores [28, 29, 30, 31]
[2025-07-18 13:34:06,383][2151817] Worker 1 uses CPU cores [4, 5, 6, 7]
[2025-07-18 13:34:06,383][2151819] Worker 3 uses CPU cores [12, 13, 14, 15]
[2025-07-18 13:34:06,384][2151818] Worker 2 uses CPU cores [8, 9, 10, 11]
[2025-07-18 13:34:06,402][2151802] Conv encoder output size: 512
[2025-07-18 13:34:06,402][2151802] Policy head output size: 512
[2025-07-18 13:34:06,402][2151815] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 13:34:06,402][2151815] Set environment var CUDA_VISIBLE_DEVICES to '0' (GPU indices [0]) for inference process 0
[2025-07-18 13:34:06,408][2151802] Created Actor Critic model with architecture:
[2025-07-18 13:34:06,408][2151802] ActorCriticSharedWeights(
  (obs_normalizer): ObservationNormalizer(
    (running_mean_std): RunningMeanStdDictInPlace(
      (running_mean_std): ModuleDict(
        (obs): RunningMeanStdInPlace()
      )
    )
  )
  (returns_normalizer): RecursiveScriptModule(original_name=RunningMeanStdInPlace)
  (encoder): VizdoomEncoder(
    (basic_encoder): ConvEncoder(
      (enc): RecursiveScriptModule(
        original_name=ConvEncoderImpl
        (conv_head): RecursiveScriptModule(
          original_name=Sequential
          (0): RecursiveScriptModule(original_name=Conv2d)
          (1): RecursiveScriptModule(original_name=ELU)
          (2): RecursiveScriptModule(original_name=Conv2d)
          (3): RecursiveScriptModule(original_name=ELU)
          (4): RecursiveScriptModule(original_name=Conv2d)
          (5): RecursiveScriptModule(original_name=ELU)
        )
        (mlp_layers): RecursiveScriptModule(
          original_name=Sequential
          (0): RecursiveScriptModule(original_name=Linear)
          (1): RecursiveScriptModule(original_name=ELU)
        )
      )
    )
  )
  (core): ModelCoreRNN(
    (core): GRU(512, 512)
  )
  (decoder): MlpDecoder(
    (mlp): Identity()
  )
  (critic_linear): Linear(in_features=512, out_features=1, bias=True)
  (action_parameterization): ActionParameterizationDefault(
    (distribution_linear): Linear(in_features=512, out_features=5, bias=True)
  )
)
[2025-07-18 13:34:06,414][2151815] Num visible devices: 1
[2025-07-18 13:34:06,474][2151802] Using optimizer <class 'torch.optim.adam.Adam'>
[2025-07-18 13:34:06,990][2151802] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000980_4014080.pth...
[2025-07-18 13:34:07,006][2151802] Loading model from checkpoint
[2025-07-18 13:34:07,007][2151802] Loaded experiment state at self.train_step=980, self.env_steps=4014080
[2025-07-18 13:34:07,007][2151802] Initialized policy 0 weights for model version 980
[2025-07-18 13:34:07,007][2151802] LearnerWorker_p0 finished initialization!
[2025-07-18 13:34:07,008][2151802] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 13:34:07,053][2151815] RunningMeanStd input shape: (3, 72, 128)
[2025-07-18 13:34:07,054][2151815] RunningMeanStd input shape: (1,)
[2025-07-18 13:34:07,059][2151815] ConvEncoder: input_channels=3
[2025-07-18 13:34:07,097][2151815] Conv encoder output size: 512
[2025-07-18 13:34:07,098][2151815] Policy head output size: 512
[2025-07-18 13:34:07,117][2151665] Inference worker 0-0 is ready!
[2025-07-18 13:34:07,118][2151665] All inference workers are ready! Signal rollout workers to start!
[2025-07-18 13:34:07,134][2151817] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:34:07,134][2151821] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:34:07,134][2151818] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:34:07,134][2151819] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:34:07,135][2151822] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:34:07,135][2151823] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:34:07,141][2151820] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:34:07,141][2151816] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:34:07,276][2151821] Decorrelating experience for 0 frames...
[2025-07-18 13:34:07,277][2151817] Decorrelating experience for 0 frames...
[2025-07-18 13:34:07,280][2151820] Decorrelating experience for 0 frames...
[2025-07-18 13:34:07,282][2151816] Decorrelating experience for 0 frames...
[2025-07-18 13:34:07,283][2151818] Decorrelating experience for 0 frames...
[2025-07-18 13:34:07,284][2151823] Decorrelating experience for 0 frames...
[2025-07-18 13:34:07,404][2151817] Decorrelating experience for 32 frames...
[2025-07-18 13:34:07,404][2151820] Decorrelating experience for 32 frames...
[2025-07-18 13:34:07,405][2151816] Decorrelating experience for 32 frames...
[2025-07-18 13:34:07,414][2151823] Decorrelating experience for 32 frames...
[2025-07-18 13:34:07,425][2151822] Decorrelating experience for 0 frames...
[2025-07-18 13:34:07,451][2151821] Decorrelating experience for 32 frames...
[2025-07-18 13:34:07,552][2151822] Decorrelating experience for 32 frames...
[2025-07-18 13:34:07,565][2151817] Decorrelating experience for 64 frames...
[2025-07-18 13:34:07,565][2151818] Decorrelating experience for 32 frames...
[2025-07-18 13:34:07,604][2151821] Decorrelating experience for 64 frames...
[2025-07-18 13:34:07,702][2151817] Decorrelating experience for 96 frames...
[2025-07-18 13:34:07,710][2151820] Decorrelating experience for 64 frames...
[2025-07-18 13:34:07,713][2151822] Decorrelating experience for 64 frames...
[2025-07-18 13:34:07,722][2151823] Decorrelating experience for 64 frames...
[2025-07-18 13:34:07,847][2151821] Decorrelating experience for 96 frames...
[2025-07-18 13:34:07,859][2151822] Decorrelating experience for 96 frames...
[2025-07-18 13:34:07,861][2151823] Decorrelating experience for 96 frames...
[2025-07-18 13:34:07,872][2151818] Decorrelating experience for 64 frames...
[2025-07-18 13:34:07,997][2151820] Decorrelating experience for 96 frames...
[2025-07-18 13:34:08,012][2151816] Decorrelating experience for 64 frames...
[2025-07-18 13:34:08,019][2151818] Decorrelating experience for 96 frames...
[2025-07-18 13:34:08,158][2151816] Decorrelating experience for 96 frames...
[2025-07-18 13:34:08,171][2151802] Signal inference workers to stop experience collection...
[2025-07-18 13:34:08,175][2151815] InferenceWorker_p0-w0: stopping experience collection
[2025-07-18 13:34:08,198][2151819] Decorrelating experience for 0 frames...
[2025-07-18 13:34:08,322][2151819] Decorrelating experience for 32 frames...
[2025-07-18 13:34:08,476][2151819] Decorrelating experience for 64 frames...
[2025-07-18 13:34:08,611][2151819] Decorrelating experience for 96 frames...
[2025-07-18 13:34:08,641][2151802] Signal inference workers to resume experience collection...
[2025-07-18 13:34:08,641][2151815] InferenceWorker_p0-w0: resuming experience collection
[2025-07-18 13:34:08,641][2151802] Stopping Batcher_0...
[2025-07-18 13:34:08,641][2151802] Loop batcher_evt_loop terminating...
[2025-07-18 13:34:08,643][2151665] Component Batcher_0 stopped!
[2025-07-18 13:34:08,653][2151815] Weights refcount: 2 0
[2025-07-18 13:34:08,654][2151815] Stopping InferenceWorker_p0-w0...
[2025-07-18 13:34:08,654][2151815] Loop inference_proc0-0_evt_loop terminating...
[2025-07-18 13:34:08,654][2151665] Component InferenceWorker_p0-w0 stopped!
[2025-07-18 13:34:08,657][2151816] Stopping RolloutWorker_w0...
[2025-07-18 13:34:08,657][2151817] Stopping RolloutWorker_w1...
[2025-07-18 13:34:08,657][2151820] Stopping RolloutWorker_w4...
[2025-07-18 13:34:08,657][2151816] Loop rollout_proc0_evt_loop terminating...
[2025-07-18 13:34:08,657][2151817] Loop rollout_proc1_evt_loop terminating...
[2025-07-18 13:34:08,657][2151820] Loop rollout_proc4_evt_loop terminating...
[2025-07-18 13:34:08,657][2151821] Stopping RolloutWorker_w5...
[2025-07-18 13:34:08,657][2151665] Component RolloutWorker_w0 stopped!
[2025-07-18 13:34:08,658][2151821] Loop rollout_proc5_evt_loop terminating...
[2025-07-18 13:34:08,658][2151665] Component RolloutWorker_w1 stopped!
[2025-07-18 13:34:08,658][2151665] Component RolloutWorker_w4 stopped!
[2025-07-18 13:34:08,658][2151665] Component RolloutWorker_w5 stopped!
[2025-07-18 13:34:08,659][2151823] Stopping RolloutWorker_w6...
[2025-07-18 13:34:08,659][2151822] Stopping RolloutWorker_w7...
[2025-07-18 13:34:08,659][2151818] Stopping RolloutWorker_w2...
[2025-07-18 13:34:08,659][2151823] Loop rollout_proc6_evt_loop terminating...
[2025-07-18 13:34:08,659][2151665] Component RolloutWorker_w6 stopped!
[2025-07-18 13:34:08,659][2151822] Loop rollout_proc7_evt_loop terminating...
[2025-07-18 13:34:08,659][2151818] Loop rollout_proc2_evt_loop terminating...
[2025-07-18 13:34:08,659][2151819] Stopping RolloutWorker_w3...
[2025-07-18 13:34:08,660][2151819] Loop rollout_proc3_evt_loop terminating...
[2025-07-18 13:34:08,660][2151665] Component RolloutWorker_w7 stopped!
[2025-07-18 13:34:08,660][2151665] Component RolloutWorker_w2 stopped!
[2025-07-18 13:34:08,660][2151665] Component RolloutWorker_w3 stopped!
[2025-07-18 13:34:08,724][2151802] Saving /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000982_4022272.pth...
[2025-07-18 13:34:08,753][2151802] Removing /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth
[2025-07-18 13:34:08,758][2151802] Saving /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000982_4022272.pth...
[2025-07-18 13:34:08,797][2151802] Stopping LearnerWorker_p0...
[2025-07-18 13:34:08,797][2151802] Loop learner_proc0_evt_loop terminating...
[2025-07-18 13:34:08,797][2151665] Component LearnerWorker_p0 stopped!
[2025-07-18 13:34:08,798][2151665] Waiting for process learner_proc0 to stop...
[2025-07-18 13:34:09,278][2151665] Waiting for process inference_proc0-0 to join...
[2025-07-18 13:34:09,279][2151665] Waiting for process rollout_proc0 to join...
[2025-07-18 13:34:09,279][2151665] Waiting for process rollout_proc1 to join...
[2025-07-18 13:34:09,280][2151665] Waiting for process rollout_proc2 to join...
[2025-07-18 13:34:09,280][2151665] Waiting for process rollout_proc3 to join...
[2025-07-18 13:34:09,280][2151665] Waiting for process rollout_proc4 to join...
[2025-07-18 13:34:09,281][2151665] Waiting for process rollout_proc5 to join...
[2025-07-18 13:34:09,281][2151665] Waiting for process rollout_proc6 to join...
[2025-07-18 13:34:09,281][2151665] Waiting for process rollout_proc7 to join...
[2025-07-18 13:34:09,282][2151665] Batcher 0 profile tree view:
batching: 0.0192, releasing_batches: 0.0003
[2025-07-18 13:34:09,282][2151665] InferenceWorker_p0-w0 profile tree view:
update_model: 0.0019
wait_policy: 0.0000
  wait_policy_total: 0.6026
one_step: 0.0007
  handle_policy_step: 0.4419
    deserialize: 0.0089, stack: 0.0010, obs_to_device_normalize: 0.0811, forward: 0.3102, send_messages: 0.0105
    prepare_outputs: 0.0228
      to_cpu: 0.0130
[2025-07-18 13:34:09,282][2151665] Learner 0 profile tree view:
misc: 0.0000, prepare_batch: 0.3246
train: 0.3158
  epoch_init: 0.0000, minibatch_init: 0.0000, losses_postprocess: 0.0001, kl_divergence: 0.0046, after_optimizer: 0.0201
  calculate_losses: 0.1058
    losses_init: 0.0000, forward_head: 0.0444, bptt_initial: 0.0394, tail: 0.0089, advantages_returns: 0.0004, losses: 0.0112
    bptt: 0.0013
      bptt_forward_core: 0.0013
  update: 0.1847
    clip: 0.0172
[2025-07-18 13:34:09,283][2151665] RolloutWorker_w0 profile tree view:
wait_for_trajectories: 0.0001, enqueue_policy_requests: 0.0002
[2025-07-18 13:34:09,283][2151665] RolloutWorker_w7 profile tree view:
wait_for_trajectories: 0.0003, enqueue_policy_requests: 0.0113, env_step: 0.1095, overhead: 0.0049, complete_rollouts: 0.0002
save_policy_outputs: 0.0093
  split_output_tensors: 0.0031
[2025-07-18 13:34:09,283][2151665] Loop Runner_EvtLoop terminating...
[2025-07-18 13:34:09,283][2151665] Runner profile tree view:
main_loop: 3.8433
[2025-07-18 13:34:09,284][2151665] Collected {0: 4022272}, FPS: 2131.5
[2025-07-18 13:34:09,346][2151665] Could not query the git revision for the logs, perhaps git is not available
[2025-07-18 13:34:09,347][2151665] Loading existing experiment configuration from /home/isaacp/repos/rl_class/train_dir/default_experiment/config.json
[2025-07-18 13:34:09,347][2151665] Overriding arg 'num_workers' with value 1 passed from command line
[2025-07-18 13:34:09,347][2151665] Adding new argument 'no_render'=True that is not in the saved config file!
[2025-07-18 13:34:09,347][2151665] Adding new argument 'save_video'=True that is not in the saved config file!
[2025-07-18 13:34:09,347][2151665] Adding new argument 'video_frames'=1000000000.0 that is not in the saved config file!
[2025-07-18 13:34:09,348][2151665] Adding new argument 'video_name'=None that is not in the saved config file!
[2025-07-18 13:34:09,348][2151665] Adding new argument 'max_num_frames'=1000000000.0 that is not in the saved config file!
[2025-07-18 13:34:09,348][2151665] Adding new argument 'max_num_episodes'=10 that is not in the saved config file!
[2025-07-18 13:34:09,348][2151665] Adding new argument 'push_to_hub'=False that is not in the saved config file!
[2025-07-18 13:34:09,349][2151665] Adding new argument 'hf_repository'=None that is not in the saved config file!
[2025-07-18 13:34:09,349][2151665] Adding new argument 'policy_index'=0 that is not in the saved config file!
[2025-07-18 13:34:09,349][2151665] Adding new argument 'eval_deterministic'=False that is not in the saved config file!
[2025-07-18 13:34:09,349][2151665] Adding new argument 'train_script'=None that is not in the saved config file!
[2025-07-18 13:34:09,349][2151665] Adding new argument 'enjoy_script'=None that is not in the saved config file!
[2025-07-18 13:34:09,350][2151665] Using frameskip 1 and render_action_repeat=4 for evaluation
[2025-07-18 13:34:09,362][2151665] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 13:34:09,362][2151665] RunningMeanStd input shape: (3, 72, 128)
[2025-07-18 13:34:09,363][2151665] RunningMeanStd input shape: (1,)
[2025-07-18 13:34:09,368][2151665] ConvEncoder: input_channels=3
[2025-07-18 13:34:09,406][2151665] Conv encoder output size: 512
[2025-07-18 13:34:09,406][2151665] Policy head output size: 512
[2025-07-18 13:34:09,482][2151665] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000982_4022272.pth...
[2025-07-18 13:34:09,847][2151665] Num frames 100...
[2025-07-18 13:34:09,891][2151665] Num frames 200...
[2025-07-18 13:34:09,934][2151665] Num frames 300...
[2025-07-18 13:34:09,977][2151665] Num frames 400...
[2025-07-18 13:34:10,019][2151665] Num frames 500...
[2025-07-18 13:34:10,063][2151665] Num frames 600...
[2025-07-18 13:34:10,104][2151665] Num frames 700...
[2025-07-18 13:34:10,185][2151665] Avg episode rewards: #0: 14.680, true rewards: #0: 7.680
[2025-07-18 13:34:10,185][2151665] Avg episode reward: 14.680, avg true_objective: 7.680
[2025-07-18 13:34:10,199][2151665] Num frames 800...
[2025-07-18 13:34:10,241][2151665] Num frames 900...
[2025-07-18 13:34:10,283][2151665] Num frames 1000...
[2025-07-18 13:34:10,324][2151665] Num frames 1100...
[2025-07-18 13:34:10,366][2151665] Num frames 1200...
[2025-07-18 13:34:10,408][2151665] Num frames 1300...
[2025-07-18 13:34:10,491][2151665] Avg episode rewards: #0: 12.380, true rewards: #0: 6.880
[2025-07-18 13:34:10,491][2151665] Avg episode reward: 12.380, avg true_objective: 6.880
[2025-07-18 13:34:10,503][2151665] Num frames 1400...
[2025-07-18 13:34:10,545][2151665] Num frames 1500...
[2025-07-18 13:34:10,586][2151665] Num frames 1600...
[2025-07-18 13:34:10,630][2151665] Num frames 1700...
[2025-07-18 13:34:10,672][2151665] Num frames 1800...
[2025-07-18 13:34:10,761][2151665] Avg episode rewards: #0: 10.627, true rewards: #0: 6.293
[2025-07-18 13:34:10,761][2151665] Avg episode reward: 10.627, avg true_objective: 6.293
[2025-07-18 13:34:10,767][2151665] Num frames 1900...
[2025-07-18 13:34:10,810][2151665] Num frames 2000...
[2025-07-18 13:34:10,853][2151665] Num frames 2100...
[2025-07-18 13:34:10,897][2151665] Num frames 2200...
[2025-07-18 13:34:10,940][2151665] Num frames 2300...
[2025-07-18 13:34:10,983][2151665] Num frames 2400...
[2025-07-18 13:34:11,026][2151665] Num frames 2500...
[2025-07-18 13:34:11,070][2151665] Num frames 2600...
[2025-07-18 13:34:11,132][2151665] Avg episode rewards: #0: 12.060, true rewards: #0: 6.560
[2025-07-18 13:34:11,132][2151665] Avg episode reward: 12.060, avg true_objective: 6.560
[2025-07-18 13:34:11,166][2151665] Num frames 2700...
[2025-07-18 13:34:11,208][2151665] Num frames 2800...
[2025-07-18 13:34:11,295][2151665] Avg episode rewards: #0: 10.160, true rewards: #0: 5.760
[2025-07-18 13:34:11,295][2151665] Avg episode reward: 10.160, avg true_objective: 5.760
[2025-07-18 13:34:11,304][2151665] Num frames 2900...
[2025-07-18 13:34:11,347][2151665] Num frames 3000...
[2025-07-18 13:34:11,392][2151665] Num frames 3100...
[2025-07-18 13:34:11,436][2151665] Num frames 3200...
[2025-07-18 13:34:11,479][2151665] Num frames 3300...
[2025-07-18 13:34:11,522][2151665] Num frames 3400...
[2025-07-18 13:34:11,564][2151665] Num frames 3500...
[2025-07-18 13:34:11,607][2151665] Num frames 3600...
[2025-07-18 13:34:11,649][2151665] Num frames 3700...
[2025-07-18 13:34:11,720][2151665] Avg episode rewards: #0: 10.907, true rewards: #0: 6.240
[2025-07-18 13:34:11,720][2151665] Avg episode reward: 10.907, avg true_objective: 6.240
[2025-07-18 13:34:11,744][2151665] Num frames 3800...
[2025-07-18 13:34:11,786][2151665] Num frames 3900...
[2025-07-18 13:34:11,853][2151665] Avg episode rewards: #0: 9.909, true rewards: #0: 5.623
[2025-07-18 13:34:11,854][2151665] Avg episode reward: 9.909, avg true_objective: 5.623
[2025-07-18 13:34:11,881][2151665] Num frames 4000...
[2025-07-18 13:34:11,923][2151665] Num frames 4100...
[2025-07-18 13:34:11,964][2151665] Num frames 4200...
[2025-07-18 13:34:12,006][2151665] Num frames 4300...
[2025-07-18 13:34:12,048][2151665] Num frames 4400...
[2025-07-18 13:34:12,090][2151665] Num frames 4500...
[2025-07-18 13:34:12,131][2151665] Num frames 4600...
[2025-07-18 13:34:12,173][2151665] Num frames 4700...
[2025-07-18 13:34:12,214][2151665] Num frames 4800...
[2025-07-18 13:34:12,257][2151665] Num frames 4900...
[2025-07-18 13:34:12,300][2151665] Num frames 5000...
[2025-07-18 13:34:12,342][2151665] Num frames 5100...
[2025-07-18 13:34:12,416][2151665] Avg episode rewards: #0: 12.065, true rewards: #0: 6.440
[2025-07-18 13:34:12,416][2151665] Avg episode reward: 12.065, avg true_objective: 6.440
[2025-07-18 13:34:12,437][2151665] Num frames 5200...
[2025-07-18 13:34:12,479][2151665] Num frames 5300...
[2025-07-18 13:34:12,521][2151665] Num frames 5400...
[2025-07-18 13:34:12,563][2151665] Num frames 5500...
[2025-07-18 13:34:12,605][2151665] Num frames 5600...
[2025-07-18 13:34:12,648][2151665] Num frames 5700...
[2025-07-18 13:34:12,712][2151665] Avg episode rewards: #0: 11.698, true rewards: #0: 6.364
[2025-07-18 13:34:12,712][2151665] Avg episode reward: 11.698, avg true_objective: 6.364
[2025-07-18 13:34:12,743][2151665] Num frames 5800...
[2025-07-18 13:34:12,786][2151665] Num frames 5900...
[2025-07-18 13:34:12,829][2151665] Num frames 6000...
[2025-07-18 13:34:12,875][2151665] Num frames 6100...
[2025-07-18 13:34:12,920][2151665] Num frames 6200...
[2025-07-18 13:34:12,962][2151665] Num frames 6300...
[2025-07-18 13:34:13,015][2151665] Avg episode rewards: #0: 11.704, true rewards: #0: 6.304
[2025-07-18 13:34:13,016][2151665] Avg episode reward: 11.704, avg true_objective: 6.304
[2025-07-18 13:34:18,073][2151665] Replay video saved to /home/isaacp/repos/rl_class/train_dir/default_experiment/replay.mp4!
[2025-07-18 13:36:15,543][2152645] Saving configuration to /home/isaacp/repos/rl_class/train_dir/default_experiment/config.json...
[2025-07-18 13:36:15,545][2152645] Rollout worker 0 uses device cpu
[2025-07-18 13:36:15,546][2152645] Rollout worker 1 uses device cpu
[2025-07-18 13:36:15,546][2152645] Rollout worker 2 uses device cpu
[2025-07-18 13:36:15,546][2152645] Rollout worker 3 uses device cpu
[2025-07-18 13:36:15,546][2152645] Rollout worker 4 uses device cpu
[2025-07-18 13:36:15,546][2152645] Rollout worker 5 uses device cpu
[2025-07-18 13:36:15,547][2152645] Rollout worker 6 uses device cpu
[2025-07-18 13:36:15,547][2152645] Rollout worker 7 uses device cpu
[2025-07-18 13:36:15,572][2152645] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 13:36:15,572][2152645] InferenceWorker_p0-w0: min num requests: 2
[2025-07-18 13:36:15,592][2152645] Starting all processes...
[2025-07-18 13:36:15,592][2152645] Starting process learner_proc0
[2025-07-18 13:36:15,642][2152645] Starting all processes...
[2025-07-18 13:36:15,644][2152645] Starting process inference_proc0-0
[2025-07-18 13:36:15,645][2152645] Starting process rollout_proc0
[2025-07-18 13:36:15,645][2152645] Starting process rollout_proc1
[2025-07-18 13:36:15,645][2152645] Starting process rollout_proc2
[2025-07-18 13:36:15,645][2152645] Starting process rollout_proc3
[2025-07-18 13:36:15,645][2152645] Starting process rollout_proc4
[2025-07-18 13:36:15,645][2152645] Starting process rollout_proc5
[2025-07-18 13:36:15,645][2152645] Starting process rollout_proc6
[2025-07-18 13:36:15,647][2152645] Starting process rollout_proc7
[2025-07-18 13:36:16,495][2152775] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 13:36:16,495][2152775] Set environment var CUDA_VISIBLE_DEVICES to '0' (GPU indices [0]) for learning process 0
[2025-07-18 13:36:16,507][2152775] Num visible devices: 1
[2025-07-18 13:36:16,507][2152775] Starting seed is not provided
[2025-07-18 13:36:16,507][2152775] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 13:36:16,508][2152775] Initializing actor-critic model on device cuda:0
[2025-07-18 13:36:16,508][2152775] RunningMeanStd input shape: (23,)
[2025-07-18 13:36:16,508][2152775] RunningMeanStd input shape: (3, 72, 128)
[2025-07-18 13:36:16,509][2152775] RunningMeanStd input shape: (1,)
[2025-07-18 13:36:16,514][2152775] ConvEncoder: input_channels=3
[2025-07-18 13:36:16,519][2152796] Worker 7 uses CPU cores [28, 29, 30, 31]
[2025-07-18 13:36:16,520][2152795] Worker 6 uses CPU cores [24, 25, 26, 27]
[2025-07-18 13:36:16,521][2152794] Worker 5 uses CPU cores [20, 21, 22, 23]
[2025-07-18 13:36:16,545][2152791] Worker 0 uses CPU cores [0, 1, 2, 3]
[2025-07-18 13:36:16,552][2152793] Worker 3 uses CPU cores [12, 13, 14, 15]
[2025-07-18 13:36:16,554][2152775] Conv encoder output size: 512
[2025-07-18 13:36:16,555][2152775] Policy head output size: 640
[2025-07-18 13:36:16,558][2152792] Worker 4 uses CPU cores [16, 17, 18, 19]
[2025-07-18 13:36:16,563][2152775] Created Actor Critic model with architecture:
[2025-07-18 13:36:16,563][2152775] ActorCriticSharedWeights(
  (obs_normalizer): ObservationNormalizer(
    (running_mean_std): RunningMeanStdDictInPlace(
      (running_mean_std): ModuleDict(
        (measurements): RunningMeanStdInPlace()
        (obs): RunningMeanStdInPlace()
      )
    )
  )
  (returns_normalizer): RecursiveScriptModule(original_name=RunningMeanStdInPlace)
  (encoder): VizdoomEncoder(
    (basic_encoder): ConvEncoder(
      (enc): RecursiveScriptModule(
        original_name=ConvEncoderImpl
        (conv_head): RecursiveScriptModule(
          original_name=Sequential
          (0): RecursiveScriptModule(original_name=Conv2d)
          (1): RecursiveScriptModule(original_name=ELU)
          (2): RecursiveScriptModule(original_name=Conv2d)
          (3): RecursiveScriptModule(original_name=ELU)
          (4): RecursiveScriptModule(original_name=Conv2d)
          (5): RecursiveScriptModule(original_name=ELU)
        )
        (mlp_layers): RecursiveScriptModule(
          original_name=Sequential
          (0): RecursiveScriptModule(original_name=Linear)
          (1): RecursiveScriptModule(original_name=ELU)
        )
      )
    )
    (measurements_head): Sequential(
      (0): Linear(in_features=23, out_features=128, bias=True)
      (1): ELU(alpha=1.0)
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ELU(alpha=1.0)
    )
  )
  (core): ModelCoreRNN(
    (core): GRU(640, 512)
  )
  (decoder): MlpDecoder(
    (mlp): Identity()
  )
  (critic_linear): Linear(in_features=512, out_features=1, bias=True)
  (action_parameterization): ActionParameterizationDefault(
    (distribution_linear): Linear(in_features=512, out_features=39, bias=True)
  )
)
[2025-07-18 13:36:16,565][2152788] Worker 1 uses CPU cores [4, 5, 6, 7]
[2025-07-18 13:36:16,633][2152775] Using optimizer <class 'torch.optim.adam.Adam'>
[2025-07-18 13:36:16,635][2152790] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 13:36:16,635][2152790] Set environment var CUDA_VISIBLE_DEVICES to '0' (GPU indices [0]) for inference process 0
[2025-07-18 13:36:16,635][2152789] Worker 2 uses CPU cores [8, 9, 10, 11]
[2025-07-18 13:36:16,646][2152790] Num visible devices: 1
[2025-07-18 13:36:17,139][2152775] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000982_4022272.pth...
[2025-07-18 13:36:17,155][2152775] Loading model from checkpoint
[2025-07-18 13:36:17,155][2152775] EvtLoop [learner_proc0_evt_loop, process=learner_proc0] unhandled exception in slot='init' connected to emitter=Emitter(object_id='Runner_EvtLoop', signal_name='start'), args=()
Traceback (most recent call last):
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/signal_slot/signal_slot.py", line 355, in _process_signal
    slot_callable(*args)
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner_worker.py", line 139, in init
    init_model_data = self.learner.init()
                      ^^^^^^^^^^^^^^^^^^^
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner.py", line 245, in init
    self.load_from_checkpoint(self.policy_id)
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner.py", line 308, in load_from_checkpoint
    self._load_state(checkpoint_dict, load_progress=load_progress)
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/sample_factory/algo/learning/learner.py", line 292, in _load_state
    self.actor_critic.load_state_dict(checkpoint_dict["model"])
  File "/home/isaacp/miniconda3/envs/rl_class_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for ActorCriticSharedWeights:
	Missing key(s) in state_dict: "obs_normalizer.running_mean_std.running_mean_std.measurements.running_mean", "obs_normalizer.running_mean_std.running_mean_std.measurements.running_var", "obs_normalizer.running_mean_std.running_mean_std.measurements.count", "encoder.measurements_head.0.weight", "encoder.measurements_head.0.bias", "encoder.measurements_head.2.weight", "encoder.measurements_head.2.bias". 
	size mismatch for core.core.weight_ih_l0: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 640]).
	size mismatch for action_parameterization.distribution_linear.weight: copying a param with shape torch.Size([5, 512]) from checkpoint, the shape in current model is torch.Size([39, 512]).
	size mismatch for action_parameterization.distribution_linear.bias: copying a param with shape torch.Size([5]) from checkpoint, the shape in current model is torch.Size([39]).
[2025-07-18 13:36:17,156][2152775] Unhandled exception Error(s) in loading state_dict for ActorCriticSharedWeights:
	Missing key(s) in state_dict: "obs_normalizer.running_mean_std.running_mean_std.measurements.running_mean", "obs_normalizer.running_mean_std.running_mean_std.measurements.running_var", "obs_normalizer.running_mean_std.running_mean_std.measurements.count", "encoder.measurements_head.0.weight", "encoder.measurements_head.0.bias", "encoder.measurements_head.2.weight", "encoder.measurements_head.2.bias". 
	size mismatch for core.core.weight_ih_l0: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([1536, 640]).
	size mismatch for action_parameterization.distribution_linear.weight: copying a param with shape torch.Size([5, 512]) from checkpoint, the shape in current model is torch.Size([39, 512]).
	size mismatch for action_parameterization.distribution_linear.bias: copying a param with shape torch.Size([5]) from checkpoint, the shape in current model is torch.Size([39]). in evt loop learner_proc0_evt_loop
[2025-07-18 13:36:30,842][2152645] Keyboard interrupt detected in the event loop EvtLoop [Runner_EvtLoop, process=main process 2152645], exiting...
[2025-07-18 13:36:30,843][2152789] Stopping RolloutWorker_w2...
[2025-07-18 13:36:30,843][2152792] Stopping RolloutWorker_w4...
[2025-07-18 13:36:30,843][2152793] Stopping RolloutWorker_w3...
[2025-07-18 13:36:30,843][2152791] Stopping RolloutWorker_w0...
[2025-07-18 13:36:30,843][2152795] Stopping RolloutWorker_w6...
[2025-07-18 13:36:30,843][2152790] Stopping InferenceWorker_p0-w0...
[2025-07-18 13:36:30,843][2152775] Stopping Batcher_0...
[2025-07-18 13:36:30,843][2152794] Stopping RolloutWorker_w5...
[2025-07-18 13:36:30,843][2152788] Stopping RolloutWorker_w1...
[2025-07-18 13:36:30,843][2152796] Stopping RolloutWorker_w7...
[2025-07-18 13:36:30,843][2152789] Loop rollout_proc2_evt_loop terminating...
[2025-07-18 13:36:30,842][2152645] Runner profile tree view:
main_loop: 15.2508
[2025-07-18 13:36:30,843][2152775] Loop batcher_evt_loop terminating...
[2025-07-18 13:36:30,843][2152792] Loop rollout_proc4_evt_loop terminating...
[2025-07-18 13:36:30,843][2152793] Loop rollout_proc3_evt_loop terminating...
[2025-07-18 13:36:30,843][2152795] Loop rollout_proc6_evt_loop terminating...
[2025-07-18 13:36:30,843][2152791] Loop rollout_proc0_evt_loop terminating...
[2025-07-18 13:36:30,843][2152790] Loop inference_proc0-0_evt_loop terminating...
[2025-07-18 13:36:30,843][2152796] Loop rollout_proc7_evt_loop terminating...
[2025-07-18 13:36:30,843][2152794] Loop rollout_proc5_evt_loop terminating...
[2025-07-18 13:36:30,843][2152788] Loop rollout_proc1_evt_loop terminating...
[2025-07-18 13:36:30,843][2152645] Collected {}, FPS: 0.0
[2025-07-18 13:38:23,614][2152645] Environment doom_basic already registered, overwriting...
[2025-07-18 13:38:23,614][2152645] Environment doom_two_colors_easy already registered, overwriting...
[2025-07-18 13:38:23,614][2152645] Environment doom_two_colors_hard already registered, overwriting...
[2025-07-18 13:38:23,615][2152645] Environment doom_dm already registered, overwriting...
[2025-07-18 13:38:23,615][2152645] Environment doom_dwango5 already registered, overwriting...
[2025-07-18 13:38:23,616][2152645] Environment doom_my_way_home_flat_actions already registered, overwriting...
[2025-07-18 13:38:23,616][2152645] Environment doom_defend_the_center_flat_actions already registered, overwriting...
[2025-07-18 13:38:23,616][2152645] Environment doom_my_way_home already registered, overwriting...
[2025-07-18 13:38:23,616][2152645] Environment doom_deadly_corridor already registered, overwriting...
[2025-07-18 13:38:23,617][2152645] Environment doom_defend_the_center already registered, overwriting...
[2025-07-18 13:38:23,617][2152645] Environment doom_defend_the_line already registered, overwriting...
[2025-07-18 13:38:23,617][2152645] Environment doom_health_gathering already registered, overwriting...
[2025-07-18 13:38:23,617][2152645] Environment doom_health_gathering_supreme already registered, overwriting...
[2025-07-18 13:38:23,618][2152645] Environment doom_battle already registered, overwriting...
[2025-07-18 13:38:23,618][2152645] Environment doom_battle2 already registered, overwriting...
[2025-07-18 13:38:23,618][2152645] Environment doom_duel_bots already registered, overwriting...
[2025-07-18 13:38:23,619][2152645] Environment doom_deathmatch_bots already registered, overwriting...
[2025-07-18 13:38:23,619][2152645] Environment doom_duel already registered, overwriting...
[2025-07-18 13:38:23,619][2152645] Environment doom_deathmatch_full already registered, overwriting...
[2025-07-18 13:38:23,619][2152645] Environment doom_benchmark already registered, overwriting...
[2025-07-18 13:38:23,619][2152645] register_encoder_factory: <function make_vizdoom_encoder at 0x79a128b6cfe0>
[2025-07-18 13:38:23,626][2152645] Could not query the git revision for the logs, perhaps git is not available
[2025-07-18 13:38:23,626][2152645] Saved parameter configuration for experiment doom_death_match not found!
[2025-07-18 13:38:23,626][2152645] Starting experiment from scratch!
[2025-07-18 13:38:23,629][2152645] Experiment dir /home/isaacp/repos/rl_class/train_dir/doom_death_match already exists!
[2025-07-18 13:38:23,630][2152645] Resuming existing experiment from /home/isaacp/repos/rl_class/train_dir/doom_death_match...
[2025-07-18 13:38:23,630][2152645] Weights and Biases integration disabled
[2025-07-18 13:38:23,631][2152645] Environment var CUDA_VISIBLE_DEVICES is 0

[2025-07-18 13:38:24,551][2152645] Automatically setting recurrence to 32
[2025-07-18 13:38:24,552][2152645] Starting experiment with the following configuration:
help=False
algo=APPO
env=doom_deathmatch_bots
experiment=doom_death_match
train_dir=/home/isaacp/repos/rl_class/train_dir
restart_behavior=resume
device=gpu
seed=None
num_policies=1
async_rl=True
serial_mode=False
batched_sampling=False
num_batches_to_accumulate=2
worker_num_splits=2
policy_workers_per_policy=1
max_policy_lag=1000
num_workers=8
num_envs_per_worker=4
batch_size=1024
num_batches_per_epoch=1
num_epochs=1
rollout=32
recurrence=32
shuffle_minibatches=False
gamma=0.99
reward_scale=1.0
reward_clip=1000.0
value_bootstrap=False
normalize_returns=True
exploration_loss_coeff=0.001
value_loss_coeff=0.5
kl_loss_coeff=0.0
exploration_loss=symmetric_kl
gae_lambda=0.95
ppo_clip_ratio=0.1
ppo_clip_value=0.2
with_vtrace=False
vtrace_rho=1.0
vtrace_c=1.0
optimizer=adam
adam_eps=1e-06
adam_beta1=0.9
adam_beta2=0.999
max_grad_norm=4.0
learning_rate=0.0001
lr_schedule=constant
lr_schedule_kl_threshold=0.008
lr_adaptive_min=1e-06
lr_adaptive_max=0.01
obs_subtract_mean=0.0
obs_scale=255.0
normalize_input=True
normalize_input_keys=None
decorrelate_experience_max_seconds=0
decorrelate_envs_on_one_worker=True
actor_worker_gpus=[]
set_workers_cpu_affinity=True
force_envs_single_thread=False
default_niceness=0
log_to_file=True
experiment_summaries_interval=10
flush_summaries_interval=30
stats_avg=100
summaries_use_frameskip=True
heartbeat_interval=20
heartbeat_reporting_interval=600
train_for_env_steps=10000000000
train_for_seconds=10000000000
save_every_sec=120
keep_checkpoints=2
load_checkpoint_kind=latest
save_milestones_sec=-1
save_best_every_sec=5
save_best_metric=reward
save_best_after=100000
benchmark=False
encoder_mlp_layers=[512, 512]
encoder_conv_architecture=convnet_simple
encoder_conv_mlp_layers=[512]
use_rnn=True
rnn_size=512
rnn_type=gru
rnn_num_layers=1
decoder_mlp_layers=[]
nonlinearity=elu
policy_initialization=orthogonal
policy_init_gain=1.0
actor_critic_share_weights=True
adaptive_stddev=True
continuous_tanh_scale=0.0
initial_stddev=1.0
use_env_info_cache=False
env_gpu_actions=False
env_gpu_observations=True
env_frameskip=4
env_framestack=1
pixel_format=CHW
use_record_episode_statistics=False
with_wandb=False
wandb_user=None
wandb_project=sample_factory
wandb_group=None
wandb_job_type=SF
wandb_tags=[]
with_pbt=False
pbt_mix_policies_in_one_env=True
pbt_period_env_steps=5000000
pbt_start_mutation=20000000
pbt_replace_fraction=0.3
pbt_mutation_rate=0.15
pbt_replace_reward_gap=0.1
pbt_replace_reward_gap_absolute=1e-06
pbt_optimize_gamma=False
pbt_target_objective=true_objective
pbt_perturb_min=1.1
pbt_perturb_max=1.5
num_agents=-1
num_humans=0
num_bots=-1
start_bot_difficulty=None
timelimit=None
res_w=128
res_h=72
wide_aspect_ratio=False
eval_env_frameskip=1
fps=35
command_line=--env=doom_deathmatch_bots --num_workers=8 --experiment=doom_death_match --num_envs_per_worker=4 --train_for_env_steps=10_000_000_000
cli_args={'env': 'doom_deathmatch_bots', 'experiment': 'doom_death_match', 'num_workers': 8, 'num_envs_per_worker': 4, 'train_for_env_steps': 10000000000}
git_hash=unknown
git_repo_name=not a git repository
[2025-07-18 13:38:24,552][2152645] Saving configuration to /home/isaacp/repos/rl_class/train_dir/doom_death_match/config.json...
[2025-07-18 13:38:24,554][2152645] Rollout worker 0 uses device cpu
[2025-07-18 13:38:24,554][2152645] Rollout worker 1 uses device cpu
[2025-07-18 13:38:24,555][2152645] Rollout worker 2 uses device cpu
[2025-07-18 13:38:24,555][2152645] Rollout worker 3 uses device cpu
[2025-07-18 13:38:24,555][2152645] Rollout worker 4 uses device cpu
[2025-07-18 13:38:24,555][2152645] Rollout worker 5 uses device cpu
[2025-07-18 13:38:24,555][2152645] Rollout worker 6 uses device cpu
[2025-07-18 13:38:24,555][2152645] Rollout worker 7 uses device cpu
[2025-07-18 13:38:24,579][2152645] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 13:38:24,579][2152645] InferenceWorker_p0-w0: min num requests: 2
[2025-07-18 13:38:24,599][2152645] Starting all processes...
[2025-07-18 13:38:24,599][2152645] Starting process learner_proc0
[2025-07-18 13:38:24,649][2152645] Starting all processes...
[2025-07-18 13:38:24,651][2152645] Starting process inference_proc0-0
[2025-07-18 13:38:24,651][2152645] Starting process rollout_proc0
[2025-07-18 13:38:24,651][2152645] Starting process rollout_proc1
[2025-07-18 13:38:24,651][2152645] Starting process rollout_proc2
[2025-07-18 13:38:24,651][2152645] Starting process rollout_proc3
[2025-07-18 13:38:24,651][2152645] Starting process rollout_proc4
[2025-07-18 13:38:24,652][2152645] Starting process rollout_proc5
[2025-07-18 13:38:24,653][2152645] Starting process rollout_proc6
[2025-07-18 13:38:24,653][2152645] Starting process rollout_proc7
[2025-07-18 13:38:26,244][2152645] Inference worker 0-0 is ready!
[2025-07-18 13:38:26,245][2152645] All inference workers are ready! Signal rollout workers to start!
[2025-07-18 13:38:28,631][2152645] Fps is (10 sec: nan, 60 sec: nan, 300 sec: nan). Total num frames: 28672. Throughput: 0: nan. Samples: 0. Policy #0 lag: (min: 0.0, avg: 0.0, max: 0.0)
[2025-07-18 13:38:33,631][2152645] Fps is (10 sec: 36044.9, 60 sec: 36044.9, 300 sec: 36044.9). Total num frames: 208896. Throughput: 0: 8203.2. Samples: 41016. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)
[2025-07-18 13:38:38,631][2152645] Fps is (10 sec: 36044.8, 60 sec: 36044.8, 300 sec: 36044.8). Total num frames: 389120. Throughput: 0: 9483.4. Samples: 94834. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2025-07-18 13:38:38,631][2152645] Avg episode reward: [(0, '-6.761')]
[2025-07-18 13:38:43,631][2152645] Fps is (10 sec: 35635.2, 60 sec: 35771.8, 300 sec: 35771.8). Total num frames: 565248. Throughput: 0: 8106.3. Samples: 121594. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)
[2025-07-18 13:38:43,631][2152645] Avg episode reward: [(0, '-6.996')]
[2025-07-18 13:38:44,574][2152645] Heartbeat connected on Batcher_0
[2025-07-18 13:38:44,576][2152645] Heartbeat connected on LearnerWorker_p0
[2025-07-18 13:38:44,582][2152645] Heartbeat connected on InferenceWorker_p0-w0
[2025-07-18 13:38:44,585][2152645] Heartbeat connected on RolloutWorker_w0
[2025-07-18 13:38:44,586][2152645] Heartbeat connected on RolloutWorker_w1
[2025-07-18 13:38:44,588][2152645] Heartbeat connected on RolloutWorker_w2
[2025-07-18 13:38:44,590][2152645] Heartbeat connected on RolloutWorker_w3
[2025-07-18 13:38:44,592][2152645] Heartbeat connected on RolloutWorker_w4
[2025-07-18 13:38:44,597][2152645] Heartbeat connected on RolloutWorker_w5
[2025-07-18 13:38:44,598][2152645] Heartbeat connected on RolloutWorker_w6
[2025-07-18 13:38:44,599][2152645] Heartbeat connected on RolloutWorker_w7
[2025-07-18 13:38:48,631][2152645] Fps is (10 sec: 35635.3, 60 sec: 35840.0, 300 sec: 35840.0). Total num frames: 745472. Throughput: 0: 8771.2. Samples: 175424. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)
[2025-07-18 13:38:48,631][2152645] Avg episode reward: [(0, '-6.996')]
[2025-07-18 13:38:53,631][2152645] Fps is (10 sec: 36044.9, 60 sec: 35881.0, 300 sec: 35881.0). Total num frames: 925696. Throughput: 0: 9154.0. Samples: 228850. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)
[2025-07-18 13:38:53,631][2152645] Avg episode reward: [(0, '-6.889')]
[2025-07-18 13:38:58,631][2152645] Fps is (10 sec: 35635.1, 60 sec: 35771.7, 300 sec: 35771.7). Total num frames: 1101824. Throughput: 0: 8521.3. Samples: 255640. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)
[2025-07-18 13:38:58,631][2152645] Avg episode reward: [(0, '-6.897')]
[2025-07-18 13:39:03,631][2152645] Fps is (10 sec: 35225.5, 60 sec: 35693.7, 300 sec: 35693.7). Total num frames: 1277952. Throughput: 0: 8807.1. Samples: 308250. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)
[2025-07-18 13:39:03,631][2152645] Avg episode reward: [(0, '-6.897')]
[2025-07-18 13:39:08,631][2152645] Fps is (10 sec: 35225.7, 60 sec: 35635.2, 300 sec: 35635.2). Total num frames: 1454080. Throughput: 0: 9030.3. Samples: 361212. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2025-07-18 13:39:08,631][2152645] Avg episode reward: [(0, '-6.757')]
[2025-07-18 13:39:13,631][2152645] Fps is (10 sec: 35225.7, 60 sec: 35589.7, 300 sec: 35589.7). Total num frames: 1630208. Throughput: 0: 8622.8. Samples: 388024. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)
[2025-07-18 13:39:13,631][2152645] Avg episode reward: [(0, '-6.398')]
[2025-07-18 13:39:18,631][2152645] Fps is (10 sec: 35635.2, 60 sec: 35635.2, 300 sec: 35635.2). Total num frames: 1810432. Throughput: 0: 8903.3. Samples: 441666. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2025-07-18 13:39:18,631][2152645] Avg episode reward: [(0, '-6.414')]
[2025-07-18 13:39:23,631][2152645] Fps is (10 sec: 36044.7, 60 sec: 35672.4, 300 sec: 35672.4). Total num frames: 1990656. Throughput: 0: 8892.1. Samples: 494980. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2025-07-18 13:39:23,631][2152645] Avg episode reward: [(0, '-5.949')]
[2025-07-18 13:39:28,631][2152645] Fps is (10 sec: 35635.3, 60 sec: 35635.2, 300 sec: 35635.2). Total num frames: 2166784. Throughput: 0: 8893.2. Samples: 521788. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)
[2025-07-18 13:39:28,631][2152645] Avg episode reward: [(0, '-5.564')]
[2025-07-18 13:39:33,631][2152645] Fps is (10 sec: 35635.0, 60 sec: 35635.2, 300 sec: 35666.7). Total num frames: 2347008. Throughput: 0: 8892.2. Samples: 575572. Policy #0 lag: (min: 0.0, avg: 0.6, max: 1.0)
[2025-07-18 13:39:33,631][2152645] Avg episode reward: [(0, '-5.440')]
[2025-07-18 13:39:38,631][2152645] Fps is (10 sec: 36044.9, 60 sec: 35635.2, 300 sec: 35693.7). Total num frames: 2527232. Throughput: 0: 8895.4. Samples: 629142. Policy #0 lag: (min: 0.0, avg: 0.5, max: 1.0)
[2025-07-18 13:39:38,631][2152645] Avg episode reward: [(0, '-5.213')]
[2025-07-18 13:39:43,631][2152645] Fps is (10 sec: 35635.3, 60 sec: 35635.2, 300 sec: 35662.5). Total num frames: 2703360. Throughput: 0: 8895.0. Samples: 655914. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)
[2025-07-18 13:39:43,631][2152645] Avg episode reward: [(0, '-5.153')]
[2025-07-18 13:39:48,631][2152645] Fps is (10 sec: 35635.0, 60 sec: 35635.2, 300 sec: 35686.4). Total num frames: 2883584. Throughput: 0: 8917.7. Samples: 709546. Policy #0 lag: (min: 0.0, avg: 0.6, max: 1.0)
[2025-07-18 13:39:48,631][2152645] Avg episode reward: [(0, '-5.090')]
[2025-07-18 13:39:53,631][2152645] Fps is (10 sec: 35635.3, 60 sec: 35566.9, 300 sec: 35659.3). Total num frames: 3059712. Throughput: 0: 8927.8. Samples: 762962. Policy #0 lag: (min: 0.0, avg: 0.7, max: 1.0)
[2025-07-18 13:39:53,631][2152645] Avg episode reward: [(0, '-5.068')]
[2025-07-18 13:39:58,631][2152645] Fps is (10 sec: 35225.5, 60 sec: 35566.9, 300 sec: 35635.2). Total num frames: 3235840. Throughput: 0: 8917.2. Samples: 789298. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)
[2025-07-18 13:39:58,631][2152645] Avg episode reward: [(0, '-4.799')]
[2025-07-18 13:40:03,631][2152645] Fps is (10 sec: 35225.7, 60 sec: 35566.9, 300 sec: 35613.6). Total num frames: 3411968. Throughput: 0: 8884.9. Samples: 841488. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)
[2025-07-18 13:40:03,631][2152645] Avg episode reward: [(0, '-4.678')]
[2025-07-18 13:40:08,631][2152645] Fps is (10 sec: 35225.5, 60 sec: 35566.9, 300 sec: 35594.2). Total num frames: 3588096. Throughput: 0: 8879.6. Samples: 894564. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)
[2025-07-18 13:40:08,631][2152645] Avg episode reward: [(0, '-4.030')]
[2025-07-18 13:40:13,631][2152645] Fps is (10 sec: 35225.6, 60 sec: 35566.9, 300 sec: 35576.7). Total num frames: 3764224. Throughput: 0: 8876.2. Samples: 921216. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)
[2025-07-18 13:40:13,631][2152645] Avg episode reward: [(0, '-3.878')]
[2025-07-18 13:40:18,631][2152645] Fps is (10 sec: 35225.8, 60 sec: 35498.7, 300 sec: 35560.7). Total num frames: 3940352. Throughput: 0: 8854.9. Samples: 974040. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)
[2025-07-18 13:40:18,631][2152645] Avg episode reward: [(0, '-3.721')]
[2025-07-18 13:40:23,631][2152645] Fps is (10 sec: 35225.4, 60 sec: 35430.4, 300 sec: 35546.1). Total num frames: 4116480. Throughput: 0: 8830.2. Samples: 1026502. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)
[2025-07-18 13:40:23,632][2152645] Avg episode reward: [(0, '-3.460')]
[2025-07-18 13:40:28,631][2152645] Fps is (10 sec: 34816.0, 60 sec: 35362.1, 300 sec: 35498.7). Total num frames: 4288512. Throughput: 0: 8818.5. Samples: 1052744. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)
[2025-07-18 13:40:28,631][2152645] Avg episode reward: [(0, '-3.611')]
[2025-07-18 13:40:33,631][2152645] Fps is (10 sec: 34406.5, 60 sec: 35225.6, 300 sec: 35455.0). Total num frames: 4460544. Throughput: 0: 8784.1. Samples: 1104830. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2025-07-18 13:40:33,632][2152645] Avg episode reward: [(0, '-3.845')]
[2025-07-18 13:40:38,631][2152645] Fps is (10 sec: 35225.6, 60 sec: 35225.6, 300 sec: 35477.7). Total num frames: 4640768. Throughput: 0: 8772.6. Samples: 1157728. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2025-07-18 13:40:38,631][2152645] Avg episode reward: [(0, '-4.123')]
[2025-07-18 13:40:43,631][2152645] Fps is (10 sec: 35635.2, 60 sec: 35225.6, 300 sec: 35468.3). Total num frames: 4816896. Throughput: 0: 8785.2. Samples: 1184632. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)
[2025-07-18 13:40:43,631][2152645] Avg episode reward: [(0, '-4.188')]
[2025-07-18 13:40:48,631][2152645] Fps is (10 sec: 35225.6, 60 sec: 35157.3, 300 sec: 35459.7). Total num frames: 4993024. Throughput: 0: 8792.2. Samples: 1237138. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)
[2025-07-18 13:40:48,631][2152645] Avg episode reward: [(0, '-4.238')]
[2025-07-18 13:40:53,631][2152645] Fps is (10 sec: 35635.1, 60 sec: 35225.6, 300 sec: 35479.8). Total num frames: 5173248. Throughput: 0: 8808.0. Samples: 1290924. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)
[2025-07-18 13:40:53,631][2152645] Avg episode reward: [(0, '-3.923')]
[2025-07-18 13:40:58,631][2152645] Fps is (10 sec: 36044.8, 60 sec: 35293.9, 300 sec: 35498.7). Total num frames: 5353472. Throughput: 0: 8819.5. Samples: 1318094. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2025-07-18 13:40:58,632][2152645] Avg episode reward: [(0, '-3.746')]
[2025-07-18 13:41:03,631][2152645] Fps is (10 sec: 35635.4, 60 sec: 35293.9, 300 sec: 35489.9). Total num frames: 5529600. Throughput: 0: 8817.1. Samples: 1370808. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)
[2025-07-18 13:41:03,631][2152645] Avg episode reward: [(0, '-3.750')]
[2025-07-18 13:41:08,631][2152645] Fps is (10 sec: 35225.7, 60 sec: 35293.9, 300 sec: 35481.6). Total num frames: 5705728. Throughput: 0: 8825.6. Samples: 1423652. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2025-07-18 13:41:08,631][2152645] Avg episode reward: [(0, '-3.603')]
[2025-07-18 13:41:09,542][2152645] Keyboard interrupt detected in the event loop EvtLoop [Runner_EvtLoop, process=main process 2152645], exiting...
[2025-07-18 13:41:09,543][2152645] Runner profile tree view:
main_loop: 164.9441
[2025-07-18 13:41:09,544][2152645] Collected {0: 5734400}, FPS: 34765.7
[2025-07-18 15:14:00,961][2175462] Saving configuration to /home/isaacp/repos/rl_class/train_dir/default_experiment/config.json...
[2025-07-18 15:14:01,577][2175462] Rollout worker 0 uses device cpu
[2025-07-18 15:14:01,577][2175462] Rollout worker 1 uses device cpu
[2025-07-18 15:14:01,578][2175462] Rollout worker 2 uses device cpu
[2025-07-18 15:14:01,578][2175462] Rollout worker 3 uses device cpu
[2025-07-18 15:14:01,578][2175462] Rollout worker 4 uses device cpu
[2025-07-18 15:14:01,579][2175462] Rollout worker 5 uses device cpu
[2025-07-18 15:14:01,579][2175462] Rollout worker 6 uses device cpu
[2025-07-18 15:14:01,579][2175462] Rollout worker 7 uses device cpu
[2025-07-18 15:14:01,611][2175462] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 15:14:01,612][2175462] InferenceWorker_p0-w0: min num requests: 2
[2025-07-18 15:14:01,636][2175462] Starting all processes...
[2025-07-18 15:14:01,636][2175462] Starting process learner_proc0
[2025-07-18 15:14:01,686][2175462] Starting all processes...
[2025-07-18 15:14:01,689][2175462] Starting process inference_proc0-0
[2025-07-18 15:14:01,689][2175462] Starting process rollout_proc0
[2025-07-18 15:14:01,690][2175462] Starting process rollout_proc1
[2025-07-18 15:14:01,690][2175462] Starting process rollout_proc2
[2025-07-18 15:14:01,690][2175462] Starting process rollout_proc3
[2025-07-18 15:14:01,690][2175462] Starting process rollout_proc4
[2025-07-18 15:14:01,691][2175462] Starting process rollout_proc5
[2025-07-18 15:14:01,691][2175462] Starting process rollout_proc6
[2025-07-18 15:14:01,692][2175462] Starting process rollout_proc7
[2025-07-18 15:14:02,730][2175621] Worker 6 uses CPU cores [24, 25, 26, 27]
[2025-07-18 15:14:02,745][2175619] Worker 3 uses CPU cores [12, 13, 14, 15]
[2025-07-18 15:14:02,748][2175601] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 15:14:02,748][2175601] Set environment var CUDA_VISIBLE_DEVICES to '0' (GPU indices [0]) for learning process 0
[2025-07-18 15:14:02,761][2175601] Num visible devices: 1
[2025-07-18 15:14:02,762][2175601] Starting seed is not provided
[2025-07-18 15:14:02,762][2175601] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 15:14:02,762][2175601] Initializing actor-critic model on device cuda:0
[2025-07-18 15:14:02,762][2175601] RunningMeanStd input shape: (3, 72, 128)
[2025-07-18 15:14:02,764][2175601] RunningMeanStd input shape: (1,)
[2025-07-18 15:14:02,764][2175614] Worker 1 uses CPU cores [4, 5, 6, 7]
[2025-07-18 15:14:02,772][2175601] ConvEncoder: input_channels=3
[2025-07-18 15:14:02,797][2175622] Worker 7 uses CPU cores [28, 29, 30, 31]
[2025-07-18 15:14:02,811][2175620] Worker 5 uses CPU cores [20, 21, 22, 23]
[2025-07-18 15:14:02,816][2175615] Worker 2 uses CPU cores [8, 9, 10, 11]
[2025-07-18 15:14:02,816][2175617] Worker 0 uses CPU cores [0, 1, 2, 3]
[2025-07-18 15:14:02,829][2175601] Conv encoder output size: 512
[2025-07-18 15:14:02,829][2175601] Policy head output size: 512
[2025-07-18 15:14:02,837][2175616] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 15:14:02,837][2175601] Created Actor Critic model with architecture:
[2025-07-18 15:14:02,837][2175616] Set environment var CUDA_VISIBLE_DEVICES to '0' (GPU indices [0]) for inference process 0
[2025-07-18 15:14:02,837][2175601] ActorCriticSharedWeights(
  (obs_normalizer): ObservationNormalizer(
    (running_mean_std): RunningMeanStdDictInPlace(
      (running_mean_std): ModuleDict(
        (obs): RunningMeanStdInPlace()
      )
    )
  )
  (returns_normalizer): RecursiveScriptModule(original_name=RunningMeanStdInPlace)
  (encoder): VizdoomEncoder(
    (basic_encoder): ConvEncoder(
      (enc): RecursiveScriptModule(
        original_name=ConvEncoderImpl
        (conv_head): RecursiveScriptModule(
          original_name=Sequential
          (0): RecursiveScriptModule(original_name=Conv2d)
          (1): RecursiveScriptModule(original_name=ELU)
          (2): RecursiveScriptModule(original_name=Conv2d)
          (3): RecursiveScriptModule(original_name=ELU)
          (4): RecursiveScriptModule(original_name=Conv2d)
          (5): RecursiveScriptModule(original_name=ELU)
        )
        (mlp_layers): RecursiveScriptModule(
          original_name=Sequential
          (0): RecursiveScriptModule(original_name=Linear)
          (1): RecursiveScriptModule(original_name=ELU)
        )
      )
    )
  )
  (core): ModelCoreRNN(
    (core): GRU(512, 512)
  )
  (decoder): MlpDecoder(
    (mlp): Identity()
  )
  (critic_linear): Linear(in_features=512, out_features=1, bias=True)
  (action_parameterization): ActionParameterizationDefault(
    (distribution_linear): Linear(in_features=512, out_features=5, bias=True)
  )
)
[2025-07-18 15:14:02,855][2175616] Num visible devices: 1
[2025-07-18 15:14:02,875][2175618] Worker 4 uses CPU cores [16, 17, 18, 19]
[2025-07-18 15:14:02,912][2175601] Using optimizer <class 'torch.optim.adam.Adam'>
[2025-07-18 15:14:03,560][2175601] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000982_4022272.pth...
[2025-07-18 15:14:03,583][2175601] Loading model from checkpoint
[2025-07-18 15:14:03,584][2175601] Loaded experiment state at self.train_step=982, self.env_steps=4022272
[2025-07-18 15:14:03,584][2175601] Initialized policy 0 weights for model version 982
[2025-07-18 15:14:03,585][2175601] LearnerWorker_p0 finished initialization!
[2025-07-18 15:14:03,585][2175601] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2025-07-18 15:14:03,640][2175616] RunningMeanStd input shape: (3, 72, 128)
[2025-07-18 15:14:03,641][2175616] RunningMeanStd input shape: (1,)
[2025-07-18 15:14:03,647][2175616] ConvEncoder: input_channels=3
[2025-07-18 15:14:03,695][2175616] Conv encoder output size: 512
[2025-07-18 15:14:03,696][2175616] Policy head output size: 512
[2025-07-18 15:14:03,698][2175462] Fps is (10 sec: nan, 60 sec: nan, 300 sec: nan). Total num frames: 4022272. Throughput: 0: nan. Samples: 0. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[2025-07-18 15:14:03,720][2175462] Inference worker 0-0 is ready!
[2025-07-18 15:14:03,721][2175462] All inference workers are ready! Signal rollout workers to start!
[2025-07-18 15:14:03,739][2175619] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 15:14:03,740][2175615] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 15:14:03,741][2175621] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 15:14:03,741][2175622] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 15:14:03,742][2175618] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 15:14:03,742][2175617] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 15:14:03,746][2175620] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 15:14:03,746][2175614] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 15:14:03,907][2175618] Decorrelating experience for 0 frames...
[2025-07-18 15:14:03,909][2175619] Decorrelating experience for 0 frames...
[2025-07-18 15:14:03,912][2175622] Decorrelating experience for 0 frames...
[2025-07-18 15:14:03,913][2175621] Decorrelating experience for 0 frames...
[2025-07-18 15:14:03,916][2175615] Decorrelating experience for 0 frames...
[2025-07-18 15:14:03,918][2175614] Decorrelating experience for 0 frames...
[2025-07-18 15:14:04,061][2175618] Decorrelating experience for 32 frames...
[2025-07-18 15:14:04,070][2175615] Decorrelating experience for 32 frames...
[2025-07-18 15:14:04,074][2175621] Decorrelating experience for 32 frames...
[2025-07-18 15:14:04,078][2175614] Decorrelating experience for 32 frames...
[2025-07-18 15:14:04,078][2175620] Decorrelating experience for 0 frames...
[2025-07-18 15:14:04,106][2175619] Decorrelating experience for 32 frames...
[2025-07-18 15:14:04,227][2175620] Decorrelating experience for 32 frames...
[2025-07-18 15:14:04,252][2175618] Decorrelating experience for 64 frames...
[2025-07-18 15:14:04,260][2175622] Decorrelating experience for 32 frames...
[2025-07-18 15:14:04,261][2175621] Decorrelating experience for 64 frames...
[2025-07-18 15:14:04,268][2175615] Decorrelating experience for 64 frames...
[2025-07-18 15:14:04,271][2175617] Decorrelating experience for 0 frames...
[2025-07-18 15:14:04,407][2175620] Decorrelating experience for 64 frames...
[2025-07-18 15:14:04,420][2175617] Decorrelating experience for 32 frames...
[2025-07-18 15:14:04,421][2175618] Decorrelating experience for 96 frames...
[2025-07-18 15:14:04,434][2175621] Decorrelating experience for 96 frames...
[2025-07-18 15:14:04,437][2175615] Decorrelating experience for 96 frames...
[2025-07-18 15:14:04,443][2175619] Decorrelating experience for 64 frames...
[2025-07-18 15:14:04,444][2175622] Decorrelating experience for 64 frames...
[2025-07-18 15:14:04,574][2175620] Decorrelating experience for 96 frames...
[2025-07-18 15:14:04,610][2175617] Decorrelating experience for 64 frames...
[2025-07-18 15:14:04,621][2175614] Decorrelating experience for 64 frames...
[2025-07-18 15:14:04,628][2175619] Decorrelating experience for 96 frames...
[2025-07-18 15:14:04,770][2175617] Decorrelating experience for 96 frames...
[2025-07-18 15:14:04,781][2175614] Decorrelating experience for 96 frames...
[2025-07-18 15:14:04,819][2175622] Decorrelating experience for 96 frames...
[2025-07-18 15:14:05,118][2175601] Signal inference workers to stop experience collection...
[2025-07-18 15:14:05,120][2175616] InferenceWorker_p0-w0: stopping experience collection
[2025-07-18 15:14:05,741][2175601] Signal inference workers to resume experience collection...
[2025-07-18 15:14:05,742][2175616] InferenceWorker_p0-w0: resuming experience collection
[2025-07-18 15:14:05,742][2175601] Stopping Batcher_0...
[2025-07-18 15:14:05,742][2175601] Loop batcher_evt_loop terminating...
[2025-07-18 15:14:05,746][2175462] Component Batcher_0 stopped!
[2025-07-18 15:14:05,755][2175616] Weights refcount: 2 0
[2025-07-18 15:14:05,757][2175616] Stopping InferenceWorker_p0-w0...
[2025-07-18 15:14:05,757][2175616] Loop inference_proc0-0_evt_loop terminating...
[2025-07-18 15:14:05,757][2175462] Component InferenceWorker_p0-w0 stopped!
[2025-07-18 15:14:05,763][2175617] Stopping RolloutWorker_w0...
[2025-07-18 15:14:05,763][2175614] Stopping RolloutWorker_w1...
[2025-07-18 15:14:05,763][2175618] Stopping RolloutWorker_w4...
[2025-07-18 15:14:05,764][2175617] Loop rollout_proc0_evt_loop terminating...
[2025-07-18 15:14:05,764][2175614] Loop rollout_proc1_evt_loop terminating...
[2025-07-18 15:14:05,764][2175618] Loop rollout_proc4_evt_loop terminating...
[2025-07-18 15:14:05,764][2175620] Stopping RolloutWorker_w5...
[2025-07-18 15:14:05,764][2175620] Loop rollout_proc5_evt_loop terminating...
[2025-07-18 15:14:05,763][2175462] Component RolloutWorker_w0 stopped!
[2025-07-18 15:14:05,764][2175462] Component RolloutWorker_w1 stopped!
[2025-07-18 15:14:05,765][2175462] Component RolloutWorker_w4 stopped!
[2025-07-18 15:14:05,765][2175462] Component RolloutWorker_w5 stopped!
[2025-07-18 15:14:05,766][2175619] Stopping RolloutWorker_w3...
[2025-07-18 15:14:05,766][2175619] Loop rollout_proc3_evt_loop terminating...
[2025-07-18 15:14:05,766][2175622] Stopping RolloutWorker_w7...
[2025-07-18 15:14:05,766][2175462] Component RolloutWorker_w3 stopped!
[2025-07-18 15:14:05,766][2175622] Loop rollout_proc7_evt_loop terminating...
[2025-07-18 15:14:05,766][2175462] Component RolloutWorker_w7 stopped!
[2025-07-18 15:14:05,767][2175621] Stopping RolloutWorker_w6...
[2025-07-18 15:14:05,767][2175615] Stopping RolloutWorker_w2...
[2025-07-18 15:14:05,767][2175621] Loop rollout_proc6_evt_loop terminating...
[2025-07-18 15:14:05,767][2175615] Loop rollout_proc2_evt_loop terminating...
[2025-07-18 15:14:05,767][2175462] Component RolloutWorker_w2 stopped!
[2025-07-18 15:14:05,767][2175462] Component RolloutWorker_w6 stopped!
[2025-07-18 15:14:05,848][2175601] Saving /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000984_4030464.pth...
[2025-07-18 15:14:05,883][2175601] Removing /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000980_4014080.pth
[2025-07-18 15:14:05,889][2175601] Saving /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000984_4030464.pth...
[2025-07-18 15:14:05,937][2175601] Stopping LearnerWorker_p0...
[2025-07-18 15:14:05,937][2175601] Loop learner_proc0_evt_loop terminating...
[2025-07-18 15:14:05,937][2175462] Component LearnerWorker_p0 stopped!
[2025-07-18 15:14:05,938][2175462] Waiting for process learner_proc0 to stop...
[2025-07-18 15:14:06,659][2175462] Waiting for process inference_proc0-0 to join...
[2025-07-18 15:14:06,660][2175462] Waiting for process rollout_proc0 to join...
[2025-07-18 15:14:06,660][2175462] Waiting for process rollout_proc1 to join...
[2025-07-18 15:14:06,660][2175462] Waiting for process rollout_proc2 to join...
[2025-07-18 15:14:06,661][2175462] Waiting for process rollout_proc3 to join...
[2025-07-18 15:14:06,661][2175462] Waiting for process rollout_proc4 to join...
[2025-07-18 15:14:06,662][2175462] Waiting for process rollout_proc5 to join...
[2025-07-18 15:14:06,662][2175462] Waiting for process rollout_proc6 to join...
[2025-07-18 15:14:06,663][2175462] Waiting for process rollout_proc7 to join...
[2025-07-18 15:14:06,663][2175462] Batcher 0 profile tree view:
batching: 0.0377, releasing_batches: 0.0004
[2025-07-18 15:14:06,663][2175462] InferenceWorker_p0-w0 profile tree view:
update_model: 0.0021
wait_policy: 0.0000
  wait_policy_total: 0.7265
one_step: 0.0012
  handle_policy_step: 0.6554
    deserialize: 0.0150, stack: 0.0011, obs_to_device_normalize: 0.1128, forward: 0.4625, send_messages: 0.0134
    prepare_outputs: 0.0429
      to_cpu: 0.0324
[2025-07-18 15:14:06,664][2175462] Learner 0 profile tree view:
misc: 0.0000, prepare_batch: 0.4212
train: 0.4176
  epoch_init: 0.0000, minibatch_init: 0.0000, losses_postprocess: 0.0005, kl_divergence: 0.0063, after_optimizer: 0.0252
  calculate_losses: 0.1388
    losses_init: 0.0000, forward_head: 0.0602, bptt_initial: 0.0520, tail: 0.0109, advantages_returns: 0.0004, losses: 0.0137
    bptt: 0.0013
      bptt_forward_core: 0.0012
  update: 0.2462
    clip: 0.0218
[2025-07-18 15:14:06,664][2175462] RolloutWorker_w0 profile tree view:
wait_for_trajectories: 0.0003, enqueue_policy_requests: 0.0113, env_step: 0.1283, overhead: 0.0064, complete_rollouts: 0.0002
save_policy_outputs: 0.0108
  split_output_tensors: 0.0036
[2025-07-18 15:14:06,664][2175462] RolloutWorker_w7 profile tree view:
wait_for_trajectories: 0.0006, enqueue_policy_requests: 0.0108, env_step: 0.1180, overhead: 0.0057, complete_rollouts: 0.0002
save_policy_outputs: 0.0095
  split_output_tensors: 0.0032
[2025-07-18 15:14:06,665][2175462] Loop Runner_EvtLoop terminating...
[2025-07-18 15:14:06,665][2175462] Runner profile tree view:
main_loop: 5.0292
[2025-07-18 15:14:06,665][2175462] Collected {0: 4030464}, FPS: 1628.9
[2025-07-18 15:14:06,753][2175462] Loading existing experiment configuration from /home/isaacp/repos/rl_class/train_dir/default_experiment/config.json
[2025-07-18 15:14:06,754][2175462] Overriding arg 'num_workers' with value 1 passed from command line
[2025-07-18 15:14:06,754][2175462] Adding new argument 'no_render'=True that is not in the saved config file!
[2025-07-18 15:14:06,754][2175462] Adding new argument 'save_video'=True that is not in the saved config file!
[2025-07-18 15:14:06,754][2175462] Adding new argument 'video_frames'=1000000000.0 that is not in the saved config file!
[2025-07-18 15:14:06,755][2175462] Adding new argument 'video_name'=None that is not in the saved config file!
[2025-07-18 15:14:06,755][2175462] Adding new argument 'max_num_frames'=1000000000.0 that is not in the saved config file!
[2025-07-18 15:14:06,755][2175462] Adding new argument 'max_num_episodes'=10 that is not in the saved config file!
[2025-07-18 15:14:06,755][2175462] Adding new argument 'push_to_hub'=False that is not in the saved config file!
[2025-07-18 15:14:06,755][2175462] Adding new argument 'hf_repository'=None that is not in the saved config file!
[2025-07-18 15:14:06,756][2175462] Adding new argument 'policy_index'=0 that is not in the saved config file!
[2025-07-18 15:14:06,756][2175462] Adding new argument 'eval_deterministic'=False that is not in the saved config file!
[2025-07-18 15:14:06,756][2175462] Adding new argument 'train_script'=None that is not in the saved config file!
[2025-07-18 15:14:06,756][2175462] Adding new argument 'enjoy_script'=None that is not in the saved config file!
[2025-07-18 15:14:06,756][2175462] Using frameskip 1 and render_action_repeat=4 for evaluation
[2025-07-18 15:14:06,769][2175462] Doom resolution: 160x120, resize resolution: (128, 72)
[2025-07-18 15:14:06,771][2175462] RunningMeanStd input shape: (3, 72, 128)
[2025-07-18 15:14:06,771][2175462] RunningMeanStd input shape: (1,)
[2025-07-18 15:14:06,777][2175462] ConvEncoder: input_channels=3
[2025-07-18 15:14:06,823][2175462] Conv encoder output size: 512
[2025-07-18 15:14:06,824][2175462] Policy head output size: 512
[2025-07-18 15:14:06,917][2175462] Loading state from checkpoint /home/isaacp/repos/rl_class/train_dir/default_experiment/checkpoint_p0/checkpoint_000000984_4030464.pth...
[2025-07-18 15:14:07,405][2175462] Num frames 100...
[2025-07-18 15:14:07,471][2175462] Num frames 200...
[2025-07-18 15:14:07,531][2175462] Num frames 300...
[2025-07-18 15:14:07,634][2175462] Avg episode rewards: #0: 7.850, true rewards: #0: 3.850
[2025-07-18 15:14:07,634][2175462] Avg episode reward: 7.850, avg true_objective: 3.850
[2025-07-18 15:14:07,645][2175462] Num frames 400...
[2025-07-18 15:14:07,703][2175462] Num frames 500...
[2025-07-18 15:14:07,774][2175462] Num frames 600...
[2025-07-18 15:14:07,838][2175462] Num frames 700...
[2025-07-18 15:14:07,898][2175462] Num frames 800...
[2025-07-18 15:14:07,954][2175462] Num frames 900...
[2025-07-18 15:14:08,009][2175462] Num frames 1000...
[2025-07-18 15:14:08,067][2175462] Num frames 1100...
[2025-07-18 15:14:08,141][2175462] Avg episode rewards: #0: 12.190, true rewards: #0: 5.690
[2025-07-18 15:14:08,142][2175462] Avg episode reward: 12.190, avg true_objective: 5.690
[2025-07-18 15:14:08,182][2175462] Num frames 1200...
[2025-07-18 15:14:08,241][2175462] Num frames 1300...
[2025-07-18 15:14:08,300][2175462] Num frames 1400...
[2025-07-18 15:14:08,355][2175462] Num frames 1500...
[2025-07-18 15:14:08,423][2175462] Num frames 1600...
[2025-07-18 15:14:08,475][2175462] Num frames 1700...
[2025-07-18 15:14:08,580][2175462] Avg episode rewards: #0: 11.993, true rewards: #0: 5.993
[2025-07-18 15:14:08,581][2175462] Avg episode reward: 11.993, avg true_objective: 5.993
[2025-07-18 15:14:08,582][2175462] Num frames 1800...
[2025-07-18 15:14:08,642][2175462] Num frames 1900...
[2025-07-18 15:14:08,706][2175462] Num frames 2000...
[2025-07-18 15:14:08,765][2175462] Num frames 2100...
[2025-07-18 15:14:08,829][2175462] Num frames 2200...
[2025-07-18 15:14:08,888][2175462] Num frames 2300...
[2025-07-18 15:14:08,947][2175462] Num frames 2400...
[2025-07-18 15:14:09,004][2175462] Num frames 2500...
[2025-07-18 15:14:09,068][2175462] Num frames 2600...
[2025-07-18 15:14:09,129][2175462] Num frames 2700...
[2025-07-18 15:14:09,194][2175462] Num frames 2800...
[2025-07-18 15:14:09,299][2175462] Avg episode rewards: #0: 13.965, true rewards: #0: 7.215
[2025-07-18 15:14:09,299][2175462] Avg episode reward: 13.965, avg true_objective: 7.215
[2025-07-18 15:14:09,310][2175462] Num frames 2900...
[2025-07-18 15:14:09,376][2175462] Num frames 3000...
[2025-07-18 15:14:09,436][2175462] Num frames 3100...
[2025-07-18 15:14:09,499][2175462] Num frames 3200...
[2025-07-18 15:14:09,557][2175462] Num frames 3300...
[2025-07-18 15:14:09,620][2175462] Num frames 3400...
[2025-07-18 15:14:09,678][2175462] Num frames 3500...
[2025-07-18 15:14:09,741][2175462] Num frames 3600...
[2025-07-18 15:14:09,801][2175462] Num frames 3700...
[2025-07-18 15:14:09,855][2175462] Num frames 3800...
[2025-07-18 15:14:09,919][2175462] Num frames 3900...
[2025-07-18 15:14:09,973][2175462] Num frames 4000...
[2025-07-18 15:14:10,033][2175462] Num frames 4100...
[2025-07-18 15:14:10,120][2175462] Avg episode rewards: #0: 16.532, true rewards: #0: 8.332
[2025-07-18 15:14:10,120][2175462] Avg episode reward: 16.532, avg true_objective: 8.332
[2025-07-18 15:14:10,145][2175462] Num frames 4200...
[2025-07-18 15:14:10,206][2175462] Num frames 4300...
[2025-07-18 15:14:10,269][2175462] Num frames 4400...
[2025-07-18 15:14:10,333][2175462] Num frames 4500...
[2025-07-18 15:14:10,435][2175462] Avg episode rewards: #0: 14.803, true rewards: #0: 7.637
[2025-07-18 15:14:10,436][2175462] Avg episode reward: 14.803, avg true_objective: 7.637
[2025-07-18 15:14:10,448][2175462] Num frames 4600...
[2025-07-18 15:14:10,505][2175462] Num frames 4700...
[2025-07-18 15:14:10,562][2175462] Num frames 4800...
[2025-07-18 15:14:10,623][2175462] Num frames 4900...
[2025-07-18 15:14:10,694][2175462] Avg episode rewards: #0: 13.334, true rewards: #0: 7.049
[2025-07-18 15:14:10,694][2175462] Avg episode reward: 13.334, avg true_objective: 7.049
[2025-07-18 15:14:10,734][2175462] Num frames 5000...
[2025-07-18 15:14:10,793][2175462] Num frames 5100...
[2025-07-18 15:14:10,857][2175462] Num frames 5200...
[2025-07-18 15:14:10,911][2175462] Num frames 5300...
[2025-07-18 15:14:10,973][2175462] Num frames 5400...
[2025-07-18 15:14:11,067][2175462] Avg episode rewards: #0: 12.598, true rewards: #0: 6.847
[2025-07-18 15:14:11,068][2175462] Avg episode reward: 12.598, avg true_objective: 6.847
[2025-07-18 15:14:11,085][2175462] Num frames 5500...
[2025-07-18 15:14:11,143][2175462] Num frames 5600...
[2025-07-18 15:14:11,204][2175462] Num frames 5700...
[2025-07-18 15:14:11,260][2175462] Num frames 5800...
[2025-07-18 15:14:11,314][2175462] Num frames 5900...
[2025-07-18 15:14:11,382][2175462] Num frames 6000...
[2025-07-18 15:14:11,441][2175462] Num frames 6100...
[2025-07-18 15:14:11,509][2175462] Num frames 6200...
[2025-07-18 15:14:11,572][2175462] Num frames 6300...
[2025-07-18 15:14:11,640][2175462] Num frames 6400...
[2025-07-18 15:14:11,699][2175462] Num frames 6500...
[2025-07-18 15:14:11,763][2175462] Num frames 6600...
[2025-07-18 15:14:11,824][2175462] Num frames 6700...
[2025-07-18 15:14:11,890][2175462] Num frames 6800...
[2025-07-18 15:14:11,949][2175462] Num frames 6900...
[2025-07-18 15:14:12,052][2175462] Avg episode rewards: #0: 14.980, true rewards: #0: 7.758
[2025-07-18 15:14:12,052][2175462] Avg episode reward: 14.980, avg true_objective: 7.758
[2025-07-18 15:14:12,064][2175462] Num frames 7000...
[2025-07-18 15:14:12,129][2175462] Num frames 7100...
[2025-07-18 15:14:12,188][2175462] Num frames 7200...
[2025-07-18 15:14:12,241][2175462] Num frames 7300...
[2025-07-18 15:14:12,303][2175462] Num frames 7400...
[2025-07-18 15:14:12,355][2175462] Num frames 7500...
[2025-07-18 15:14:12,425][2175462] Avg episode rewards: #0: 14.226, true rewards: #0: 7.526
[2025-07-18 15:14:12,425][2175462] Avg episode reward: 14.226, avg true_objective: 7.526
[2025-07-18 15:14:19,374][2175462] Replay video saved to /home/isaacp/repos/rl_class/train_dir/default_experiment/replay.mp4!
